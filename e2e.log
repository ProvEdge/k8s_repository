I0305 07:47:12.154959      26 test_context.go:410] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-378331353
I0305 07:47:12.155015      26 test_context.go:423] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0305 07:47:12.155196      26 e2e.go:124] Starting e2e run "60939dfd-5b1d-4aed-9f99-bbd14685f386" on Ginkgo node 1
{"msg":"Test Suite starting","total":277,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1614930430 - Will randomize all specs
Will run 277 of 4992 specs

Mar  5 07:47:12.179: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 07:47:12.185: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0305 07:47:12.187907      26 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp 127.0.0.1:8099: connect: connection refused
Mar  5 07:47:12.227: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  5 07:47:12.316: INFO: 37 / 37 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  5 07:47:12.316: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Mar  5 07:47:12.316: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  5 07:47:12.330: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Mar  5 07:47:12.330: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar  5 07:47:12.330: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Mar  5 07:47:12.330: INFO: e2e test version: v1.18.6
Mar  5 07:47:12.332: INFO: kube-apiserver version: v1.18.6
Mar  5 07:47:12.332: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 07:47:12.340: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:47:12.342: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename downward-api
Mar  5 07:47:12.479: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Mar  5 07:47:12.509: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6191
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 07:47:12.651: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13544641-73c4-4552-bfaf-73b731532d4e" in namespace "downward-api-6191" to be "Succeeded or Failed"
Mar  5 07:47:12.656: INFO: Pod "downwardapi-volume-13544641-73c4-4552-bfaf-73b731532d4e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.652381ms
Mar  5 07:47:14.662: INFO: Pod "downwardapi-volume-13544641-73c4-4552-bfaf-73b731532d4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010899975s
Mar  5 07:47:16.668: INFO: Pod "downwardapi-volume-13544641-73c4-4552-bfaf-73b731532d4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016778884s
STEP: Saw pod success
Mar  5 07:47:16.668: INFO: Pod "downwardapi-volume-13544641-73c4-4552-bfaf-73b731532d4e" satisfied condition "Succeeded or Failed"
Mar  5 07:47:16.673: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-13544641-73c4-4552-bfaf-73b731532d4e container client-container: <nil>
STEP: delete the pod
Mar  5 07:47:16.732: INFO: Waiting for pod downwardapi-volume-13544641-73c4-4552-bfaf-73b731532d4e to disappear
Mar  5 07:47:16.743: INFO: Pod downwardapi-volume-13544641-73c4-4552-bfaf-73b731532d4e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:47:16.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6191" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":277,"completed":1,"skipped":36,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:47:16.766: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename pod-network-test
E0305 07:47:16.770691      26 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp 127.0.0.1:8099: connect: connection refused
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7433
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-7433
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  5 07:47:16.955: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  5 07:47:17.081: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  5 07:47:19.090: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  5 07:47:21.089: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  5 07:47:23.089: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  5 07:47:25.090: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 07:47:27.090: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 07:47:29.089: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 07:47:31.088: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 07:47:33.090: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 07:47:35.088: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 07:47:37.096: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 07:47:39.088: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  5 07:47:39.099: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  5 07:47:39.111: INFO: The status of Pod netserver-2 is Running (Ready = true)
Mar  5 07:47:39.122: INFO: The status of Pod netserver-3 is Running (Ready = true)
Mar  5 07:47:39.133: INFO: The status of Pod netserver-4 is Running (Ready = false)
Mar  5 07:47:41.141: INFO: The status of Pod netserver-4 is Running (Ready = true)
Mar  5 07:47:41.152: INFO: The status of Pod netserver-5 is Running (Ready = false)
Mar  5 07:47:43.172: INFO: The status of Pod netserver-5 is Running (Ready = true)
STEP: Creating test pods
Mar  5 07:47:45.245: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.229:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7433 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 07:47:45.245: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 07:47:45.439: INFO: Found all expected endpoints: [netserver-0]
Mar  5 07:47:45.445: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.69:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7433 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 07:47:45.445: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 07:47:45.700: INFO: Found all expected endpoints: [netserver-1]
Mar  5 07:47:45.705: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.2:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7433 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 07:47:45.705: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 07:47:45.857: INFO: Found all expected endpoints: [netserver-2]
Mar  5 07:47:45.862: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.7:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7433 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 07:47:45.863: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 07:47:46.062: INFO: Found all expected endpoints: [netserver-3]
Mar  5 07:47:46.067: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.5.195:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7433 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 07:47:46.067: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 07:47:46.278: INFO: Found all expected endpoints: [netserver-4]
Mar  5 07:47:46.285: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.4.170:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7433 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 07:47:46.285: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 07:47:46.520: INFO: Found all expected endpoints: [netserver-5]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:47:46.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7433" for this suite.

â€¢ [SLOW TEST:29.777 seconds]
[sig-network] Networking
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":2,"skipped":49,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:47:46.547: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  5 07:47:47.433: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 07:47:50.479: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 07:47:50.485: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:47:51.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3189" for this suite.
STEP: Destroying namespace "webhook-3189-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.764 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":277,"completed":3,"skipped":55,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:47:52.311: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2553
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 07:47:52.672: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  5 07:48:03.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-2553 create -f -'
Mar  5 07:48:04.318: INFO: stderr: ""
Mar  5 07:48:04.318: INFO: stdout: "e2e-test-crd-publish-openapi-2817-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  5 07:48:04.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-2553 delete e2e-test-crd-publish-openapi-2817-crds test-cr'
Mar  5 07:48:04.455: INFO: stderr: ""
Mar  5 07:48:04.455: INFO: stdout: "e2e-test-crd-publish-openapi-2817-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar  5 07:48:04.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-2553 apply -f -'
Mar  5 07:48:04.918: INFO: stderr: ""
Mar  5 07:48:04.918: INFO: stdout: "e2e-test-crd-publish-openapi-2817-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  5 07:48:04.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-2553 delete e2e-test-crd-publish-openapi-2817-crds test-cr'
Mar  5 07:48:05.129: INFO: stderr: ""
Mar  5 07:48:05.129: INFO: stdout: "e2e-test-crd-publish-openapi-2817-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar  5 07:48:05.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 explain e2e-test-crd-publish-openapi-2817-crds'
Mar  5 07:48:05.689: INFO: stderr: ""
Mar  5 07:48:05.689: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2817-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:48:14.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2553" for this suite.

â€¢ [SLOW TEST:21.826 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":277,"completed":4,"skipped":64,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:48:14.138: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8891
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 07:48:14.434: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  5 07:48:19.442: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  5 07:48:25.475: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar  5 07:48:25.513: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8891 /apis/apps/v1/namespaces/deployment-8891/deployments/test-cleanup-deployment 4aa774aa-bc68-4c8c-92a6-e75ba1249300 5603810 1 2021-03-05 07:48:25 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-03-05 07:48:25 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0068d1658 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar  5 07:48:25.518: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Mar  5 07:48:25.518: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar  5 07:48:25.518: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8891 /apis/apps/v1/namespaces/deployment-8891/replicasets/test-cleanup-controller b3e814eb-ffa1-4aa8-bddf-c352ad249794 5603811 1 2021-03-05 07:48:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 4aa774aa-bc68-4c8c-92a6-e75ba1249300 0xc0068a238f 0xc0068a23e0}] []  [{e2e.test Update apps/v1 2021-03-05 07:48:14 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2021-03-05 07:48:25 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 97 97 55 55 52 97 97 45 98 99 54 56 45 52 99 56 99 45 57 50 97 54 45 101 55 53 98 97 49 50 52 57 51 48 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0068a24e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  5 07:48:25.526: INFO: Pod "test-cleanup-controller-5pkcw" is available:
&Pod{ObjectMeta:{test-cleanup-controller-5pkcw test-cleanup-controller- deployment-8891 /api/v1/namespaces/deployment-8891/pods/test-cleanup-controller-5pkcw 26ac75f7-b072-417e-97eb-1769cd8dae1f 5603796 0 2021-03-05 07:48:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.244.3.10/32 cni.projectcalico.org/podIPs:10.244.3.10/32] [{apps/v1 ReplicaSet test-cleanup-controller b3e814eb-ffa1-4aa8-bddf-c352ad249794 0xc0068a2ca7 0xc0068a2ca8}] []  [{kube-controller-manager Update v1 2021-03-05 07:48:14 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 51 101 56 49 52 101 98 45 102 102 97 49 45 52 97 97 56 45 98 100 100 102 45 99 51 53 50 97 100 50 52 57 55 57 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2021-03-05 07:48:15 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2021-03-05 07:48:24 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 51 46 49 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mzsnx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mzsnx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mzsnx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-8tgs6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:48:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:48:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:48:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:48:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.8,PodIP:10.244.3.10,StartTime:2021-03-05 07:48:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-05 07:48:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6233b791874aa06308f9e6bc8f9e4a1d78e72bc99b21099cf90d540a3bedb9ab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:48:25.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8891" for this suite.

â€¢ [SLOW TEST:11.416 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":277,"completed":5,"skipped":74,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:48:25.555: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7509
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7509.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7509.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7509.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7509.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  5 07:48:39.818: INFO: DNS probes using dns-test-40cca26e-9d10-4530-b4b0-dfbc9677d0e0 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7509.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7509.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7509.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7509.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  5 07:48:51.933: INFO: DNS probes using dns-test-d24839b8-261a-4d96-871c-bca50aae49b2 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7509.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7509.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7509.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7509.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  5 07:48:54.072: INFO: DNS probes using dns-test-24ced7f8-8477-4b84-8b05-a5afccd9ed5b succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:48:54.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7509" for this suite.

â€¢ [SLOW TEST:28.622 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":277,"completed":6,"skipped":90,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:48:54.182: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4062
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  5 07:48:55.012: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  5 07:48:57.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750527335, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750527335, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750527335, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750527335, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 07:49:00.096: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 07:49:00.105: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3383-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:49:01.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4062" for this suite.
STEP: Destroying namespace "webhook-4062-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:7.597 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":277,"completed":7,"skipped":184,"failed":0}
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:49:01.780: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-4085
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 07:49:02.216: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4085
I0305 07:49:02.344926      26 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4085, replica count: 1
I0305 07:49:03.395800      26 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0305 07:49:04.397179      26 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  5 07:49:04.528: INFO: Created: latency-svc-bsmdg
Mar  5 07:49:04.675: INFO: Got endpoints: latency-svc-bsmdg [176.677586ms]
Mar  5 07:49:04.705: INFO: Created: latency-svc-vwqn7
Mar  5 07:49:04.733: INFO: Created: latency-svc-ckzvj
Mar  5 07:49:04.752: INFO: Created: latency-svc-f5vwx
Mar  5 07:49:04.778: INFO: Created: latency-svc-dftd6
Mar  5 07:49:04.792: INFO: Got endpoints: latency-svc-vwqn7 [116.997613ms]
Mar  5 07:49:04.793: INFO: Got endpoints: latency-svc-ckzvj [117.804097ms]
Mar  5 07:49:04.797: INFO: Got endpoints: latency-svc-f5vwx [121.176548ms]
Mar  5 07:49:04.802: INFO: Created: latency-svc-8m9nb
Mar  5 07:49:04.819: INFO: Created: latency-svc-wq2sb
Mar  5 07:49:04.838: INFO: Created: latency-svc-v7g86
Mar  5 07:49:04.853: INFO: Created: latency-svc-nx5zj
Mar  5 07:49:04.878: INFO: Created: latency-svc-f7psk
Mar  5 07:49:04.898: INFO: Created: latency-svc-f9nbr
Mar  5 07:49:04.920: INFO: Created: latency-svc-8jw4z
Mar  5 07:49:04.970: INFO: Created: latency-svc-lwwst
Mar  5 07:49:05.001: INFO: Created: latency-svc-kwhq8
Mar  5 07:49:05.014: INFO: Created: latency-svc-zmwlr
Mar  5 07:49:05.045: INFO: Created: latency-svc-zfpb7
Mar  5 07:49:05.056: INFO: Created: latency-svc-hrzwh
Mar  5 07:49:05.077: INFO: Created: latency-svc-zjd57
Mar  5 07:49:05.109: INFO: Created: latency-svc-cxxdw
Mar  5 07:49:05.287: INFO: Got endpoints: latency-svc-dftd6 [610.908259ms]
Mar  5 07:49:05.318: INFO: Created: latency-svc-h7rdz
Mar  5 07:49:05.334: INFO: Got endpoints: latency-svc-8m9nb [657.797618ms]
Mar  5 07:49:05.334: INFO: Got endpoints: latency-svc-wq2sb [658.150082ms]
Mar  5 07:49:05.335: INFO: Got endpoints: latency-svc-nx5zj [658.442894ms]
Mar  5 07:49:05.335: INFO: Got endpoints: latency-svc-v7g86 [658.931591ms]
Mar  5 07:49:05.365: INFO: Created: latency-svc-xbp6m
Mar  5 07:49:05.392: INFO: Created: latency-svc-htjgh
Mar  5 07:49:05.406: INFO: Created: latency-svc-n6hz6
Mar  5 07:49:05.425: INFO: Created: latency-svc-mvwsk
Mar  5 07:49:05.921: INFO: Got endpoints: latency-svc-f7psk [1.244987908s]
Mar  5 07:49:05.984: INFO: Got endpoints: latency-svc-8jw4z [1.308554821s]
Mar  5 07:49:05.985: INFO: Got endpoints: latency-svc-lwwst [1.309068604s]
Mar  5 07:49:05.984: INFO: Got endpoints: latency-svc-kwhq8 [1.308261045s]
Mar  5 07:49:05.984: INFO: Got endpoints: latency-svc-f9nbr [1.308316799s]
Mar  5 07:49:05.993: INFO: Created: latency-svc-5x2kj
Mar  5 07:49:06.013: INFO: Created: latency-svc-9dq2f
Mar  5 07:49:06.031: INFO: Got endpoints: latency-svc-zmwlr [1.355133065s]
Mar  5 07:49:06.037: INFO: Created: latency-svc-47r28
Mar  5 07:49:06.056: INFO: Created: latency-svc-hgthg
Mar  5 07:49:06.073: INFO: Got endpoints: latency-svc-zfpb7 [1.397358472s]
Mar  5 07:49:06.087: INFO: Got endpoints: latency-svc-hrzwh [1.295015177s]
Mar  5 07:49:06.095: INFO: Created: latency-svc-tjdn5
Mar  5 07:49:06.095: INFO: Got endpoints: latency-svc-cxxdw [1.298850381s]
Mar  5 07:49:06.096: INFO: Got endpoints: latency-svc-zjd57 [1.30277312s]
Mar  5 07:49:06.136: INFO: Got endpoints: latency-svc-xbp6m [802.438614ms]
Mar  5 07:49:06.137: INFO: Got endpoints: latency-svc-htjgh [802.482731ms]
Mar  5 07:49:06.136: INFO: Got endpoints: latency-svc-n6hz6 [801.726059ms]
Mar  5 07:49:06.136: INFO: Got endpoints: latency-svc-h7rdz [849.639955ms]
Mar  5 07:49:06.137: INFO: Got endpoints: latency-svc-mvwsk [802.138549ms]
Mar  5 07:49:06.137: INFO: Created: latency-svc-fq6mc
Mar  5 07:49:06.166: INFO: Created: latency-svc-tjftx
Mar  5 07:49:06.175: INFO: Got endpoints: latency-svc-5x2kj [253.612326ms]
Mar  5 07:49:06.180: INFO: Got endpoints: latency-svc-47r28 [194.573952ms]
Mar  5 07:49:06.180: INFO: Got endpoints: latency-svc-tjdn5 [194.158474ms]
Mar  5 07:49:06.180: INFO: Got endpoints: latency-svc-9dq2f [195.40152ms]
Mar  5 07:49:06.180: INFO: Got endpoints: latency-svc-hgthg [194.215878ms]
Mar  5 07:49:06.201: INFO: Created: latency-svc-8n7qm
Mar  5 07:49:06.206: INFO: Got endpoints: latency-svc-fq6mc [174.251178ms]
Mar  5 07:49:06.208: INFO: Got endpoints: latency-svc-tjftx [135.009252ms]
Mar  5 07:49:06.226: INFO: Got endpoints: latency-svc-8n7qm [139.342711ms]
Mar  5 07:49:06.244: INFO: Created: latency-svc-t8mmw
Mar  5 07:49:06.286: INFO: Got endpoints: latency-svc-t8mmw [190.462498ms]
Mar  5 07:49:06.316: INFO: Created: latency-svc-xt7q4
Mar  5 07:49:06.326: INFO: Got endpoints: latency-svc-xt7q4 [230.539176ms]
Mar  5 07:49:06.520: INFO: Created: latency-svc-8p6nl
Mar  5 07:49:06.593: INFO: Got endpoints: latency-svc-8p6nl [456.17041ms]
Mar  5 07:49:06.600: INFO: Created: latency-svc-ssh42
Mar  5 07:49:06.608: INFO: Got endpoints: latency-svc-ssh42 [470.971024ms]
Mar  5 07:49:06.628: INFO: Created: latency-svc-7npmk
Mar  5 07:49:06.648: INFO: Got endpoints: latency-svc-7npmk [504.588842ms]
Mar  5 07:49:06.675: INFO: Created: latency-svc-nb9bk
Mar  5 07:49:06.724: INFO: Created: latency-svc-4lclz
Mar  5 07:49:06.731: INFO: Created: latency-svc-pmdgp
Mar  5 07:49:06.772: INFO: Created: latency-svc-5g66b
Mar  5 07:49:06.788: INFO: Created: latency-svc-bwb4m
Mar  5 07:49:06.807: INFO: Created: latency-svc-c2bqd
Mar  5 07:49:06.831: INFO: Created: latency-svc-mhfzz
Mar  5 07:49:06.853: INFO: Created: latency-svc-fszl4
Mar  5 07:49:06.876: INFO: Created: latency-svc-9gjqp
Mar  5 07:49:06.911: INFO: Created: latency-svc-mzm86
Mar  5 07:49:06.930: INFO: Created: latency-svc-jzb8f
Mar  5 07:49:06.977: INFO: Created: latency-svc-hv4cs
Mar  5 07:49:07.005: INFO: Created: latency-svc-9kj8b
Mar  5 07:49:07.022: INFO: Created: latency-svc-pgs7r
Mar  5 07:49:07.041: INFO: Created: latency-svc-rmh6s
Mar  5 07:49:07.122: INFO: Got endpoints: latency-svc-nb9bk [978.145398ms]
Mar  5 07:49:07.122: INFO: Got endpoints: latency-svc-4lclz [977.339486ms]
Mar  5 07:49:07.123: INFO: Got endpoints: latency-svc-pmdgp [948.346623ms]
Mar  5 07:49:07.123: INFO: Got endpoints: latency-svc-5g66b [943.003055ms]
Mar  5 07:49:07.123: INFO: Got endpoints: latency-svc-bwb4m [942.865057ms]
Mar  5 07:49:07.144: INFO: Got endpoints: latency-svc-c2bqd [963.427712ms]
Mar  5 07:49:07.196: INFO: Created: latency-svc-z7pfc
Mar  5 07:49:07.230: INFO: Got endpoints: latency-svc-mhfzz [1.04970915s]
Mar  5 07:49:07.232: INFO: Got endpoints: latency-svc-fszl4 [1.023418814s]
Mar  5 07:49:07.233: INFO: Got endpoints: latency-svc-jzb8f [946.025248ms]
Mar  5 07:49:07.233: INFO: Got endpoints: latency-svc-9gjqp [1.026182595s]
Mar  5 07:49:07.233: INFO: Got endpoints: latency-svc-mzm86 [1.006692087s]
Mar  5 07:49:07.250: INFO: Created: latency-svc-8sr4t
Mar  5 07:49:07.273: INFO: Got endpoints: latency-svc-9kj8b [680.349281ms]
Mar  5 07:49:07.275: INFO: Got endpoints: latency-svc-z7pfc [151.251003ms]
Mar  5 07:49:07.296: INFO: Got endpoints: latency-svc-pgs7r [688.220148ms]
Mar  5 07:49:07.296: INFO: Got endpoints: latency-svc-hv4cs [970.269459ms]
Mar  5 07:49:07.310: INFO: Created: latency-svc-5tjcg
Mar  5 07:49:07.311: INFO: Got endpoints: latency-svc-rmh6s [662.720855ms]
Mar  5 07:49:07.592: INFO: Got endpoints: latency-svc-8sr4t [467.858953ms]
Mar  5 07:49:07.599: INFO: Got endpoints: latency-svc-5tjcg [474.938005ms]
Mar  5 07:49:07.651: INFO: Created: latency-svc-4hqn5
Mar  5 07:49:07.693: INFO: Got endpoints: latency-svc-4hqn5 [570.993737ms]
Mar  5 07:49:07.711: INFO: Created: latency-svc-vtwh5
Mar  5 07:49:07.742: INFO: Created: latency-svc-rtz22
Mar  5 07:49:07.756: INFO: Created: latency-svc-6d5hg
Mar  5 07:49:07.783: INFO: Got endpoints: latency-svc-vtwh5 [661.056714ms]
Mar  5 07:49:07.801: INFO: Created: latency-svc-9chlg
Mar  5 07:49:07.819: INFO: Got endpoints: latency-svc-6d5hg [588.905162ms]
Mar  5 07:49:07.823: INFO: Created: latency-svc-vlrct
Mar  5 07:49:07.824: INFO: Got endpoints: latency-svc-rtz22 [679.718503ms]
Mar  5 07:49:07.830: INFO: Got endpoints: latency-svc-9chlg [597.619492ms]
Mar  5 07:49:07.852: INFO: Got endpoints: latency-svc-vlrct [618.638759ms]
Mar  5 07:49:07.905: INFO: Created: latency-svc-wb897
Mar  5 07:49:07.923: INFO: Created: latency-svc-kqjj2
Mar  5 07:49:07.951: INFO: Created: latency-svc-l8pkj
Mar  5 07:49:07.951: INFO: Got endpoints: latency-svc-wb897 [718.82247ms]
Mar  5 07:49:07.965: INFO: Got endpoints: latency-svc-kqjj2 [731.494528ms]
Mar  5 07:49:07.986: INFO: Got endpoints: latency-svc-l8pkj [712.777755ms]
Mar  5 07:49:07.992: INFO: Created: latency-svc-zcsbs
Mar  5 07:49:08.012: INFO: Created: latency-svc-zx56f
Mar  5 07:49:08.039: INFO: Got endpoints: latency-svc-zx56f [742.676411ms]
Mar  5 07:49:08.039: INFO: Got endpoints: latency-svc-zcsbs [764.129076ms]
Mar  5 07:49:08.045: INFO: Created: latency-svc-xvd5s
Mar  5 07:49:08.070: INFO: Got endpoints: latency-svc-xvd5s [773.134645ms]
Mar  5 07:49:08.070: INFO: Created: latency-svc-smxd5
Mar  5 07:49:08.078: INFO: Created: latency-svc-rmzff
Mar  5 07:49:08.099: INFO: Created: latency-svc-j2k6s
Mar  5 07:49:08.112: INFO: Created: latency-svc-rjbnd
Mar  5 07:49:08.127: INFO: Created: latency-svc-mtpxn
Mar  5 07:49:08.142: INFO: Created: latency-svc-p779t
Mar  5 07:49:08.161: INFO: Created: latency-svc-4h4ll
Mar  5 07:49:08.187: INFO: Created: latency-svc-675x9
Mar  5 07:49:08.194: INFO: Got endpoints: latency-svc-smxd5 [882.729985ms]
Mar  5 07:49:08.194: INFO: Got endpoints: latency-svc-rjbnd [500.845059ms]
Mar  5 07:49:08.196: INFO: Got endpoints: latency-svc-rmzff [604.129506ms]
Mar  5 07:49:08.197: INFO: Got endpoints: latency-svc-j2k6s [598.616858ms]
Mar  5 07:49:08.198: INFO: Got endpoints: latency-svc-mtpxn [414.612027ms]
Mar  5 07:49:08.217: INFO: Created: latency-svc-srtsp
Mar  5 07:49:08.242: INFO: Created: latency-svc-vqt5t
Mar  5 07:49:08.267: INFO: Got endpoints: latency-svc-4h4ll [443.330615ms]
Mar  5 07:49:08.268: INFO: Got endpoints: latency-svc-srtsp [415.714877ms]
Mar  5 07:49:08.271: INFO: Got endpoints: latency-svc-675x9 [440.976773ms]
Mar  5 07:49:08.272: INFO: Got endpoints: latency-svc-p779t [452.320602ms]
Mar  5 07:49:08.272: INFO: Got endpoints: latency-svc-vqt5t [73.905523ms]
Mar  5 07:49:08.308: INFO: Created: latency-svc-mflqf
Mar  5 07:49:08.332: INFO: Got endpoints: latency-svc-mflqf [380.569165ms]
Mar  5 07:49:08.342: INFO: Created: latency-svc-v9c4s
Mar  5 07:49:08.356: INFO: Got endpoints: latency-svc-v9c4s [391.510037ms]
Mar  5 07:49:08.368: INFO: Created: latency-svc-xdv44
Mar  5 07:49:08.383: INFO: Got endpoints: latency-svc-xdv44 [397.420942ms]
Mar  5 07:49:08.392: INFO: Created: latency-svc-8sl5f
Mar  5 07:49:08.403: INFO: Got endpoints: latency-svc-8sl5f [364.355287ms]
Mar  5 07:49:08.408: INFO: Created: latency-svc-wgkrw
Mar  5 07:49:08.424: INFO: Got endpoints: latency-svc-wgkrw [383.614795ms]
Mar  5 07:49:08.431: INFO: Created: latency-svc-75jph
Mar  5 07:49:08.441: INFO: Got endpoints: latency-svc-75jph [371.443987ms]
Mar  5 07:49:08.464: INFO: Created: latency-svc-kp7k6
Mar  5 07:49:08.469: INFO: Got endpoints: latency-svc-kp7k6 [275.341995ms]
Mar  5 07:49:08.497: INFO: Created: latency-svc-mf2j9
Mar  5 07:49:08.514: INFO: Got endpoints: latency-svc-mf2j9 [319.147375ms]
Mar  5 07:49:08.518: INFO: Created: latency-svc-pcsx7
Mar  5 07:49:08.544: INFO: Got endpoints: latency-svc-pcsx7 [348.037006ms]
Mar  5 07:49:08.545: INFO: Created: latency-svc-g7gqb
Mar  5 07:49:08.561: INFO: Got endpoints: latency-svc-g7gqb [363.286403ms]
Mar  5 07:49:08.573: INFO: Created: latency-svc-cd6xm
Mar  5 07:49:08.589: INFO: Created: latency-svc-shqhq
Mar  5 07:49:08.600: INFO: Got endpoints: latency-svc-cd6xm [332.401817ms]
Mar  5 07:49:08.610: INFO: Created: latency-svc-njsdv
Mar  5 07:49:08.616: INFO: Got endpoints: latency-svc-shqhq [348.657915ms]
Mar  5 07:49:08.623: INFO: Got endpoints: latency-svc-njsdv [352.486237ms]
Mar  5 07:49:08.632: INFO: Created: latency-svc-wjf5m
Mar  5 07:49:08.655: INFO: Got endpoints: latency-svc-wjf5m [382.572894ms]
Mar  5 07:49:08.656: INFO: Created: latency-svc-b45jc
Mar  5 07:49:08.682: INFO: Created: latency-svc-5gzpt
Mar  5 07:49:08.696: INFO: Got endpoints: latency-svc-b45jc [423.888973ms]
Mar  5 07:49:08.702: INFO: Got endpoints: latency-svc-5gzpt [369.766203ms]
Mar  5 07:49:08.716: INFO: Created: latency-svc-wh2wc
Mar  5 07:49:08.736: INFO: Got endpoints: latency-svc-wh2wc [379.247943ms]
Mar  5 07:49:08.754: INFO: Created: latency-svc-mhjhx
Mar  5 07:49:08.766: INFO: Got endpoints: latency-svc-mhjhx [382.407493ms]
Mar  5 07:49:08.782: INFO: Created: latency-svc-fxc99
Mar  5 07:49:08.810: INFO: Got endpoints: latency-svc-fxc99 [406.893828ms]
Mar  5 07:49:08.814: INFO: Created: latency-svc-c6kp9
Mar  5 07:49:08.850: INFO: Got endpoints: latency-svc-c6kp9 [426.734893ms]
Mar  5 07:49:08.873: INFO: Created: latency-svc-pvz84
Mar  5 07:49:08.880: INFO: Got endpoints: latency-svc-pvz84 [438.81059ms]
Mar  5 07:49:08.898: INFO: Created: latency-svc-zk4nq
Mar  5 07:49:08.932: INFO: Created: latency-svc-4bb5s
Mar  5 07:49:08.935: INFO: Got endpoints: latency-svc-zk4nq [465.264049ms]
Mar  5 07:49:08.969: INFO: Created: latency-svc-kxfwj
Mar  5 07:49:09.068: INFO: Got endpoints: latency-svc-kxfwj [524.119215ms]
Mar  5 07:49:09.075: INFO: Got endpoints: latency-svc-4bb5s [561.747949ms]
Mar  5 07:49:09.110: INFO: Created: latency-svc-ptrwc
Mar  5 07:49:09.180: INFO: Got endpoints: latency-svc-ptrwc [619.708358ms]
Mar  5 07:49:09.467: INFO: Created: latency-svc-2jwz7
Mar  5 07:49:09.488: INFO: Created: latency-svc-t5g5x
Mar  5 07:49:09.528: INFO: Created: latency-svc-pc7gx
Mar  5 07:49:09.531: INFO: Got endpoints: latency-svc-2jwz7 [930.677188ms]
Mar  5 07:49:09.533: INFO: Got endpoints: latency-svc-t5g5x [916.334767ms]
Mar  5 07:49:09.563: INFO: Created: latency-svc-jzm2z
Mar  5 07:49:09.581: INFO: Created: latency-svc-7s5t7
Mar  5 07:49:09.600: INFO: Created: latency-svc-p2pm6
Mar  5 07:49:09.624: INFO: Created: latency-svc-ms7zj
Mar  5 07:49:09.643: INFO: Created: latency-svc-vnxkh
Mar  5 07:49:09.680: INFO: Got endpoints: latency-svc-pc7gx [1.055949405s]
Mar  5 07:49:09.687: INFO: Got endpoints: latency-svc-jzm2z [1.031615575s]
Mar  5 07:49:09.688: INFO: Created: latency-svc-2xtpj
Mar  5 07:49:09.688: INFO: Got endpoints: latency-svc-7s5t7 [991.565287ms]
Mar  5 07:49:09.708: INFO: Created: latency-svc-5lc4k
Mar  5 07:49:09.726: INFO: Created: latency-svc-z5t4n
Mar  5 07:49:09.750: INFO: Created: latency-svc-lgh2h
Mar  5 07:49:09.766: INFO: Created: latency-svc-zzk52
Mar  5 07:49:09.787: INFO: Created: latency-svc-8hlzd
Mar  5 07:49:09.809: INFO: Created: latency-svc-czz8x
Mar  5 07:49:09.830: INFO: Created: latency-svc-g45r4
Mar  5 07:49:09.854: INFO: Created: latency-svc-xrjns
Mar  5 07:49:09.881: INFO: Created: latency-svc-d5kwj
Mar  5 07:49:09.895: INFO: Created: latency-svc-p4lx7
Mar  5 07:49:09.913: INFO: Created: latency-svc-7nr9v
Mar  5 07:49:10.355: INFO: Got endpoints: latency-svc-ms7zj [1.618959103s]
Mar  5 07:49:10.358: INFO: Got endpoints: latency-svc-vnxkh [1.591753895s]
Mar  5 07:49:10.358: INFO: Got endpoints: latency-svc-p2pm6 [1.656143801s]
Mar  5 07:49:10.363: INFO: Got endpoints: latency-svc-2xtpj [1.552406506s]
Mar  5 07:49:10.388: INFO: Created: latency-svc-nqbkt
Mar  5 07:49:10.412: INFO: Created: latency-svc-qhd9b
Mar  5 07:49:10.433: INFO: Created: latency-svc-s96jp
Mar  5 07:49:10.450: INFO: Created: latency-svc-f59x8
Mar  5 07:49:10.537: INFO: Got endpoints: latency-svc-5lc4k [1.686223348s]
Mar  5 07:49:10.538: INFO: Got endpoints: latency-svc-zzk52 [1.462016119s]
Mar  5 07:49:10.538: INFO: Got endpoints: latency-svc-z5t4n [1.658163311s]
Mar  5 07:49:10.544: INFO: Got endpoints: latency-svc-lgh2h [1.609335223s]
Mar  5 07:49:10.546: INFO: Got endpoints: latency-svc-8hlzd [1.458187341s]
Mar  5 07:49:10.579: INFO: Created: latency-svc-pgzr8
Mar  5 07:49:10.614: INFO: Created: latency-svc-69vtl
Mar  5 07:49:10.639: INFO: Got endpoints: latency-svc-g45r4 [1.107846362s]
Mar  5 07:49:10.649: INFO: Got endpoints: latency-svc-p4lx7 [961.946448ms]
Mar  5 07:49:10.650: INFO: Got endpoints: latency-svc-czz8x [1.469838929s]
Mar  5 07:49:10.654: INFO: Got endpoints: latency-svc-xrjns [1.120618478s]
Mar  5 07:49:10.654: INFO: Got endpoints: latency-svc-d5kwj [974.015038ms]
Mar  5 07:49:10.672: INFO: Created: latency-svc-xbl44
Mar  5 07:49:10.696: INFO: Created: latency-svc-lj9k8
Mar  5 07:49:10.703: INFO: Created: latency-svc-tjsgq
Mar  5 07:49:10.721: INFO: Created: latency-svc-cch4z
Mar  5 07:49:10.746: INFO: Created: latency-svc-ng5ck
Mar  5 07:49:10.766: INFO: Created: latency-svc-vrzpl
Mar  5 07:49:10.775: INFO: Created: latency-svc-f7g2r
Mar  5 07:49:10.799: INFO: Created: latency-svc-dpjpv
Mar  5 07:49:11.042: INFO: Got endpoints: latency-svc-f59x8 [678.875936ms]
Mar  5 07:49:11.043: INFO: Got endpoints: latency-svc-7nr9v [1.354802338s]
Mar  5 07:49:11.047: INFO: Got endpoints: latency-svc-qhd9b [688.040916ms]
Mar  5 07:49:11.047: INFO: Got endpoints: latency-svc-s96jp [688.977467ms]
Mar  5 07:49:11.048: INFO: Got endpoints: latency-svc-nqbkt [692.487933ms]
Mar  5 07:49:11.078: INFO: Created: latency-svc-fr27b
Mar  5 07:49:11.101: INFO: Created: latency-svc-5jpvs
Mar  5 07:49:11.114: INFO: Got endpoints: latency-svc-xbl44 [576.054936ms]
Mar  5 07:49:11.115: INFO: Got endpoints: latency-svc-tjsgq [460.835711ms]
Mar  5 07:49:11.116: INFO: Got endpoints: latency-svc-lj9k8 [462.266335ms]
Mar  5 07:49:11.117: INFO: Got endpoints: latency-svc-pgzr8 [579.540407ms]
Mar  5 07:49:11.119: INFO: Got endpoints: latency-svc-69vtl [581.501507ms]
Mar  5 07:49:11.124: INFO: Created: latency-svc-tmlj2
Mar  5 07:49:11.142: INFO: Created: latency-svc-p72tb
Mar  5 07:49:11.172: INFO: Created: latency-svc-8d2dj
Mar  5 07:49:11.188: INFO: Got endpoints: latency-svc-f7g2r [537.434219ms]
Mar  5 07:49:11.188: INFO: Got endpoints: latency-svc-cch4z [639.796625ms]
Mar  5 07:49:11.191: INFO: Got endpoints: latency-svc-ng5ck [552.518434ms]
Mar  5 07:49:11.192: INFO: Got endpoints: latency-svc-dpjpv [647.284063ms]
Mar  5 07:49:11.192: INFO: Got endpoints: latency-svc-vrzpl [542.804284ms]
Mar  5 07:49:11.217: INFO: Created: latency-svc-8zpjs
Mar  5 07:49:11.227: INFO: Got endpoints: latency-svc-fr27b [184.754363ms]
Mar  5 07:49:11.230: INFO: Got endpoints: latency-svc-tmlj2 [183.182605ms]
Mar  5 07:49:11.230: INFO: Got endpoints: latency-svc-p72tb [183.165429ms]
Mar  5 07:49:11.231: INFO: Got endpoints: latency-svc-8d2dj [182.769855ms]
Mar  5 07:49:11.231: INFO: Got endpoints: latency-svc-5jpvs [187.536292ms]
Mar  5 07:49:11.345: INFO: Got endpoints: latency-svc-8zpjs [230.480004ms]
Mar  5 07:49:11.352: INFO: Created: latency-svc-pvlqq
Mar  5 07:49:11.385: INFO: Created: latency-svc-p6rxg
Mar  5 07:49:11.402: INFO: Created: latency-svc-42tjw
Mar  5 07:49:11.429: INFO: Got endpoints: latency-svc-p6rxg [312.901365ms]
Mar  5 07:49:11.430: INFO: Got endpoints: latency-svc-pvlqq [315.154061ms]
Mar  5 07:49:11.431: INFO: Got endpoints: latency-svc-42tjw [313.901864ms]
Mar  5 07:49:11.434: INFO: Created: latency-svc-pjh5n
Mar  5 07:49:11.455: INFO: Created: latency-svc-c4nqg
Mar  5 07:49:11.478: INFO: Created: latency-svc-k9rwf
Mar  5 07:49:11.501: INFO: Created: latency-svc-49fzc
Mar  5 07:49:11.517: INFO: Created: latency-svc-5sxsm
Mar  5 07:49:11.530: INFO: Created: latency-svc-cccz2
Mar  5 07:49:11.559: INFO: Created: latency-svc-pkbzn
Mar  5 07:49:11.582: INFO: Created: latency-svc-blnmv
Mar  5 07:49:11.619: INFO: Created: latency-svc-9g54v
Mar  5 07:49:11.700: INFO: Created: latency-svc-flfkw
Mar  5 07:49:11.732: INFO: Created: latency-svc-fm4dm
Mar  5 07:49:11.765: INFO: Created: latency-svc-8c5tz
Mar  5 07:49:11.782: INFO: Got endpoints: latency-svc-pjh5n [663.031311ms]
Mar  5 07:49:11.815: INFO: Got endpoints: latency-svc-c4nqg [626.863923ms]
Mar  5 07:49:11.815: INFO: Created: latency-svc-4hbwz
Mar  5 07:49:11.822: INFO: Got endpoints: latency-svc-5sxsm [630.464635ms]
Mar  5 07:49:11.823: INFO: Got endpoints: latency-svc-k9rwf [634.64984ms]
Mar  5 07:49:11.827: INFO: Got endpoints: latency-svc-49fzc [634.898718ms]
Mar  5 07:49:11.829: INFO: Created: latency-svc-qvwnz
Mar  5 07:49:11.852: INFO: Got endpoints: latency-svc-cccz2 [660.539309ms]
Mar  5 07:49:11.871: INFO: Created: latency-svc-mhxg2
Mar  5 07:49:11.894: INFO: Got endpoints: latency-svc-pkbzn [667.647819ms]
Mar  5 07:49:11.922: INFO: Created: latency-svc-zqtnl
Mar  5 07:49:11.936: INFO: Created: latency-svc-dlf4r
Mar  5 07:49:11.954: INFO: Created: latency-svc-p6v4r
Mar  5 07:49:11.954: INFO: Got endpoints: latency-svc-blnmv [723.624926ms]
Mar  5 07:49:11.986: INFO: Created: latency-svc-jdwdb
Mar  5 07:49:12.015: INFO: Created: latency-svc-rvw5x
Mar  5 07:49:12.044: INFO: Created: latency-svc-4x5b6
Mar  5 07:49:12.070: INFO: Created: latency-svc-db4vz
Mar  5 07:49:12.090: INFO: Created: latency-svc-h8vwj
Mar  5 07:49:12.112: INFO: Got endpoints: latency-svc-9g54v [880.604607ms]
Mar  5 07:49:12.115: INFO: Got endpoints: latency-svc-fm4dm [883.185977ms]
Mar  5 07:49:12.116: INFO: Got endpoints: latency-svc-8c5tz [770.954815ms]
Mar  5 07:49:12.117: INFO: Got endpoints: latency-svc-4hbwz [687.286893ms]
Mar  5 07:49:12.116: INFO: Got endpoints: latency-svc-flfkw [884.686504ms]
Mar  5 07:49:12.184: INFO: Created: latency-svc-hvqtm
Mar  5 07:49:12.432: INFO: Got endpoints: latency-svc-qvwnz [1.001604049s]
Mar  5 07:49:12.432: INFO: Got endpoints: latency-svc-zqtnl [649.389104ms]
Mar  5 07:49:12.432: INFO: Got endpoints: latency-svc-dlf4r [616.716791ms]
Mar  5 07:49:12.433: INFO: Got endpoints: latency-svc-mhxg2 [1.002187465s]
Mar  5 07:49:12.433: INFO: Got endpoints: latency-svc-p6v4r [610.538222ms]
Mar  5 07:49:12.489: INFO: Created: latency-svc-gsplr
Mar  5 07:49:12.498: INFO: Got endpoints: latency-svc-h8vwj [543.854308ms]
Mar  5 07:49:12.499: INFO: Got endpoints: latency-svc-rvw5x [672.275385ms]
Mar  5 07:49:12.500: INFO: Got endpoints: latency-svc-db4vz [605.232059ms]
Mar  5 07:49:12.500: INFO: Got endpoints: latency-svc-4x5b6 [648.228783ms]
Mar  5 07:49:12.500: INFO: Got endpoints: latency-svc-jdwdb [676.937865ms]
Mar  5 07:49:12.705: INFO: Got endpoints: latency-svc-gsplr [589.009947ms]
Mar  5 07:49:12.706: INFO: Got endpoints: latency-svc-hvqtm [593.32402ms]
Mar  5 07:49:12.707: INFO: Created: latency-svc-t2xvh
Mar  5 07:49:12.728: INFO: Got endpoints: latency-svc-t2xvh [611.315028ms]
Mar  5 07:49:12.754: INFO: Created: latency-svc-bmb2x
Mar  5 07:49:12.802: INFO: Got endpoints: latency-svc-bmb2x [686.883043ms]
Mar  5 07:49:12.851: INFO: Created: latency-svc-vzzz6
Mar  5 07:49:12.882: INFO: Got endpoints: latency-svc-vzzz6 [765.236889ms]
Mar  5 07:49:12.893: INFO: Created: latency-svc-2wktn
Mar  5 07:49:12.915: INFO: Created: latency-svc-gt6bs
Mar  5 07:49:12.933: INFO: Created: latency-svc-r5jwx
Mar  5 07:49:12.952: INFO: Got endpoints: latency-svc-gt6bs [519.909013ms]
Mar  5 07:49:12.952: INFO: Got endpoints: latency-svc-2wktn [520.163443ms]
Mar  5 07:49:12.962: INFO: Created: latency-svc-42fsz
Mar  5 07:49:12.976: INFO: Got endpoints: latency-svc-r5jwx [542.065562ms]
Mar  5 07:49:12.988: INFO: Created: latency-svc-7xpck
Mar  5 07:49:13.005: INFO: Got endpoints: latency-svc-42fsz [571.738826ms]
Mar  5 07:49:13.016: INFO: Created: latency-svc-zl5sf
Mar  5 07:49:13.043: INFO: Created: latency-svc-6qq72
Mar  5 07:49:13.106: INFO: Created: latency-svc-q9zcz
Mar  5 07:49:13.111: INFO: Got endpoints: latency-svc-6qq72 [612.339396ms]
Mar  5 07:49:13.111: INFO: Got endpoints: latency-svc-zl5sf [611.615353ms]
Mar  5 07:49:13.111: INFO: Got endpoints: latency-svc-7xpck [678.363007ms]
Mar  5 07:49:13.116: INFO: Got endpoints: latency-svc-q9zcz [616.042901ms]
Mar  5 07:49:13.132: INFO: Created: latency-svc-n49pl
Mar  5 07:49:13.144: INFO: Got endpoints: latency-svc-n49pl [643.275655ms]
Mar  5 07:49:13.187: INFO: Created: latency-svc-xxmrp
Mar  5 07:49:13.199: INFO: Created: latency-svc-62hdq
Mar  5 07:49:13.237: INFO: Got endpoints: latency-svc-xxmrp [736.731857ms]
Mar  5 07:49:13.238: INFO: Got endpoints: latency-svc-62hdq [531.776617ms]
Mar  5 07:49:13.253: INFO: Created: latency-svc-h76dt
Mar  5 07:49:13.262: INFO: Got endpoints: latency-svc-h76dt [556.178789ms]
Mar  5 07:49:13.268: INFO: Created: latency-svc-wftpp
Mar  5 07:49:13.288: INFO: Created: latency-svc-p7sm4
Mar  5 07:49:13.305: INFO: Created: latency-svc-9szsf
Mar  5 07:49:13.326: INFO: Created: latency-svc-v4chd
Mar  5 07:49:13.346: INFO: Created: latency-svc-g8hz2
Mar  5 07:49:13.362: INFO: Created: latency-svc-vmdb7
Mar  5 07:49:13.392: INFO: Created: latency-svc-sq99z
Mar  5 07:49:13.407: INFO: Created: latency-svc-cqz77
Mar  5 07:49:13.418: INFO: Created: latency-svc-xzww9
Mar  5 07:49:13.436: INFO: Created: latency-svc-v2nv2
Mar  5 07:49:13.454: INFO: Created: latency-svc-dgspf
Mar  5 07:49:13.468: INFO: Created: latency-svc-bhshw
Mar  5 07:49:13.496: INFO: Created: latency-svc-p56k5
Mar  5 07:49:13.514: INFO: Created: latency-svc-8zbxv
Mar  5 07:49:13.557: INFO: Created: latency-svc-f74vn
Mar  5 07:49:13.818: INFO: Got endpoints: latency-svc-wftpp [1.089997898s]
Mar  5 07:49:13.821: INFO: Got endpoints: latency-svc-p7sm4 [1.019046485s]
Mar  5 07:49:13.852: INFO: Got endpoints: latency-svc-9szsf [969.908377ms]
Mar  5 07:49:13.863: INFO: Created: latency-svc-4tt6q
Mar  5 07:49:13.947: INFO: Got endpoints: latency-svc-v4chd [995.45375ms]
Mar  5 07:49:13.950: INFO: Got endpoints: latency-svc-cqz77 [838.30928ms]
Mar  5 07:49:13.950: INFO: Got endpoints: latency-svc-sq99z [945.308556ms]
Mar  5 07:49:13.950: INFO: Got endpoints: latency-svc-g8hz2 [998.145507ms]
Mar  5 07:49:13.950: INFO: Got endpoints: latency-svc-vmdb7 [972.337591ms]
Mar  5 07:49:14.164: INFO: Got endpoints: latency-svc-p56k5 [927.438874ms]
Mar  5 07:49:14.167: INFO: Got endpoints: latency-svc-v2nv2 [1.054704485s]
Mar  5 07:49:14.167: INFO: Got endpoints: latency-svc-xzww9 [1.055666502s]
Mar  5 07:49:14.170: INFO: Got endpoints: latency-svc-bhshw [1.026401951s]
Mar  5 07:49:14.170: INFO: Got endpoints: latency-svc-dgspf [1.054364914s]
Mar  5 07:49:14.419: INFO: Got endpoints: latency-svc-f74vn [1.15682307s]
Mar  5 07:49:14.422: INFO: Got endpoints: latency-svc-4tt6q [604.042954ms]
Mar  5 07:49:14.423: INFO: Got endpoints: latency-svc-8zbxv [1.185152756s]
Mar  5 07:49:14.423: INFO: Latencies: [73.905523ms 116.997613ms 117.804097ms 121.176548ms 135.009252ms 139.342711ms 151.251003ms 174.251178ms 182.769855ms 183.165429ms 183.182605ms 184.754363ms 187.536292ms 190.462498ms 194.158474ms 194.215878ms 194.573952ms 195.40152ms 230.480004ms 230.539176ms 253.612326ms 275.341995ms 312.901365ms 313.901864ms 315.154061ms 319.147375ms 332.401817ms 348.037006ms 348.657915ms 352.486237ms 363.286403ms 364.355287ms 369.766203ms 371.443987ms 379.247943ms 380.569165ms 382.407493ms 382.572894ms 383.614795ms 391.510037ms 397.420942ms 406.893828ms 414.612027ms 415.714877ms 423.888973ms 426.734893ms 438.81059ms 440.976773ms 443.330615ms 452.320602ms 456.17041ms 460.835711ms 462.266335ms 465.264049ms 467.858953ms 470.971024ms 474.938005ms 500.845059ms 504.588842ms 519.909013ms 520.163443ms 524.119215ms 531.776617ms 537.434219ms 542.065562ms 542.804284ms 543.854308ms 552.518434ms 556.178789ms 561.747949ms 570.993737ms 571.738826ms 576.054936ms 579.540407ms 581.501507ms 588.905162ms 589.009947ms 593.32402ms 597.619492ms 598.616858ms 604.042954ms 604.129506ms 605.232059ms 610.538222ms 610.908259ms 611.315028ms 611.615353ms 612.339396ms 616.042901ms 616.716791ms 618.638759ms 619.708358ms 626.863923ms 630.464635ms 634.64984ms 634.898718ms 639.796625ms 643.275655ms 647.284063ms 648.228783ms 649.389104ms 657.797618ms 658.150082ms 658.442894ms 658.931591ms 660.539309ms 661.056714ms 662.720855ms 663.031311ms 667.647819ms 672.275385ms 676.937865ms 678.363007ms 678.875936ms 679.718503ms 680.349281ms 686.883043ms 687.286893ms 688.040916ms 688.220148ms 688.977467ms 692.487933ms 712.777755ms 718.82247ms 723.624926ms 731.494528ms 736.731857ms 742.676411ms 764.129076ms 765.236889ms 770.954815ms 773.134645ms 801.726059ms 802.138549ms 802.438614ms 802.482731ms 838.30928ms 849.639955ms 880.604607ms 882.729985ms 883.185977ms 884.686504ms 916.334767ms 927.438874ms 930.677188ms 942.865057ms 943.003055ms 945.308556ms 946.025248ms 948.346623ms 961.946448ms 963.427712ms 969.908377ms 970.269459ms 972.337591ms 974.015038ms 977.339486ms 978.145398ms 991.565287ms 995.45375ms 998.145507ms 1.001604049s 1.002187465s 1.006692087s 1.019046485s 1.023418814s 1.026182595s 1.026401951s 1.031615575s 1.04970915s 1.054364914s 1.054704485s 1.055666502s 1.055949405s 1.089997898s 1.107846362s 1.120618478s 1.15682307s 1.185152756s 1.244987908s 1.295015177s 1.298850381s 1.30277312s 1.308261045s 1.308316799s 1.308554821s 1.309068604s 1.354802338s 1.355133065s 1.397358472s 1.458187341s 1.462016119s 1.469838929s 1.552406506s 1.591753895s 1.609335223s 1.618959103s 1.656143801s 1.658163311s 1.686223348s]
Mar  5 07:49:14.423: INFO: 50 %ile: 649.389104ms
Mar  5 07:49:14.423: INFO: 90 %ile: 1.295015177s
Mar  5 07:49:14.423: INFO: 99 %ile: 1.658163311s
Mar  5 07:49:14.423: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:49:14.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4085" for this suite.

â€¢ [SLOW TEST:12.681 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":277,"completed":8,"skipped":184,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:49:14.461: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6571
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar  5 07:49:15.025: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  5 07:49:15.055: INFO: Waiting for terminating namespaces to be deleted...
Mar  5 07:49:15.066: INFO: 
Logging pods the kubelet thinks is on node devops-control-plane-1 before test
Mar  5 07:49:15.115: INFO: kube-proxy-v54pf from kube-system started at 2021-02-23 18:20:31 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 07:49:15.115: INFO: node-local-dns-27gsv from kube-system started at 2021-02-23 18:22:38 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 07:49:15.115: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-4n6zm from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 07:49:15.115: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 07:49:15.115: INFO: ratings-v1-675894856b-tjjdl from serv-mesh-ex started at 2021-03-01 08:46:17 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 07:49:15.115: INFO: 	Container ratings ready: true, restart count 1
Mar  5 07:49:15.115: INFO: enginsgungor-tutorial-deployment-late-e4m-3f096c9b3046-jobgsk6b from tutorial-s2i started at 2021-03-02 18:05:58 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 07:49:15.115: INFO: harbor-da5q24-harbor-database-0 from pipeline-tools started at 2021-03-04 10:01:12 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container database ready: true, restart count 1
Mar  5 07:49:15.115: INFO: rook-ceph-crashcollector-devops-control-plane-1-66d7bd67c8zzdhg from rook-ceph started at 2021-02-23 18:35:33 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 07:49:15.115: INFO: kube-scheduler-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container kube-scheduler ready: true, restart count 10
Mar  5 07:49:15.115: INFO: canal-v67nk from kube-system started at 2021-02-23 18:23:29 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 07:49:15.115: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 07:49:15.115: INFO: kube-controller-manager-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container kube-controller-manager ready: true, restart count 14
Mar  5 07:49:15.115: INFO: redis-ha-server-2 from kubesphere-system started at 2021-02-24 13:03:31 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container redis ready: true, restart count 1
Mar  5 07:49:15.115: INFO: 	Container sentinel ready: true, restart count 1
Mar  5 07:49:15.115: INFO: fluent-bit-6z2kz from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 07:49:15.115: INFO: node-exporter-grhwd from kubesphere-monitoring-system started at 2021-03-04 17:34:37 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 07:49:15.115: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 07:49:15.115: INFO: etcd-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container etcd ready: true, restart count 1
Mar  5 07:49:15.115: INFO: rook-ceph-osd-2-7d8c68d866-pljjp from rook-ceph started at 2021-02-23 18:35:33 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container osd ready: true, restart count 1
Mar  5 07:49:15.115: INFO: thanos-ruler-k8s-0 from kubesphere-monitoring-system started at 2021-02-24 13:23:29 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container rules-configmap-reloader ready: true, restart count 1
Mar  5 07:49:15.115: INFO: 	Container thanos-ruler ready: true, restart count 1
Mar  5 07:49:15.115: INFO: kube-apiserver-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar  5 07:49:15.115: INFO: csi-cephfsplugin-provisioner-c68f789b8-2rdqm from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (6 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container csi-attacher ready: true, restart count 10
Mar  5 07:49:15.115: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 07:49:15.115: INFO: 	Container csi-provisioner ready: true, restart count 13
Mar  5 07:49:15.115: INFO: 	Container csi-resizer ready: true, restart count 11
Mar  5 07:49:15.115: INFO: 	Container csi-snapshotter ready: true, restart count 19
Mar  5 07:49:15.115: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 07:49:15.115: INFO: istiod-1-6-10-7d869d7f58-tg6zc from istio-system started at 2021-03-01 07:43:53 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container discovery ready: true, restart count 1
Mar  5 07:49:15.115: INFO: rook-ceph-mon-d-c99989cc-lrjlz from rook-ceph started at 2021-03-04 08:21:35 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container mon ready: true, restart count 1
Mar  5 07:49:15.115: INFO: harbor-da5q24-harbor-jobservice-b55d6b698-8zwxd from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container jobservice ready: true, restart count 2
Mar  5 07:49:15.115: INFO: rook-ceph-osd-prepare-devops-control-plane-1-fzs7q from rook-ceph started at 2021-03-04 11:34:43 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container provision ready: false, restart count 0
Mar  5 07:49:15.115: INFO: csi-cephfsplugin-8sbwl from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 07:49:15.115: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 07:49:15.115: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 07:49:15.115: INFO: redis-ha-haproxy-5c6559d588-b2n68 from kubesphere-system started at 2021-03-04 08:11:54 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container haproxy ready: true, restart count 2
Mar  5 07:49:15.115: INFO: csi-rbdplugin-l7fcg from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 07:49:15.115: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 07:49:15.115: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 07:49:15.115: INFO: details-v1-dc74fc894-m9kzk from serv-mesh-ex started at 2021-03-01 08:46:17 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container details ready: true, restart count 1
Mar  5 07:49:15.115: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 07:49:15.115: INFO: harbor-da5q24-harbor-notary-signer-6785bf5b4-6w2dq from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.115: INFO: 	Container notary-signer ready: true, restart count 2
Mar  5 07:49:15.115: INFO: 
Logging pods the kubelet thinks is on node devops-control-plane-2 before test
Mar  5 07:49:15.179: INFO: rook-ceph-osd-0-855b585564-645vx from rook-ceph started at 2021-02-23 18:34:51 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container osd ready: true, restart count 1
Mar  5 07:49:15.179: INFO: redis-ha-haproxy-5c6559d588-l22bp from kubesphere-system started at 2021-02-24 13:03:15 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container haproxy ready: true, restart count 2
Mar  5 07:49:15.179: INFO: jaeger-es-index-cleaner-1614729300-p7kmh from istio-system started at 2021-03-02 23:55:03 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container jaeger-es-index-cleaner ready: false, restart count 0
Mar  5 07:49:15.179: INFO: logsidecar-injector-deploy-74c66bfd85-vpkvb from kubesphere-logging-system started at 2021-02-24 13:22:19 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container config-reloader ready: true, restart count 2
Mar  5 07:49:15.179: INFO: 	Container logsidecar-injector ready: true, restart count 2
Mar  5 07:49:15.179: INFO: minio-image-st-668b878c77-4rwrb from pipeline-tools started at 2021-03-03 17:33:04 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container minio ready: true, restart count 1
Mar  5 07:49:15.179: INFO: harbor-da5q24-harbor-trivy-0 from pipeline-tools started at 2021-03-04 10:01:13 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container trivy ready: true, restart count 1
Mar  5 07:49:15.179: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-5bzp2 from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 07:49:15.179: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 07:49:15.179: INFO: node-exporter-cvhfl from kubesphere-monitoring-system started at 2021-03-04 17:34:19 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 07:49:15.179: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 07:49:15.179: INFO: kube-scheduler-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container kube-scheduler ready: true, restart count 8
Mar  5 07:49:15.179: INFO: etcd-65796969c7-mr9hs from kubesphere-system started at 2021-02-24 13:19:44 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container etcd ready: true, restart count 1
Mar  5 07:49:15.179: INFO: notification-deployment-65547747f7-zxr25 from kubesphere-alerting-system started at 2021-02-24 13:45:42 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container notification ready: true, restart count 1
Mar  5 07:49:15.179: INFO: harbor-da5q24-harbor-portal-84fb8bdb7f-nttwj from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container portal ready: true, restart count 1
Mar  5 07:49:15.179: INFO: etcd-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:23 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container etcd ready: true, restart count 1
Mar  5 07:49:15.179: INFO: csi-rbdplugin-4hzbs from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 07:49:15.179: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 07:49:15.179: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 07:49:15.179: INFO: openldap-1 from kubesphere-system started at 2021-02-24 13:04:13 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container openldap-ha ready: true, restart count 1
Mar  5 07:49:15.179: INFO: ks-installer-74b46bd68d-q6nrh from kubesphere-system started at 2021-03-04 08:15:54 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container installer ready: true, restart count 1
Mar  5 07:49:15.179: INFO: harbor-da5q24-harbor-core-67659b6b55-4mcwv from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container core ready: true, restart count 2
Mar  5 07:49:15.179: INFO: harbor-da5q24-harbor-notary-server-7fc788c84b-bdpf9 from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container notary-server ready: true, restart count 3
Mar  5 07:49:15.179: INFO: kube-controller-manager-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container kube-controller-manager ready: true, restart count 7
Mar  5 07:49:15.179: INFO: fluent-bit-grhml from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 07:49:15.179: INFO: ks-events-ruler-698b7899c7-9qnjp from kubesphere-logging-system started at 2021-02-24 13:22:32 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 07:49:15.179: INFO: 	Container events-ruler ready: true, restart count 1
Mar  5 07:49:15.179: INFO: jaeger-operator-c78679c9f-r9pbh from istio-system started at 2021-03-01 07:44:49 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container jaeger-operator ready: true, restart count 2
Mar  5 07:49:15.179: INFO: harbor-da5q24-harbor-chartmuseum-874964bb8-7dqmz from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container chartmuseum ready: true, restart count 1
Mar  5 07:49:15.179: INFO: gogs-gogs-test-http from proteus started at 2021-03-03 07:35:46 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container wget ready: false, restart count 0
Mar  5 07:49:15.179: INFO: rook-ceph-osd-prepare-devops-control-plane-2-l69rc from rook-ceph started at 2021-03-04 11:34:45 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container provision ready: false, restart count 0
Mar  5 07:49:15.179: INFO: csi-cephfsplugin-l8vgz from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 07:49:15.179: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 07:49:15.179: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 07:49:15.179: INFO: ks-console-fb7f5895-rxb4f from kubesphere-system started at 2021-02-24 13:04:07 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container ks-console ready: true, restart count 1
Mar  5 07:49:15.179: INFO: harbor-da5q24-harbor-registry-56df8d46bf-2s6ct from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container registry ready: true, restart count 1
Mar  5 07:49:15.179: INFO: 	Container registryctl ready: true, restart count 1
Mar  5 07:49:15.179: INFO: ks-apiserver-5894746f6b-gprr9 from kubesphere-system started at 2021-03-04 18:53:58 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container ks-apiserver ready: true, restart count 0
Mar  5 07:49:15.179: INFO: canal-hctnd from kube-system started at 2021-02-23 18:23:44 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 07:49:15.179: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 07:49:15.179: INFO: kube-auditing-webhook-deploy-b74bfb885-5cnp5 from kubesphere-logging-system started at 2021-02-24 13:34:12 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container kube-auditing-webhook ready: true, restart count 1
Mar  5 07:49:15.179: INFO: harbor-da5q24-harbor-clair-5dd889dd4f-4nclv from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container adapter ready: true, restart count 1
Mar  5 07:49:15.179: INFO: 	Container clair ready: true, restart count 5
Mar  5 07:49:15.179: INFO: jaeger-es-index-cleaner-1614815700-7s2kc from istio-system started at 2021-03-03 23:55:05 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container jaeger-es-index-cleaner ready: false, restart count 0
Mar  5 07:49:15.179: INFO: coredns-6cf46fccfd-vgfsb from kube-system started at 2021-02-23 18:23:00 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container coredns ready: true, restart count 1
Mar  5 07:49:15.179: INFO: kube-apiserver-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar  5 07:49:15.179: INFO: jaeger-collector-7887b45bf-n8cbz from istio-system started at 2021-03-01 07:45:14 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container jaeger-collector ready: true, restart count 3
Mar  5 07:49:15.179: INFO: elasticsearch-logging-curator-elasticsearch-curator-161473657d4 from kubesphere-logging-system started at 2021-03-03 01:00:02 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.179: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Mar  5 07:49:15.179: INFO: thanos-ruler-kubesphere-0 from kubesphere-monitoring-system started at 2021-03-04 07:58:11 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.180: INFO: 	Container rules-configmap-reloader ready: true, restart count 1
Mar  5 07:49:15.180: INFO: 	Container thanos-ruler ready: true, restart count 1
Mar  5 07:49:15.180: INFO: redis-ha-server-1 from kubesphere-system started at 2021-02-24 13:03:23 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.180: INFO: 	Container redis ready: true, restart count 1
Mar  5 07:49:15.180: INFO: 	Container sentinel ready: true, restart count 1
Mar  5 07:49:15.180: INFO: harbor-da5q24-harbor-nginx-687f459b8d-drvcb from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.180: INFO: 	Container nginx ready: true, restart count 2
Mar  5 07:49:15.180: INFO: kube-proxy-vkr26 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.180: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 07:49:15.180: INFO: coredns-6cf46fccfd-lbcwc from kube-system started at 2021-02-23 18:23:00 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.180: INFO: 	Container coredns ready: true, restart count 1
Mar  5 07:49:15.180: INFO: hcloud-cloud-controller-manager-7c5d46cc54-rhlq7 from kube-system started at 2021-02-23 18:23:00 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.180: INFO: 	Container hcloud-cloud-controller-manager ready: true, restart count 2
Mar  5 07:49:15.180: INFO: ks-events-operator-8dbf7fccc-v8zdm from kubesphere-logging-system started at 2021-02-24 13:22:25 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.180: INFO: 	Container events-operator ready: true, restart count 1
Mar  5 07:49:15.180: INFO: node-local-dns-4wxxm from kube-system started at 2021-02-23 18:22:39 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.180: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 07:49:15.180: INFO: reviews-v1-549f6b9d47-rttzb from serv-mesh-ex started at 2021-03-01 10:23:35 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.180: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 07:49:15.180: INFO: 	Container reviews ready: true, restart count 1
Mar  5 07:49:15.180: INFO: ks-controller-manager-8556448648-dk5xc from kubesphere-system started at 2021-03-04 18:53:58 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.180: INFO: 	Container ks-controller-manager ready: true, restart count 0
Mar  5 07:49:15.180: INFO: rook-ceph-crashcollector-devops-control-plane-2-6686b8d74fqx7lt from rook-ceph started at 2021-02-23 18:34:51 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.180: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 07:49:15.180: INFO: 
Logging pods the kubelet thinks is on node devops-control-plane-3 before test
Mar  5 07:49:15.247: INFO: machine-controller-74bbdbb8b8-7r4nj from kube-system started at 2021-02-23 18:23:35 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container machine-controller ready: true, restart count 27
Mar  5 07:49:15.247: INFO: minio-7bfdb5968b-cnd55 from kubesphere-system started at 2021-02-24 14:10:33 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container minio ready: true, restart count 1
Mar  5 07:49:15.247: INFO: thanos-ruler-kubesphere-1 from kubesphere-monitoring-system started at 2021-03-04 07:58:11 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container rules-configmap-reloader ready: true, restart count 1
Mar  5 07:49:15.247: INFO: 	Container thanos-ruler ready: true, restart count 1
Mar  5 07:49:15.247: INFO: ks-apiserver-5894746f6b-8p8vg from kubesphere-system started at 2021-03-04 18:54:02 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container ks-apiserver ready: true, restart count 0
Mar  5 07:49:15.247: INFO: rook-ceph-osd-prepare-devops-control-plane-3-mnfq2 from rook-ceph started at 2021-03-04 11:34:47 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container provision ready: false, restart count 0
Mar  5 07:49:15.247: INFO: gogs-gogs-test-ssh from proteus started at 2021-03-03 07:35:46 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container wget ready: false, restart count 0
Mar  5 07:49:15.247: INFO: kube-proxy-nrg4j from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 07:49:15.247: INFO: csi-cephfsplugin-287cb from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 07:49:15.247: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 07:49:15.247: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 07:49:15.247: INFO: ks-console-fb7f5895-5pmpn from kubesphere-system started at 2021-02-24 13:04:07 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container ks-console ready: true, restart count 1
Mar  5 07:49:15.247: INFO: csi-rbdplugin-provisioner-5bc97cdbb9-zjbws from rook-ceph started at 2021-03-04 08:15:54 +0000 UTC (6 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container csi-attacher ready: true, restart count 1
Mar  5 07:49:15.247: INFO: 	Container csi-provisioner ready: true, restart count 1
Mar  5 07:49:15.247: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 07:49:15.247: INFO: 	Container csi-resizer ready: true, restart count 1
Mar  5 07:49:15.247: INFO: 	Container csi-snapshotter ready: true, restart count 1
Mar  5 07:49:15.247: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 07:49:15.247: INFO: kube-controller-manager-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container kube-controller-manager ready: true, restart count 11
Mar  5 07:49:15.247: INFO: kube-scheduler-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container kube-scheduler ready: true, restart count 13
Mar  5 07:49:15.247: INFO: fluentbit-operator-7c56d66dd-7b4df from kubesphere-logging-system started at 2021-02-24 13:20:01 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container fluentbit-operator ready: true, restart count 2
Mar  5 07:49:15.247: INFO: istio-ingressgateway-76df6567c6-tndjv from istio-system started at 2021-03-01 07:44:17 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 07:49:15.247: INFO: ks-controller-manager-8556448648-c4spd from kubesphere-system started at 2021-03-04 18:54:03 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container ks-controller-manager ready: true, restart count 0
Mar  5 07:49:15.247: INFO: machine-controller-webhook-747c599b5c-k7xw4 from kube-system started at 2021-02-23 18:23:36 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container machine-controller-webhook ready: true, restart count 1
Mar  5 07:49:15.247: INFO: notification-manager-deployment-7c8df68d94-5dv72 from kubesphere-monitoring-system started at 2021-02-24 13:05:51 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container notification-manager ready: true, restart count 1
Mar  5 07:49:15.247: INFO: jaeger-query-b546558bf-frn98 from istio-system started at 2021-03-01 07:45:14 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container jaeger-agent ready: true, restart count 1
Mar  5 07:49:15.247: INFO: 	Container jaeger-query ready: true, restart count 1
Mar  5 07:49:15.247: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2021-03-04 17:34:29 +0000 UTC (3 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container prometheus ready: true, restart count 1
Mar  5 07:49:15.247: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  5 07:49:15.247: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  5 07:49:15.247: INFO: notification-db-init-job-mqs2r from kubesphere-alerting-system started at 2021-03-03 17:34:05 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container notification-db-init ready: false, restart count 0
Mar  5 07:49:15.247: INFO: notification-db-ctrl-job-7kzd5 from kubesphere-alerting-system started at 2021-03-03 17:34:15 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container notification-db-ctrl ready: false, restart count 0
Mar  5 07:49:15.247: INFO: without-dockerfile-v1-7dc7678956-mh5vw from serv-mesh-ex started at 2021-03-04 08:15:54 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container container-2a87d6 ready: true, restart count 1
Mar  5 07:49:15.247: INFO: node-local-dns-twq6t from kube-system started at 2021-02-23 18:22:38 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 07:49:15.247: INFO: rook-ceph-crashcollector-devops-control-plane-3-587c5dc7b8mt27z from rook-ceph started at 2021-02-23 18:34:53 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 07:49:15.247: INFO: redis-ha-server-0 from kubesphere-system started at 2021-02-24 13:03:18 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container redis ready: true, restart count 1
Mar  5 07:49:15.247: INFO: 	Container sentinel ready: true, restart count 1
Mar  5 07:49:15.247: INFO: fluent-bit-s284n from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 07:49:15.247: INFO: kube-auditing-operator-6ddc8db4b-vfxqc from kubesphere-logging-system started at 2021-02-24 13:34:03 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container kube-auditing-operator ready: true, restart count 1
Mar  5 07:49:15.247: INFO: etcd-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:36 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container etcd ready: true, restart count 1
Mar  5 07:49:15.247: INFO: redis-ha-haproxy-5c6559d588-qhqhx from kubesphere-system started at 2021-02-24 13:03:15 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container haproxy ready: true, restart count 1
Mar  5 07:49:15.247: INFO: logsidecar-injector-deploy-74c66bfd85-62xbg from kubesphere-logging-system started at 2021-02-24 13:22:19 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 07:49:15.247: INFO: 	Container logsidecar-injector ready: true, restart count 1
Mar  5 07:49:15.247: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-wp8ts from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 07:49:15.247: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 07:49:15.247: INFO: kiali-operator-58b8765b9c-6qh7r from istio-system started at 2021-03-01 07:44:57 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container operator ready: true, restart count 1
Mar  5 07:49:15.247: INFO: kube-apiserver-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar  5 07:49:15.247: INFO: csi-rbdplugin-7zr2t from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 07:49:15.247: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 07:49:15.247: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 07:49:15.247: INFO: node-exporter-6fsk8 from kubesphere-monitoring-system started at 2021-03-04 17:34:28 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 07:49:15.247: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 07:49:15.247: INFO: rook-ceph-osd-1-845f4b4486-5phnd from rook-ceph started at 2021-02-23 18:34:53 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container osd ready: true, restart count 1
Mar  5 07:49:15.247: INFO: openldap-0 from kubesphere-system started at 2021-02-24 13:03:27 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container openldap-ha ready: true, restart count 1
Mar  5 07:49:15.247: INFO: calico-kube-controllers-589975f454-whrcl from kube-system started at 2021-02-23 18:23:24 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container calico-kube-controllers ready: true, restart count 1
Mar  5 07:49:15.247: INFO: canal-r57rn from kube-system started at 2021-02-23 18:23:39 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 07:49:15.247: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 07:49:15.247: INFO: ks-events-ruler-698b7899c7-rjqqv from kubesphere-logging-system started at 2021-02-24 13:22:32 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.247: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 07:49:15.247: INFO: 	Container events-ruler ready: true, restart count 1
Mar  5 07:49:15.247: INFO: 
Logging pods the kubelet thinks is on node devops-pool1-6c76d44df9-8tgs6 before test
Mar  5 07:49:15.279: INFO: s2ioperator-0 from kubesphere-devops-system started at 2021-03-04 18:50:25 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.279: INFO: 	Container manager ready: true, restart count 0
Mar  5 07:49:15.279: INFO: elasticsearch-logging-data-2 from kubesphere-logging-system started at 2021-03-04 11:51:19 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.279: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  5 07:49:15.279: INFO: snapshot-controller-0 from kube-system started at 2021-03-04 11:51:26 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.279: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  5 07:49:15.279: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-9ffgc from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.279: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 07:49:15.279: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 07:49:15.279: INFO: csi-rbdplugin-9km6t from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 07:49:15.279: INFO: 	Container csi-rbdplugin ready: true, restart count 8
Mar  5 07:49:15.279: INFO: 	Container driver-registrar ready: true, restart count 8
Mar  5 07:49:15.279: INFO: 	Container liveness-prometheus ready: true, restart count 8
Mar  5 07:49:15.279: INFO: alertmanager-main-0 from kubesphere-monitoring-system started at 2021-03-04 11:52:06 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.279: INFO: 	Container alertmanager ready: true, restart count 0
Mar  5 07:49:15.279: INFO: 	Container config-reloader ready: true, restart count 0
Mar  5 07:49:15.279: INFO: gogs-postgresql-0 from default started at 2021-03-04 13:06:21 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.279: INFO: 	Container gogs-postgresql ready: true, restart count 0
Mar  5 07:49:15.279: INFO: elasticsearch-logging-curator-elasticsearch-curator-1614905cqqk from kubesphere-logging-system started at 2021-03-05 01:00:08 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.280: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Mar  5 07:49:15.280: INFO: sonobuoy from sonobuoy started at 2021-03-05 07:46:37 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.280: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  5 07:49:15.280: INFO: canal-rm2l9 from kube-system started at 2021-02-23 18:25:59 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.280: INFO: 	Container calico-node ready: true, restart count 8
Mar  5 07:49:15.280: INFO: 	Container kube-flannel ready: true, restart count 8
Mar  5 07:49:15.280: INFO: jaeger-es-index-cleaner-1614902100-xcqvw from istio-system started at 2021-03-04 23:55:01 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.280: INFO: 	Container jaeger-es-index-cleaner ready: false, restart count 0
Mar  5 07:49:15.280: INFO: thanos-ruler-k8s-1 from kubesphere-monitoring-system started at 2021-03-04 11:51:27 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.280: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  5 07:49:15.280: INFO: 	Container thanos-ruler ready: true, restart count 0
Mar  5 07:49:15.280: INFO: openpitrix-hyperpitrix-deployment-6d48d87c6c-2xhgh from openpitrix-system started at 2021-03-04 18:51:18 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.280: INFO: 	Container hyperpitrix ready: true, restart count 0
Mar  5 07:49:15.280: INFO: hyperpitrix-generate-kubeconfig-9f4fb from openpitrix-system started at 2021-03-04 18:51:36 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.280: INFO: 	Container hyperpitrix-generate-kubeconfig ready: false, restart count 0
Mar  5 07:49:15.280: INFO: 	Container hyperpitrix-migrate-runtime ready: false, restart count 0
Mar  5 07:49:15.280: INFO: node-local-dns-kh572 from kube-system started at 2021-02-23 18:25:59 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.280: INFO: 	Container node-cache ready: true, restart count 8
Mar  5 07:49:15.280: INFO: node-exporter-824lt from kubesphere-monitoring-system started at 2021-03-04 17:34:42 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.280: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 07:49:15.280: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 07:49:15.280: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2021-03-04 17:35:33 +0000 UTC (3 container statuses recorded)
Mar  5 07:49:15.280: INFO: 	Container prometheus ready: true, restart count 1
Mar  5 07:49:15.280: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  5 07:49:15.280: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  5 07:49:15.280: INFO: kube-proxy-jxx7v from kube-system started at 2021-02-23 18:25:59 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.280: INFO: 	Container kube-proxy ready: true, restart count 8
Mar  5 07:49:15.280: INFO: fluent-bit-2vc49 from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.280: INFO: 	Container fluent-bit ready: true, restart count 8
Mar  5 07:49:15.280: INFO: svc-latency-rc-wzrzt from svc-latency-4085 started at 2021-03-05 07:49:02 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.280: INFO: 	Container svc-latency-rc ready: true, restart count 0
Mar  5 07:49:15.280: INFO: ks-sample-dev-5c5d97c6cc-c9b48 from kubesphere-offline-dev started at 2021-03-04 17:48:16 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.280: INFO: 	Container js-sample ready: true, restart count 0
Mar  5 07:49:15.280: INFO: csi-cephfsplugin-9k9tq from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 07:49:15.280: INFO: 	Container csi-cephfsplugin ready: true, restart count 8
Mar  5 07:49:15.280: INFO: 	Container driver-registrar ready: true, restart count 8
Mar  5 07:49:15.280: INFO: 	Container liveness-prometheus ready: true, restart count 8
Mar  5 07:49:15.280: INFO: elasticsearch-logging-discovery-1 from kubesphere-logging-system started at 2021-03-04 11:51:15 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.280: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  5 07:49:15.280: INFO: gogs-gogs-7467fdcf8f-mckdc from default started at 2021-03-04 13:06:22 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.280: INFO: 	Container gogs ready: true, restart count 0
Mar  5 07:49:15.280: INFO: 
Logging pods the kubelet thinks is on node devops-pool1-6c76d44df9-dwtv7 before test
Mar  5 07:49:15.326: INFO: csi-cephfsplugin-provisioner-c68f789b8-k89fp from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (6 container statuses recorded)
Mar  5 07:49:15.326: INFO: 	Container csi-attacher ready: true, restart count 14
Mar  5 07:49:15.326: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 07:49:15.326: INFO: 	Container csi-provisioner ready: true, restart count 14
Mar  5 07:49:15.326: INFO: 	Container csi-resizer ready: true, restart count 12
Mar  5 07:49:15.326: INFO: 	Container csi-snapshotter ready: true, restart count 13
Mar  5 07:49:15.326: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 07:49:15.326: INFO: ks-apiserver-5894746f6b-2dq28 from kubesphere-system started at 2021-03-04 18:53:54 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.326: INFO: 	Container ks-apiserver ready: true, restart count 0
Mar  5 07:49:15.326: INFO: ks-console-fb7f5895-vvxbg from kubesphere-system started at 2021-02-25 07:43:56 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.326: INFO: 	Container ks-console ready: true, restart count 1
Mar  5 07:49:15.326: INFO: tutorial-deployment-v1-5549b8b9d8-t8t9z from tutorial-s2i started at 2021-03-02 18:10:32 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.326: INFO: 	Container container-t1u32h ready: true, restart count 1
Mar  5 07:49:15.326: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-f7ttl from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.326: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 07:49:15.326: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 07:49:15.326: INFO: kube-proxy-2czsl from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.326: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 07:49:15.326: INFO: alertmanager-main-1 from kubesphere-monitoring-system started at 2021-02-24 13:05:28 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.326: INFO: 	Container alertmanager ready: true, restart count 1
Mar  5 07:49:15.326: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 07:49:15.326: INFO: elasticsearch-logging-discovery-2 from kubesphere-logging-system started at 2021-02-24 13:21:07 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.326: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 07:49:15.326: INFO: rook-ceph-mon-a-85cd5968bc-hq6sb from rook-ceph started at 2021-02-23 18:33:25 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.326: INFO: 	Container mon ready: true, restart count 1
Mar  5 07:49:15.326: INFO: node-local-dns-8s8tk from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.326: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 07:49:15.326: INFO: rook-ceph-crashcollector-devops-pool1-6c76d44df9-dwtv7-7dfdvvgh from rook-ceph started at 2021-02-23 18:34:00 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.326: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 07:49:15.326: INFO: canal-mfdhb from kube-system started at 2021-02-23 18:26:00 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 07:49:15.327: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 07:49:15.327: INFO: csi-rbdplugin-nhvm9 from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 07:49:15.327: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 07:49:15.327: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 07:49:15.327: INFO: enginsgungor-hello-deployment-latest--0dq-14b4f67d0b7e-jobptskv from serv-mesh-ex started at 2021-03-01 19:28:59 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 07:49:15.327: INFO: ks-sample-dev-7bb944b987-hnlc4 from kubesphere-sample-dev started at 2021-03-03 11:03:12 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container js-sample ready: true, restart count 1
Mar  5 07:49:15.327: INFO: notification-manager-operator-6958786cd6-26xxr from kubesphere-monitoring-system started at 2021-03-04 08:15:54 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Mar  5 07:49:15.327: INFO: 	Container notification-manager-operator ready: true, restart count 2
Mar  5 07:49:15.327: INFO: rook-ceph-operator-6fb9f456fc-8g9z2 from rook-ceph started at 2021-02-23 18:29:52 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container rook-ceph-operator ready: true, restart count 1
Mar  5 07:49:15.327: INFO: kiali-5fc8bfd66b-wjrqr from istio-system started at 2021-03-01 07:45:58 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container kiali ready: true, restart count 1
Mar  5 07:49:15.327: INFO: harbor-da5q24-harbor-redis-0 from pipeline-tools started at 2021-03-04 10:01:12 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container redis ready: true, restart count 1
Mar  5 07:49:15.327: INFO: kube-auditing-webhook-deploy-b74bfb885-qf48j from kubesphere-logging-system started at 2021-02-24 13:34:12 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container kube-auditing-webhook ready: true, restart count 1
Mar  5 07:49:15.327: INFO: fluent-bit-s42x9 from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 07:49:15.327: INFO: hyperpitrix-release-app-job-tqln9 from openpitrix-system started at 2021-03-04 18:51:28 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container hyperpitrix-release-app-job ready: false, restart count 0
Mar  5 07:49:15.327: INFO: enginsgungor-hello-deployment-latest--6eg-3afd31aa2f90-jobgl2pn from serv-mesh-ex started at 2021-03-01 19:15:53 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 07:49:15.327: INFO: enginsgungor-tutorial-deployment-late-9pz-042c363e1509-job598vw from tutorial-s2i started at 2021-03-02 18:10:13 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 07:49:15.327: INFO: csi-cephfsplugin-nxcmz from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 07:49:15.327: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 07:49:15.327: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 07:49:15.327: INFO: ks-controller-manager-8556448648-cbt26 from kubesphere-system started at 2021-03-04 18:53:55 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container ks-controller-manager ready: true, restart count 1
Mar  5 07:49:15.327: INFO: enginsgungor-hello-world-latest-ug1-ejy-f941edb3103d-job-pr5nr from serv-mesh-ex started at 2021-03-01 18:50:41 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 07:49:15.327: INFO: elasticsearch-logging-curator-elasticsearch-curator-161481wvfqt from kubesphere-logging-system started at 2021-03-04 01:00:01 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Mar  5 07:49:15.327: INFO: node-exporter-f6kh5 from kubesphere-monitoring-system started at 2021-03-04 17:34:32 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 07:49:15.327: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 07:49:15.327: INFO: sonobuoy-e2e-job-41e333abf2c348c1 from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container e2e ready: true, restart count 0
Mar  5 07:49:15.327: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 07:49:15.327: INFO: elasticsearch-logging-data-1 from kubesphere-logging-system started at 2021-02-24 13:20:42 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 07:49:15.327: INFO: rook-ceph-mgr-a-55fb9d48b6-clq94 from rook-ceph started at 2021-02-23 18:34:00 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.327: INFO: 	Container mgr ready: true, restart count 2
Mar  5 07:49:15.327: INFO: 
Logging pods the kubelet thinks is on node devops-pool1-6c76d44df9-njb8n before test
Mar  5 07:49:15.396: INFO: kube-state-metrics-95c974544-drf59 from kubesphere-monitoring-system started at 2021-02-24 13:05:20 +0000 UTC (3 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 1
Mar  5 07:49:15.396: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 1
Mar  5 07:49:15.396: INFO: 	Container kube-state-metrics ready: true, restart count 1
Mar  5 07:49:15.396: INFO: hellojs-v1-5f4d98c7df-snl82 from serv-mesh-ex started at 2021-03-02 06:09:42 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container container-3nysic ready: true, restart count 1
Mar  5 07:49:15.396: INFO: csi-rbdplugin-5n26w from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 07:49:15.396: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 07:49:15.396: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 07:49:15.396: INFO: kubectl-admin-5d98567c78-m72x5 from kubesphere-controls-system started at 2021-02-24 13:06:05 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container kubectl ready: true, restart count 1
Mar  5 07:49:15.396: INFO: enginsgungor-hello-deployment-latest--fsk-bb68efbac665-jobm9zxh from serv-mesh-ex started at 2021-03-02 06:09:25 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 07:49:15.396: INFO: node-exporter-fhq4b from kubesphere-monitoring-system started at 2021-03-04 17:34:24 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 07:49:15.396: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 07:49:15.396: INFO: rook-ceph-mon-c-764d6d7649-wcbn5 from rook-ceph started at 2021-02-23 18:33:46 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container mon ready: true, restart count 1
Mar  5 07:49:15.396: INFO: elasticsearch-logging-discovery-0 from kubesphere-logging-system started at 2021-02-24 13:19:50 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 07:49:15.396: INFO: fluent-bit-cqbf6 from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 07:49:15.396: INFO: ks-jenkins-5bf5cbf449-tzqjz from kubesphere-devops-system started at 2021-02-28 09:58:29 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container ks-jenkins ready: true, restart count 1
Mar  5 07:49:15.396: INFO: enginsgungor-hello-deployment-latest--cnc-39caa4291750-jobgnx27 from serv-mesh-ex started at 2021-03-01 19:24:09 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 07:49:15.396: INFO: productpage-v1-795bd6db76-trctn from serv-mesh-ex started at 2021-03-04 08:15:54 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 07:49:15.396: INFO: 	Container productpage ready: true, restart count 1
Mar  5 07:49:15.396: INFO: alertmanager-main-2 from kubesphere-monitoring-system started at 2021-02-24 13:05:28 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container alertmanager ready: true, restart count 1
Mar  5 07:49:15.396: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 07:49:15.396: INFO: ks-events-exporter-5bc4d9f496-zm5tx from kubesphere-logging-system started at 2021-02-24 13:22:33 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 07:49:15.396: INFO: 	Container events-exporter ready: true, restart count 1
Mar  5 07:49:15.396: INFO: kubesphere-router-serv-mesh-ex-585558c74b-45mf8 from kubesphere-controls-system started at 2021-03-01 08:45:33 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 07:49:15.396: INFO: 	Container nginx-ingress-controller ready: true, restart count 1
Mar  5 07:49:15.396: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-xprqf from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 07:49:15.396: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 07:49:15.396: INFO: csi-rbdplugin-provisioner-5bc97cdbb9-nlb2z from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (6 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container csi-attacher ready: true, restart count 21
Mar  5 07:49:15.396: INFO: 	Container csi-provisioner ready: true, restart count 12
Mar  5 07:49:15.396: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 07:49:15.396: INFO: 	Container csi-resizer ready: true, restart count 11
Mar  5 07:49:15.396: INFO: 	Container csi-snapshotter ready: true, restart count 13
Mar  5 07:49:15.396: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 07:49:15.396: INFO: kube-proxy-bpngj from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 07:49:15.396: INFO: default-http-backend-857d7b6856-gt6hh from kubesphere-controls-system started at 2021-02-24 13:03:47 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container default-http-backend ready: true, restart count 1
Mar  5 07:49:15.396: INFO: node-local-dns-5qx5m from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 07:49:15.396: INFO: rook-ceph-crashcollector-devops-pool1-6c76d44df9-njb8n-cfbd6xbn from rook-ceph started at 2021-02-23 18:33:46 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 07:49:15.396: INFO: prometheus-operator-84d58bf775-q2drc from kubesphere-monitoring-system started at 2021-02-24 13:05:18 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.396: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Mar  5 07:49:15.396: INFO: 	Container prometheus-operator ready: true, restart count 1
Mar  5 07:49:15.396: INFO: notification-manager-deployment-7c8df68d94-7vd2s from kubesphere-monitoring-system started at 2021-02-24 13:05:51 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.397: INFO: 	Container notification-manager ready: true, restart count 1
Mar  5 07:49:15.397: INFO: mysql-7f64d9f584-49fnb from kubesphere-system started at 2021-02-24 13:19:43 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.397: INFO: 	Container mysql ready: true, restart count 1
Mar  5 07:49:15.397: INFO: rook-ceph-tools-64bd84c8b5-hd7mk from rook-ceph started at 2021-02-25 07:43:01 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.397: INFO: 	Container rook-ceph-tools ready: true, restart count 1
Mar  5 07:49:15.397: INFO: csi-cephfsplugin-t7l65 from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 07:49:15.397: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 07:49:15.397: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 07:49:15.397: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 07:49:15.397: INFO: elasticsearch-logging-data-0 from kubesphere-logging-system started at 2021-02-24 13:19:49 +0000 UTC (1 container statuses recorded)
Mar  5 07:49:15.397: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 07:49:15.397: INFO: canal-tp2fn from kube-system started at 2021-02-23 18:26:00 +0000 UTC (2 container statuses recorded)
Mar  5 07:49:15.397: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 07:49:15.397: INFO: 	Container kube-flannel ready: true, restart count 1
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-efb93d6d-468e-4e18-b35e-074f149736a8 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-efb93d6d-468e-4e18-b35e-074f149736a8 off the node devops-pool1-6c76d44df9-8tgs6
STEP: verifying the node doesn't have the label kubernetes.io/e2e-efb93d6d-468e-4e18-b35e-074f149736a8
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:54:21.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6571" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

â€¢ [SLOW TEST:307.174 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":277,"completed":9,"skipped":187,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:54:21.639: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-198
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 07:54:21.840: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:54:27.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-198" for this suite.

â€¢ [SLOW TEST:6.308 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":277,"completed":10,"skipped":208,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:54:27.958: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8037
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:54:31.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8037" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":277,"completed":11,"skipped":216,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:54:31.264: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  5 07:54:32.349: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  5 07:54:34.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750527672, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750527672, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750527672, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750527672, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 07:54:37.414: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:54:37.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4917" for this suite.
STEP: Destroying namespace "webhook-4917-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.421 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":277,"completed":12,"skipped":234,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:54:37.685: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7794
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 07:54:37.917: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94342712-971b-4178-ad9c-f0ea6505a916" in namespace "projected-7794" to be "Succeeded or Failed"
Mar  5 07:54:37.922: INFO: Pod "downwardapi-volume-94342712-971b-4178-ad9c-f0ea6505a916": Phase="Pending", Reason="", readiness=false. Elapsed: 4.887818ms
Mar  5 07:54:39.935: INFO: Pod "downwardapi-volume-94342712-971b-4178-ad9c-f0ea6505a916": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01708819s
STEP: Saw pod success
Mar  5 07:54:39.935: INFO: Pod "downwardapi-volume-94342712-971b-4178-ad9c-f0ea6505a916" satisfied condition "Succeeded or Failed"
Mar  5 07:54:39.942: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-94342712-971b-4178-ad9c-f0ea6505a916 container client-container: <nil>
STEP: delete the pod
Mar  5 07:54:40.002: INFO: Waiting for pod downwardapi-volume-94342712-971b-4178-ad9c-f0ea6505a916 to disappear
Mar  5 07:54:40.008: INFO: Pod downwardapi-volume-94342712-971b-4178-ad9c-f0ea6505a916 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:54:40.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7794" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":277,"completed":13,"skipped":234,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:54:40.038: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  5 07:54:41.183: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  5 07:54:43.204: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750527681, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750527681, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750527681, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750527681, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 07:54:46.242: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:54:46.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8867" for this suite.
STEP: Destroying namespace "webhook-8867-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.476 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":277,"completed":14,"skipped":246,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:54:46.516: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1814
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-468f5537-dbcb-465d-ba64-61479842f93e
STEP: Creating a pod to test consume configMaps
Mar  5 07:54:46.733: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ce675680-4c72-4d46-abaa-22c774f06332" in namespace "projected-1814" to be "Succeeded or Failed"
Mar  5 07:54:46.741: INFO: Pod "pod-projected-configmaps-ce675680-4c72-4d46-abaa-22c774f06332": Phase="Pending", Reason="", readiness=false. Elapsed: 7.087115ms
Mar  5 07:54:48.748: INFO: Pod "pod-projected-configmaps-ce675680-4c72-4d46-abaa-22c774f06332": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014167072s
Mar  5 07:54:50.755: INFO: Pod "pod-projected-configmaps-ce675680-4c72-4d46-abaa-22c774f06332": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021275713s
STEP: Saw pod success
Mar  5 07:54:50.755: INFO: Pod "pod-projected-configmaps-ce675680-4c72-4d46-abaa-22c774f06332" satisfied condition "Succeeded or Failed"
Mar  5 07:54:50.761: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-projected-configmaps-ce675680-4c72-4d46-abaa-22c774f06332 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 07:54:50.801: INFO: Waiting for pod pod-projected-configmaps-ce675680-4c72-4d46-abaa-22c774f06332 to disappear
Mar  5 07:54:50.809: INFO: Pod pod-projected-configmaps-ce675680-4c72-4d46-abaa-22c774f06332 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:54:50.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1814" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":15,"skipped":278,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:54:50.836: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9509
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 07:54:51.027: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:54:53.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9509" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":277,"completed":16,"skipped":338,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:54:53.284: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-543
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-5ecf905c-c5b9-4928-a6e5-cb91c189efed
STEP: Creating configMap with name cm-test-opt-upd-98944d41-bd1f-41a6-83bb-4e572c8eacb4
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-5ecf905c-c5b9-4928-a6e5-cb91c189efed
STEP: Updating configmap cm-test-opt-upd-98944d41-bd1f-41a6-83bb-4e572c8eacb4
STEP: Creating configMap with name cm-test-opt-create-0be0e7a2-f4ff-4799-b0f2-3b56ea920ce7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:54:57.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-543" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":17,"skipped":344,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:54:57.751: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4117
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:55:09.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4117" for this suite.

â€¢ [SLOW TEST:11.314 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":277,"completed":18,"skipped":357,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:55:09.068: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8809
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-b50a5dd8-b141-48e7-8410-2a95fe05f60c
STEP: Creating a pod to test consume configMaps
Mar  5 07:55:09.319: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fdbf1c05-50f0-4ed5-827b-8e661861569f" in namespace "projected-8809" to be "Succeeded or Failed"
Mar  5 07:55:09.328: INFO: Pod "pod-projected-configmaps-fdbf1c05-50f0-4ed5-827b-8e661861569f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.633154ms
Mar  5 07:55:11.335: INFO: Pod "pod-projected-configmaps-fdbf1c05-50f0-4ed5-827b-8e661861569f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016128436s
STEP: Saw pod success
Mar  5 07:55:11.335: INFO: Pod "pod-projected-configmaps-fdbf1c05-50f0-4ed5-827b-8e661861569f" satisfied condition "Succeeded or Failed"
Mar  5 07:55:11.343: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-projected-configmaps-fdbf1c05-50f0-4ed5-827b-8e661861569f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 07:55:11.397: INFO: Waiting for pod pod-projected-configmaps-fdbf1c05-50f0-4ed5-827b-8e661861569f to disappear
Mar  5 07:55:11.405: INFO: Pod pod-projected-configmaps-fdbf1c05-50f0-4ed5-827b-8e661861569f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:55:11.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8809" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":277,"completed":19,"skipped":374,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:55:11.429: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7911
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar  5 07:55:11.622: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar  5 07:55:41.248: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 07:55:49.444: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:56:19.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7911" for this suite.

â€¢ [SLOW TEST:68.284 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":277,"completed":20,"skipped":380,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:56:19.714: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6936
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 07:56:19.915: INFO: Creating deployment "webserver-deployment"
Mar  5 07:56:19.924: INFO: Waiting for observed generation 1
Mar  5 07:56:21.939: INFO: Waiting for all required pods to come up
Mar  5 07:56:21.946: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar  5 07:56:29.969: INFO: Waiting for deployment "webserver-deployment" to complete
Mar  5 07:56:30.008: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar  5 07:56:30.073: INFO: Updating deployment webserver-deployment
Mar  5 07:56:30.073: INFO: Waiting for observed generation 2
Mar  5 07:56:32.095: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  5 07:56:32.102: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  5 07:56:32.107: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  5 07:56:32.126: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  5 07:56:32.126: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  5 07:56:32.133: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  5 07:56:32.147: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar  5 07:56:32.147: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar  5 07:56:32.166: INFO: Updating deployment webserver-deployment
Mar  5 07:56:32.167: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar  5 07:56:32.189: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  5 07:56:32.196: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar  5 07:56:32.208: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6936 /apis/apps/v1/namespaces/deployment-6936/deployments/webserver-deployment faba103b-db63-4d00-b1b2-29f5559a1e7f 5609923 3 2021-03-05 07:56:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{kube-controller-manager Update apps/v1 2021-03-05 07:56:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}} {e2e.test Update apps/v1 2021-03-05 07:56:32 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003fd14d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-05 07:56:25 +0000 UTC,LastTransitionTime:2021-03-05 07:56:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-6676bcd6d4" is progressing.,LastUpdateTime:2021-03-05 07:56:30 +0000 UTC,LastTransitionTime:2021-03-05 07:56:19 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar  5 07:56:32.227: INFO: New ReplicaSet "webserver-deployment-6676bcd6d4" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-6676bcd6d4  deployment-6936 /apis/apps/v1/namespaces/deployment-6936/replicasets/webserver-deployment-6676bcd6d4 ae67012d-e644-4301-8585-18728f4faea4 5609926 3 2021-03-05 07:56:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment faba103b-db63-4d00-b1b2-29f5559a1e7f 0xc004098467 0xc004098468}] []  [{kube-controller-manager Update apps/v1 2021-03-05 07:56:32 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 97 98 97 49 48 51 98 45 100 98 54 51 45 52 100 48 48 45 98 49 98 50 45 50 57 102 53 53 53 57 97 49 101 55 102 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 6676bcd6d4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004098568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  5 07:56:32.227: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar  5 07:56:32.227: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-84855cf797  deployment-6936 /apis/apps/v1/namespaces/deployment-6936/replicasets/webserver-deployment-84855cf797 f2eb00ea-5d1e-4724-957f-30431ef5aa41 5609924 3 2021-03-05 07:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment faba103b-db63-4d00-b1b2-29f5559a1e7f 0xc0040985f7 0xc0040985f8}] []  [{kube-controller-manager Update apps/v1 2021-03-05 07:56:32 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 97 98 97 49 48 51 98 45 100 98 54 51 45 52 100 48 48 45 98 49 98 50 45 50 57 102 53 53 53 57 97 49 101 55 102 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 84855cf797,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004098688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar  5 07:56:32.240: INFO: Pod "webserver-deployment-6676bcd6d4-4qsvs" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-4qsvs webserver-deployment-6676bcd6d4- deployment-6936 /api/v1/namespaces/deployment-6936/pods/webserver-deployment-6676bcd6d4-4qsvs 0a7d3807-7704-4074-97b5-f247c917c2f9 5609927 0 2021-03-05 07:56:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 ae67012d-e644-4301-8585-18728f4faea4 0xc004098da7 0xc004098da8}] []  [{kube-controller-manager Update v1 2021-03-05 07:56:32 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 101 54 55 48 49 50 100 45 101 54 52 52 45 52 51 48 49 45 56 53 56 53 45 49 56 55 50 56 102 52 102 97 101 97 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvhbx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  5 07:56:32.241: INFO: Pod "webserver-deployment-6676bcd6d4-7zq5h" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-7zq5h webserver-deployment-6676bcd6d4- deployment-6936 /api/v1/namespaces/deployment-6936/pods/webserver-deployment-6676bcd6d4-7zq5h fb780317-e648-4c4c-826f-82305e88597e 5609906 0 2021-03-05 07:56:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:10.244.2.3/32 cni.projectcalico.org/podIPs:10.244.2.3/32] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 ae67012d-e644-4301-8585-18728f4faea4 0xc004098f30 0xc004098f31}] []  [{kube-controller-manager Update v1 2021-03-05 07:56:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 101 54 55 48 49 50 100 45 101 54 52 52 45 52 51 48 49 45 56 53 56 53 45 49 56 55 50 56 102 52 102 97 101 97 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-05 07:56:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2021-03-05 07:56:31 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvhbx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-control-plane-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.5,PodIP:,StartTime:2021-03-05 07:56:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  5 07:56:32.241: INFO: Pod "webserver-deployment-6676bcd6d4-97j7q" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-97j7q webserver-deployment-6676bcd6d4- deployment-6936 /api/v1/namespaces/deployment-6936/pods/webserver-deployment-6676bcd6d4-97j7q 9755fce6-f9f0-43fd-80df-4f450ee7f0da 5609912 0 2021-03-05 07:56:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:10.244.3.39/32 cni.projectcalico.org/podIPs:10.244.3.39/32] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 ae67012d-e644-4301-8585-18728f4faea4 0xc004099157 0xc004099158}] []  [{kube-controller-manager Update v1 2021-03-05 07:56:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 101 54 55 48 49 50 100 45 101 54 52 52 45 52 51 48 49 45 56 53 56 53 45 49 56 55 50 56 102 52 102 97 101 97 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-05 07:56:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2021-03-05 07:56:31 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvhbx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-8tgs6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.8,PodIP:,StartTime:2021-03-05 07:56:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  5 07:56:32.242: INFO: Pod "webserver-deployment-6676bcd6d4-l7x8n" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-l7x8n webserver-deployment-6676bcd6d4- deployment-6936 /api/v1/namespaces/deployment-6936/pods/webserver-deployment-6676bcd6d4-l7x8n 65381f2d-6a36-47e6-b9c7-691f01e76cc1 5609910 0 2021-03-05 07:56:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:10.244.3.37/32 cni.projectcalico.org/podIPs:10.244.3.37/32] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 ae67012d-e644-4301-8585-18728f4faea4 0xc004099347 0xc004099348}] []  [{kube-controller-manager Update v1 2021-03-05 07:56:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 101 54 55 48 49 50 100 45 101 54 52 52 45 52 51 48 49 45 56 53 56 53 45 49 56 55 50 56 102 52 102 97 101 97 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-05 07:56:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2021-03-05 07:56:31 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvhbx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-8tgs6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.8,PodIP:,StartTime:2021-03-05 07:56:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  5 07:56:32.242: INFO: Pod "webserver-deployment-6676bcd6d4-n2wqk" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-n2wqk webserver-deployment-6676bcd6d4- deployment-6936 /api/v1/namespaces/deployment-6936/pods/webserver-deployment-6676bcd6d4-n2wqk 6d5b9531-17fe-47fa-9329-96c4cac7c1c7 5609907 0 2021-03-05 07:56:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:10.244.3.36/32 cni.projectcalico.org/podIPs:10.244.3.36/32] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 ae67012d-e644-4301-8585-18728f4faea4 0xc004099527 0xc004099528}] []  [{kube-controller-manager Update v1 2021-03-05 07:56:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 101 54 55 48 49 50 100 45 101 54 52 52 45 52 51 48 49 45 56 53 56 53 45 49 56 55 50 56 102 52 102 97 101 97 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-05 07:56:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2021-03-05 07:56:31 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvhbx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-8tgs6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.8,PodIP:,StartTime:2021-03-05 07:56:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  5 07:56:32.243: INFO: Pod "webserver-deployment-6676bcd6d4-qtqln" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-qtqln webserver-deployment-6676bcd6d4- deployment-6936 /api/v1/namespaces/deployment-6936/pods/webserver-deployment-6676bcd6d4-qtqln 1a2f0a50-d9d2-4092-aeae-11a63b347fe4 5609913 0 2021-03-05 07:56:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:10.244.3.38/32 cni.projectcalico.org/podIPs:10.244.3.38/32] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 ae67012d-e644-4301-8585-18728f4faea4 0xc004099797 0xc004099798}] []  [{kube-controller-manager Update v1 2021-03-05 07:56:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 101 54 55 48 49 50 100 45 101 54 52 52 45 52 51 48 49 45 56 53 56 53 45 49 56 55 50 56 102 52 102 97 101 97 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-05 07:56:30 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2021-03-05 07:56:31 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvhbx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-8tgs6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.8,PodIP:,StartTime:2021-03-05 07:56:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  5 07:56:32.243: INFO: Pod "webserver-deployment-84855cf797-6qf4c" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-6qf4c webserver-deployment-84855cf797- deployment-6936 /api/v1/namespaces/deployment-6936/pods/webserver-deployment-84855cf797-6qf4c a47d09c4-e1d0-418e-a79d-733772cf48ce 5609797 0 2021-03-05 07:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:10.244.3.33/32 cni.projectcalico.org/podIPs:10.244.3.33/32] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f2eb00ea-5d1e-4724-957f-30431ef5aa41 0xc0040999a7 0xc0040999a8}] []  [{kube-controller-manager Update v1 2021-03-05 07:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 50 101 98 48 48 101 97 45 53 100 49 101 45 52 55 50 52 45 57 53 55 102 45 51 48 52 51 49 101 102 53 97 97 52 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2021-03-05 07:56:22 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2021-03-05 07:56:25 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 51 46 51 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvhbx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-8tgs6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.8,PodIP:10.244.3.33,StartTime:2021-03-05 07:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-05 07:56:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e4aad3b4c2278a679f45d3338b7225239fc7bdc668e35253775df0bcd6d72de8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  5 07:56:32.245: INFO: Pod "webserver-deployment-84855cf797-7rxhr" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-7rxhr webserver-deployment-84855cf797- deployment-6936 /api/v1/namespaces/deployment-6936/pods/webserver-deployment-84855cf797-7rxhr d1a1928c-e802-4352-a14b-d00e5497a050 5609792 0 2021-03-05 07:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:10.244.3.34/32 cni.projectcalico.org/podIPs:10.244.3.34/32] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f2eb00ea-5d1e-4724-957f-30431ef5aa41 0xc004099bf7 0xc004099bf8}] []  [{kube-controller-manager Update v1 2021-03-05 07:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 50 101 98 48 48 101 97 45 53 100 49 101 45 52 55 50 52 45 57 53 55 102 45 51 48 52 51 49 101 102 53 97 97 52 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2021-03-05 07:56:23 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2021-03-05 07:56:24 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 51 46 51 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvhbx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-8tgs6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.8,PodIP:10.244.3.34,StartTime:2021-03-05 07:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-05 07:56:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://56780757a113f34e7517766a7cd47aa9528bb70bec84281ae071ed44578baa9d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  5 07:56:32.245: INFO: Pod "webserver-deployment-84855cf797-b5tbb" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-b5tbb webserver-deployment-84855cf797- deployment-6936 /api/v1/namespaces/deployment-6936/pods/webserver-deployment-84855cf797-b5tbb 3d640412-2a1b-42ad-847e-4bd5b4050fe7 5609931 0 2021-03-05 07:56:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f2eb00ea-5d1e-4724-957f-30431ef5aa41 0xc004099e27 0xc004099e28}] []  [{kube-controller-manager Update v1 2021-03-05 07:56:32 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 50 101 98 48 48 101 97 45 53 100 49 101 45 52 55 50 52 45 57 53 55 102 45 51 48 52 51 49 101 102 53 97 97 52 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvhbx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  5 07:56:32.245: INFO: Pod "webserver-deployment-84855cf797-bbrsp" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-bbrsp webserver-deployment-84855cf797- deployment-6936 /api/v1/namespaces/deployment-6936/pods/webserver-deployment-84855cf797-bbrsp c0fd45b4-2d9e-4673-a090-50c884deb2e1 5609932 0 2021-03-05 07:56:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f2eb00ea-5d1e-4724-957f-30431ef5aa41 0xc004099fa0 0xc004099fa1}] []  [{kube-controller-manager Update v1 2021-03-05 07:56:32 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 50 101 98 48 48 101 97 45 53 100 49 101 45 52 55 50 52 45 57 53 55 102 45 51 48 52 51 49 101 102 53 97 97 52 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvhbx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  5 07:56:32.249: INFO: Pod "webserver-deployment-84855cf797-cfrrj" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-cfrrj webserver-deployment-84855cf797- deployment-6936 /api/v1/namespaces/deployment-6936/pods/webserver-deployment-84855cf797-cfrrj 66757dda-a46e-4ede-94a7-62eaf72107cd 5609928 0 2021-03-05 07:56:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f2eb00ea-5d1e-4724-957f-30431ef5aa41 0xc0041320d0 0xc0041320d1}] []  [{kube-controller-manager Update v1 2021-03-05 07:56:32 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 50 101 98 48 48 101 97 45 53 100 49 101 45 52 55 50 52 45 57 53 55 102 45 51 48 52 51 49 101 102 53 97 97 52 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvhbx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-dwtv7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  5 07:56:32.250: INFO: Pod "webserver-deployment-84855cf797-gt7fg" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-gt7fg webserver-deployment-84855cf797- deployment-6936 /api/v1/namespaces/deployment-6936/pods/webserver-deployment-84855cf797-gt7fg f2f445b7-e5c1-4d77-ac63-44ee30ea3b96 5609845 0 2021-03-05 07:56:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:10.244.5.197/32 cni.projectcalico.org/podIPs:10.244.5.197/32] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f2eb00ea-5d1e-4724-957f-30431ef5aa41 0xc004132287 0xc004132288}] []  [{kube-controller-manager Update v1 2021-03-05 07:56:20 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 50 101 98 48 48 101 97 45 53 100 49 101 45 52 55 50 52 45 57 53 55 102 45 51 48 52 51 49 101 102 53 97 97 52 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2021-03-05 07:56:21 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2021-03-05 07:56:29 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 53 46 49 57 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvhbx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-dwtv7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.6,PodIP:10.244.5.197,StartTime:2021-03-05 07:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-05 07:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://1d01dde0a3d467becbb66394d76a98696524a8e597abb96d616306de9351cbfe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.197,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  5 07:56:32.250: INFO: Pod "webserver-deployment-84855cf797-hsrqw" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-hsrqw webserver-deployment-84855cf797- deployment-6936 /api/v1/namespaces/deployment-6936/pods/webserver-deployment-84855cf797-hsrqw 2cbb98fa-e2ae-4a86-a687-27efae3914de 5609802 0 2021-03-05 07:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:10.244.3.29/32 cni.projectcalico.org/podIPs:10.244.3.29/32] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f2eb00ea-5d1e-4724-957f-30431ef5aa41 0xc004132467 0xc004132468}] []  [{kube-controller-manager Update v1 2021-03-05 07:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 50 101 98 48 48 101 97 45 53 100 49 101 45 52 55 50 52 45 57 53 55 102 45 51 48 52 51 49 101 102 53 97 97 52 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2021-03-05 07:56:21 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2021-03-05 07:56:25 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 51 46 50 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvhbx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-8tgs6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.8,PodIP:10.244.3.29,StartTime:2021-03-05 07:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-05 07:56:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6dc90b59d236022c9da77dd7cc54198763a401ddb9c1063ea190aed8bb8e5a10,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  5 07:56:32.251: INFO: Pod "webserver-deployment-84855cf797-m4vvp" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-m4vvp webserver-deployment-84855cf797- deployment-6936 /api/v1/namespaces/deployment-6936/pods/webserver-deployment-84855cf797-m4vvp a6b84717-50cc-46e0-9ddc-3ea35f3c164d 5609785 0 2021-03-05 07:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:10.244.3.30/32 cni.projectcalico.org/podIPs:10.244.3.30/32] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f2eb00ea-5d1e-4724-957f-30431ef5aa41 0xc004132747 0xc004132748}] []  [{kube-controller-manager Update v1 2021-03-05 07:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 50 101 98 48 48 101 97 45 53 100 49 101 45 52 55 50 52 45 57 53 55 102 45 51 48 52 51 49 101 102 53 97 97 52 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2021-03-05 07:56:22 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2021-03-05 07:56:24 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 51 46 51 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvhbx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-8tgs6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.8,PodIP:10.244.3.30,StartTime:2021-03-05 07:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-05 07:56:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d7a72a8cb0aabaf9b09519f90f9a59d2f33a6d1989af497a4163344c1368b7cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  5 07:56:32.252: INFO: Pod "webserver-deployment-84855cf797-n87ds" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-n87ds webserver-deployment-84855cf797- deployment-6936 /api/v1/namespaces/deployment-6936/pods/webserver-deployment-84855cf797-n87ds 299a6c4c-d835-4632-ac7e-f0b0ed932c44 5609789 0 2021-03-05 07:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:10.244.3.31/32 cni.projectcalico.org/podIPs:10.244.3.31/32] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f2eb00ea-5d1e-4724-957f-30431ef5aa41 0xc004132957 0xc004132958}] []  [{kube-controller-manager Update v1 2021-03-05 07:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 50 101 98 48 48 101 97 45 53 100 49 101 45 52 55 50 52 45 57 53 55 102 45 51 48 52 51 49 101 102 53 97 97 52 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2021-03-05 07:56:22 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2021-03-05 07:56:24 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 51 46 51 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvhbx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-8tgs6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.8,PodIP:10.244.3.31,StartTime:2021-03-05 07:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-05 07:56:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b21647e5c6a3e30cbd9a2d4b463537777baaa690814495d229245f444bd1b3e1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  5 07:56:32.253: INFO: Pod "webserver-deployment-84855cf797-rpngq" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-rpngq webserver-deployment-84855cf797- deployment-6936 /api/v1/namespaces/deployment-6936/pods/webserver-deployment-84855cf797-rpngq 0e7f3d3f-a902-4e1c-9652-fcc7fd086fe2 5609754 0 2021-03-05 07:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:10.244.3.27/32 cni.projectcalico.org/podIPs:10.244.3.27/32] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f2eb00ea-5d1e-4724-957f-30431ef5aa41 0xc004132b87 0xc004132b88}] []  [{kube-controller-manager Update v1 2021-03-05 07:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 50 101 98 48 48 101 97 45 53 100 49 101 45 52 55 50 52 45 57 53 55 102 45 51 48 52 51 49 101 102 53 97 97 52 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2021-03-05 07:56:21 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2021-03-05 07:56:23 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 51 46 50 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvhbx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-8tgs6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.8,PodIP:10.244.3.27,StartTime:2021-03-05 07:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-05 07:56:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e52dba437295b1dbe82dc04356eb6bdd32a571d4bde2163e08f07d73415e8fc0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.27,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  5 07:56:32.253: INFO: Pod "webserver-deployment-84855cf797-zxtgk" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-zxtgk webserver-deployment-84855cf797- deployment-6936 /api/v1/namespaces/deployment-6936/pods/webserver-deployment-84855cf797-zxtgk 9d340fc7-d391-4838-a812-6268af05b859 5609747 0 2021-03-05 07:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:10.244.3.28/32 cni.projectcalico.org/podIPs:10.244.3.28/32] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 f2eb00ea-5d1e-4724-957f-30431ef5aa41 0xc004132d57 0xc004132d58}] []  [{kube-controller-manager Update v1 2021-03-05 07:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 50 101 98 48 48 101 97 45 53 100 49 101 45 52 55 50 52 45 57 53 55 102 45 51 48 52 51 49 101 102 53 97 97 52 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2021-03-05 07:56:21 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2021-03-05 07:56:23 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 51 46 50 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zvhbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zvhbx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zvhbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-8tgs6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 07:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.8,PodIP:10.244.3.28,StartTime:2021-03-05 07:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-05 07:56:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://0c132de027f3d7d49ae3d4f2da71e2452d6cb135fffdb06434f6b8572e4e24c1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:56:32.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6936" for this suite.

â€¢ [SLOW TEST:12.584 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":277,"completed":21,"skipped":407,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:56:32.299: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2042
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar  5 07:56:32.619: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:56:52.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2042" for this suite.

â€¢ [SLOW TEST:19.996 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":277,"completed":22,"skipped":409,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:56:52.298: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3816
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  5 07:56:52.586: INFO: Number of nodes with available pods: 0
Mar  5 07:56:52.587: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 07:56:53.604: INFO: Number of nodes with available pods: 0
Mar  5 07:56:53.604: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 07:56:54.609: INFO: Number of nodes with available pods: 2
Mar  5 07:56:54.609: INFO: Node devops-control-plane-2 is running more than one daemon pod
Mar  5 07:56:55.710: INFO: Number of nodes with available pods: 4
Mar  5 07:56:55.710: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 07:56:56.608: INFO: Number of nodes with available pods: 4
Mar  5 07:56:56.608: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 07:56:57.611: INFO: Number of nodes with available pods: 4
Mar  5 07:56:57.611: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 07:56:58.613: INFO: Number of nodes with available pods: 4
Mar  5 07:56:58.613: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 07:56:59.610: INFO: Number of nodes with available pods: 4
Mar  5 07:56:59.610: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 07:57:00.605: INFO: Number of nodes with available pods: 4
Mar  5 07:57:00.605: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 07:57:01.615: INFO: Number of nodes with available pods: 4
Mar  5 07:57:01.616: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 07:57:02.615: INFO: Number of nodes with available pods: 6
Mar  5 07:57:02.615: INFO: Number of running nodes: 6, number of available pods: 6
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  5 07:57:02.661: INFO: Number of nodes with available pods: 5
Mar  5 07:57:02.661: INFO: Node devops-pool1-6c76d44df9-njb8n is running more than one daemon pod
Mar  5 07:57:03.722: INFO: Number of nodes with available pods: 5
Mar  5 07:57:03.722: INFO: Node devops-pool1-6c76d44df9-njb8n is running more than one daemon pod
Mar  5 07:57:04.688: INFO: Number of nodes with available pods: 6
Mar  5 07:57:04.688: INFO: Number of running nodes: 6, number of available pods: 6
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3816, will wait for the garbage collector to delete the pods
Mar  5 07:57:04.773: INFO: Deleting DaemonSet.extensions daemon-set took: 15.05157ms
Mar  5 07:57:04.873: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.350592ms
Mar  5 07:57:13.779: INFO: Number of nodes with available pods: 0
Mar  5 07:57:13.779: INFO: Number of running nodes: 0, number of available pods: 0
Mar  5 07:57:13.795: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3816/daemonsets","resourceVersion":"5610650"},"items":null}

Mar  5 07:57:13.800: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3816/pods","resourceVersion":"5610650"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:57:13.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3816" for this suite.

â€¢ [SLOW TEST:21.571 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":277,"completed":23,"skipped":426,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:57:13.871: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9010
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  5 07:57:18.150: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  5 07:57:18.158: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  5 07:57:20.158: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  5 07:57:20.172: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  5 07:57:22.158: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  5 07:57:22.166: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:57:22.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9010" for this suite.

â€¢ [SLOW TEST:8.351 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":277,"completed":24,"skipped":443,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:57:22.223: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-e06025c8-4aed-4189-9347-4406cdf472cb-2545
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:57:22.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3818" for this suite.
STEP: Destroying namespace "nspatchtest-e06025c8-4aed-4189-9347-4406cdf472cb-2545" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":277,"completed":25,"skipped":447,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:57:22.851: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3819
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-72de4e3a-15fd-415a-bde1-b6daa8262cf2
STEP: Creating a pod to test consume secrets
Mar  5 07:57:23.063: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-42d7fa5c-5fb1-45cb-b983-452ab33c50f1" in namespace "projected-3819" to be "Succeeded or Failed"
Mar  5 07:57:23.078: INFO: Pod "pod-projected-secrets-42d7fa5c-5fb1-45cb-b983-452ab33c50f1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.079402ms
Mar  5 07:57:25.088: INFO: Pod "pod-projected-secrets-42d7fa5c-5fb1-45cb-b983-452ab33c50f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02482761s
STEP: Saw pod success
Mar  5 07:57:25.089: INFO: Pod "pod-projected-secrets-42d7fa5c-5fb1-45cb-b983-452ab33c50f1" satisfied condition "Succeeded or Failed"
Mar  5 07:57:25.102: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-projected-secrets-42d7fa5c-5fb1-45cb-b983-452ab33c50f1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  5 07:57:25.146: INFO: Waiting for pod pod-projected-secrets-42d7fa5c-5fb1-45cb-b983-452ab33c50f1 to disappear
Mar  5 07:57:25.151: INFO: Pod pod-projected-secrets-42d7fa5c-5fb1-45cb-b983-452ab33c50f1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:57:25.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3819" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":26,"skipped":471,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:57:25.177: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:58:25.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-616" for this suite.

â€¢ [SLOW TEST:60.318 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":277,"completed":27,"skipped":487,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:58:25.498: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-169
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name secret-emptykey-test-8c1a2dfd-a5c9-491f-9b7c-c7c7b22fcd7c
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:58:25.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-169" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":277,"completed":28,"skipped":498,"failed":0}
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:58:25.719: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-707
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  5 07:58:30.078: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  5 07:58:30.084: INFO: Pod pod-with-poststart-http-hook still exists
Mar  5 07:58:32.084: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  5 07:58:32.091: INFO: Pod pod-with-poststart-http-hook still exists
Mar  5 07:58:34.084: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  5 07:58:34.093: INFO: Pod pod-with-poststart-http-hook still exists
Mar  5 07:58:36.087: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  5 07:58:36.103: INFO: Pod pod-with-poststart-http-hook still exists
Mar  5 07:58:38.084: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  5 07:58:38.092: INFO: Pod pod-with-poststart-http-hook still exists
Mar  5 07:58:40.084: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  5 07:58:40.093: INFO: Pod pod-with-poststart-http-hook still exists
Mar  5 07:58:42.084: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  5 07:58:42.091: INFO: Pod pod-with-poststart-http-hook still exists
Mar  5 07:58:44.084: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  5 07:58:44.092: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:58:44.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-707" for this suite.

â€¢ [SLOW TEST:18.403 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":277,"completed":29,"skipped":502,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:58:44.124: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3754
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-3754
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  5 07:58:44.302: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  5 07:58:44.447: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  5 07:58:46.454: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 07:58:48.456: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 07:58:50.456: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 07:58:52.455: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 07:58:54.456: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 07:58:56.454: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 07:58:58.457: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 07:59:00.455: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 07:59:02.455: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  5 07:59:02.468: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  5 07:59:02.481: INFO: The status of Pod netserver-2 is Running (Ready = true)
Mar  5 07:59:02.494: INFO: The status of Pod netserver-3 is Running (Ready = true)
Mar  5 07:59:02.505: INFO: The status of Pod netserver-4 is Running (Ready = false)
Mar  5 07:59:04.511: INFO: The status of Pod netserver-4 is Running (Ready = true)
Mar  5 07:59:04.530: INFO: The status of Pod netserver-5 is Running (Ready = true)
STEP: Creating test pods
Mar  5 07:59:08.581: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.50:8080/dial?request=hostname&protocol=udp&host=10.244.0.233&port=8081&tries=1'] Namespace:pod-network-test-3754 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 07:59:08.581: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 07:59:08.798: INFO: Waiting for responses: map[]
Mar  5 07:59:08.804: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.50:8080/dial?request=hostname&protocol=udp&host=10.244.1.73&port=8081&tries=1'] Namespace:pod-network-test-3754 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 07:59:08.804: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 07:59:09.007: INFO: Waiting for responses: map[]
Mar  5 07:59:09.014: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.50:8080/dial?request=hostname&protocol=udp&host=10.244.2.7&port=8081&tries=1'] Namespace:pod-network-test-3754 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 07:59:09.015: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 07:59:09.228: INFO: Waiting for responses: map[]
Mar  5 07:59:09.237: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.50:8080/dial?request=hostname&protocol=udp&host=10.244.3.49&port=8081&tries=1'] Namespace:pod-network-test-3754 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 07:59:09.237: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 07:59:09.457: INFO: Waiting for responses: map[]
Mar  5 07:59:09.464: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.50:8080/dial?request=hostname&protocol=udp&host=10.244.5.202&port=8081&tries=1'] Namespace:pod-network-test-3754 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 07:59:09.464: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 07:59:09.709: INFO: Waiting for responses: map[]
Mar  5 07:59:09.729: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.50:8080/dial?request=hostname&protocol=udp&host=10.244.4.173&port=8081&tries=1'] Namespace:pod-network-test-3754 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 07:59:09.729: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 07:59:09.946: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:59:09.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3754" for this suite.

â€¢ [SLOW TEST:25.851 seconds]
[sig-network] Networking
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":277,"completed":30,"skipped":523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:59:09.979: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1314
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  5 07:59:12.203: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:59:12.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1314" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":31,"skipped":561,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:59:12.253: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1938
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:59:29.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1938" for this suite.

â€¢ [SLOW TEST:17.319 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":277,"completed":32,"skipped":565,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:59:29.573: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-725
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:59:29.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-725" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
â€¢{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":277,"completed":33,"skipped":576,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:59:29.824: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5460
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 07:59:30.041: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84942643-c4bf-4b6c-bc17-ab01cf311d82" in namespace "projected-5460" to be "Succeeded or Failed"
Mar  5 07:59:30.047: INFO: Pod "downwardapi-volume-84942643-c4bf-4b6c-bc17-ab01cf311d82": Phase="Pending", Reason="", readiness=false. Elapsed: 5.148794ms
Mar  5 07:59:32.053: INFO: Pod "downwardapi-volume-84942643-c4bf-4b6c-bc17-ab01cf311d82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012023446s
STEP: Saw pod success
Mar  5 07:59:32.054: INFO: Pod "downwardapi-volume-84942643-c4bf-4b6c-bc17-ab01cf311d82" satisfied condition "Succeeded or Failed"
Mar  5 07:59:32.060: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-84942643-c4bf-4b6c-bc17-ab01cf311d82 container client-container: <nil>
STEP: delete the pod
Mar  5 07:59:32.111: INFO: Waiting for pod downwardapi-volume-84942643-c4bf-4b6c-bc17-ab01cf311d82 to disappear
Mar  5 07:59:32.122: INFO: Pod downwardapi-volume-84942643-c4bf-4b6c-bc17-ab01cf311d82 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:59:32.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5460" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":277,"completed":34,"skipped":623,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:59:32.168: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-3382/configmap-test-320392c1-d1d0-441d-af70-d5696adc2163
STEP: Creating a pod to test consume configMaps
Mar  5 07:59:32.412: INFO: Waiting up to 5m0s for pod "pod-configmaps-835018d8-a381-4835-9fd5-574be6c862e4" in namespace "configmap-3382" to be "Succeeded or Failed"
Mar  5 07:59:32.428: INFO: Pod "pod-configmaps-835018d8-a381-4835-9fd5-574be6c862e4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.392447ms
Mar  5 07:59:34.440: INFO: Pod "pod-configmaps-835018d8-a381-4835-9fd5-574be6c862e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028522646s
STEP: Saw pod success
Mar  5 07:59:34.440: INFO: Pod "pod-configmaps-835018d8-a381-4835-9fd5-574be6c862e4" satisfied condition "Succeeded or Failed"
Mar  5 07:59:34.448: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-configmaps-835018d8-a381-4835-9fd5-574be6c862e4 container env-test: <nil>
STEP: delete the pod
Mar  5 07:59:34.503: INFO: Waiting for pod pod-configmaps-835018d8-a381-4835-9fd5-574be6c862e4 to disappear
Mar  5 07:59:34.509: INFO: Pod pod-configmaps-835018d8-a381-4835-9fd5-574be6c862e4 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 07:59:34.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3382" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":277,"completed":35,"skipped":655,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 07:59:34.546: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6323
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar  5 07:59:34.748: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 07:59:43.284: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:00:14.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6323" for this suite.

â€¢ [SLOW TEST:39.981 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":277,"completed":36,"skipped":676,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:00:14.528: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1514
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Mar  5 08:00:14.716: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:00:20.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1514" for this suite.

â€¢ [SLOW TEST:5.645 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":277,"completed":37,"skipped":696,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:00:20.175: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5527
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  5 08:00:20.400: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5527 /api/v1/namespaces/watch-5527/configmaps/e2e-watch-test-configmap-a a49899b7-ee18-4c23-8a38-b8745fe4b27a 5612580 0 2021-03-05 08:00:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-05 08:00:20 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  5 08:00:20.400: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5527 /api/v1/namespaces/watch-5527/configmaps/e2e-watch-test-configmap-a a49899b7-ee18-4c23-8a38-b8745fe4b27a 5612580 0 2021-03-05 08:00:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-05 08:00:20 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar  5 08:00:30.420: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5527 /api/v1/namespaces/watch-5527/configmaps/e2e-watch-test-configmap-a a49899b7-ee18-4c23-8a38-b8745fe4b27a 5612670 0 2021-03-05 08:00:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-05 08:00:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  5 08:00:30.421: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5527 /api/v1/namespaces/watch-5527/configmaps/e2e-watch-test-configmap-a a49899b7-ee18-4c23-8a38-b8745fe4b27a 5612670 0 2021-03-05 08:00:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-05 08:00:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  5 08:00:40.436: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5527 /api/v1/namespaces/watch-5527/configmaps/e2e-watch-test-configmap-a a49899b7-ee18-4c23-8a38-b8745fe4b27a 5612738 0 2021-03-05 08:00:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-05 08:00:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  5 08:00:40.436: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5527 /api/v1/namespaces/watch-5527/configmaps/e2e-watch-test-configmap-a a49899b7-ee18-4c23-8a38-b8745fe4b27a 5612738 0 2021-03-05 08:00:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-05 08:00:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar  5 08:00:50.452: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5527 /api/v1/namespaces/watch-5527/configmaps/e2e-watch-test-configmap-a a49899b7-ee18-4c23-8a38-b8745fe4b27a 5612806 0 2021-03-05 08:00:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-05 08:00:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  5 08:00:50.452: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5527 /api/v1/namespaces/watch-5527/configmaps/e2e-watch-test-configmap-a a49899b7-ee18-4c23-8a38-b8745fe4b27a 5612806 0 2021-03-05 08:00:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-05 08:00:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  5 08:01:00.474: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5527 /api/v1/namespaces/watch-5527/configmaps/e2e-watch-test-configmap-b 358cc51a-ce45-4935-82e5-02c59868f4c9 5612877 0 2021-03-05 08:01:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-05 08:01:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  5 08:01:00.474: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5527 /api/v1/namespaces/watch-5527/configmaps/e2e-watch-test-configmap-b 358cc51a-ce45-4935-82e5-02c59868f4c9 5612877 0 2021-03-05 08:01:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-05 08:01:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar  5 08:01:10.491: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5527 /api/v1/namespaces/watch-5527/configmaps/e2e-watch-test-configmap-b 358cc51a-ce45-4935-82e5-02c59868f4c9 5612944 0 2021-03-05 08:01:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-05 08:01:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  5 08:01:10.491: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5527 /api/v1/namespaces/watch-5527/configmaps/e2e-watch-test-configmap-b 358cc51a-ce45-4935-82e5-02c59868f4c9 5612944 0 2021-03-05 08:01:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-05 08:01:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:01:20.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5527" for this suite.

â€¢ [SLOW TEST:60.343 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":277,"completed":38,"skipped":710,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:01:20.518: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3439
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-9fae6c4c-eb04-4b58-9ed2-b341f597690f in namespace container-probe-3439
Mar  5 08:01:22.813: INFO: Started pod busybox-9fae6c4c-eb04-4b58-9ed2-b341f597690f in namespace container-probe-3439
STEP: checking the pod's current state and verifying that restartCount is present
Mar  5 08:01:22.820: INFO: Initial restart count of pod busybox-9fae6c4c-eb04-4b58-9ed2-b341f597690f is 0
Mar  5 08:02:17.070: INFO: Restart count of pod container-probe-3439/busybox-9fae6c4c-eb04-4b58-9ed2-b341f597690f is now 1 (54.250297728s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:02:17.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3439" for this suite.

â€¢ [SLOW TEST:56.604 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":277,"completed":39,"skipped":714,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:02:17.125: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3220
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service endpoint-test2 in namespace services-3220
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3220 to expose endpoints map[]
Mar  5 08:02:17.393: INFO: Get endpoints failed (10.309266ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Mar  5 08:02:18.400: INFO: successfully validated that service endpoint-test2 in namespace services-3220 exposes endpoints map[] (1.016874985s elapsed)
STEP: Creating pod pod1 in namespace services-3220
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3220 to expose endpoints map[pod1:[80]]
Mar  5 08:02:20.457: INFO: successfully validated that service endpoint-test2 in namespace services-3220 exposes endpoints map[pod1:[80]] (2.037061507s elapsed)
STEP: Creating pod pod2 in namespace services-3220
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3220 to expose endpoints map[pod1:[80] pod2:[80]]
Mar  5 08:02:22.550: INFO: successfully validated that service endpoint-test2 in namespace services-3220 exposes endpoints map[pod1:[80] pod2:[80]] (2.074944803s elapsed)
STEP: Deleting pod pod1 in namespace services-3220
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3220 to expose endpoints map[pod2:[80]]
Mar  5 08:02:23.610: INFO: successfully validated that service endpoint-test2 in namespace services-3220 exposes endpoints map[pod2:[80]] (1.045740576s elapsed)
STEP: Deleting pod pod2 in namespace services-3220
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3220 to expose endpoints map[]
Mar  5 08:02:24.647: INFO: successfully validated that service endpoint-test2 in namespace services-3220 exposes endpoints map[] (1.01756428s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:02:24.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3220" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:7.632 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":277,"completed":40,"skipped":729,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:02:24.763: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7867
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-10b07568-f814-4ef1-b524-267d369cb11d
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-10b07568-f814-4ef1-b524-267d369cb11d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:02:29.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7867" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":41,"skipped":744,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:02:29.154: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1427
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar  5 08:02:35.427: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  5 08:02:35.435: INFO: Pod pod-with-prestop-http-hook still exists
Mar  5 08:02:37.435: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  5 08:02:37.443: INFO: Pod pod-with-prestop-http-hook still exists
Mar  5 08:02:39.435: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  5 08:02:39.454: INFO: Pod pod-with-prestop-http-hook still exists
Mar  5 08:02:41.435: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  5 08:02:41.441: INFO: Pod pod-with-prestop-http-hook still exists
Mar  5 08:02:43.435: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  5 08:02:43.448: INFO: Pod pod-with-prestop-http-hook still exists
Mar  5 08:02:45.440: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  5 08:02:45.452: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:02:45.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1427" for this suite.

â€¢ [SLOW TEST:16.378 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":277,"completed":42,"skipped":770,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:02:45.532: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5362
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:02:50.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5362" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":277,"completed":43,"skipped":777,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:02:50.291: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6734
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 08:02:50.515: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8e4b179c-8a94-49ef-9c30-277f7e108ef2" in namespace "downward-api-6734" to be "Succeeded or Failed"
Mar  5 08:02:50.527: INFO: Pod "downwardapi-volume-8e4b179c-8a94-49ef-9c30-277f7e108ef2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.900208ms
Mar  5 08:02:52.534: INFO: Pod "downwardapi-volume-8e4b179c-8a94-49ef-9c30-277f7e108ef2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018792745s
STEP: Saw pod success
Mar  5 08:02:52.534: INFO: Pod "downwardapi-volume-8e4b179c-8a94-49ef-9c30-277f7e108ef2" satisfied condition "Succeeded or Failed"
Mar  5 08:02:52.539: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-8e4b179c-8a94-49ef-9c30-277f7e108ef2 container client-container: <nil>
STEP: delete the pod
Mar  5 08:02:52.578: INFO: Waiting for pod downwardapi-volume-8e4b179c-8a94-49ef-9c30-277f7e108ef2 to disappear
Mar  5 08:02:52.585: INFO: Pod downwardapi-volume-8e4b179c-8a94-49ef-9c30-277f7e108ef2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:02:52.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6734" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":44,"skipped":787,"failed":0}

------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:02:52.618: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5402
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override all
Mar  5 08:02:52.875: INFO: Waiting up to 5m0s for pod "client-containers-e6bfccbf-0446-450c-9677-e56ca8a938c0" in namespace "containers-5402" to be "Succeeded or Failed"
Mar  5 08:02:52.890: INFO: Pod "client-containers-e6bfccbf-0446-450c-9677-e56ca8a938c0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.720086ms
Mar  5 08:02:54.899: INFO: Pod "client-containers-e6bfccbf-0446-450c-9677-e56ca8a938c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024149184s
STEP: Saw pod success
Mar  5 08:02:54.899: INFO: Pod "client-containers-e6bfccbf-0446-450c-9677-e56ca8a938c0" satisfied condition "Succeeded or Failed"
Mar  5 08:02:54.906: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod client-containers-e6bfccbf-0446-450c-9677-e56ca8a938c0 container test-container: <nil>
STEP: delete the pod
Mar  5 08:02:54.951: INFO: Waiting for pod client-containers-e6bfccbf-0446-450c-9677-e56ca8a938c0 to disappear
Mar  5 08:02:54.960: INFO: Pod client-containers-e6bfccbf-0446-450c-9677-e56ca8a938c0 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:02:54.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5402" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":277,"completed":45,"skipped":787,"failed":0}
SSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:02:54.981: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-3056
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar  5 08:02:55.247: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the sample API server.
Mar  5 08:02:55.942: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Mar  5 08:02:58.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 08:03:00.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 08:03:02.055: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 08:03:04.051: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 08:03:06.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 08:03:08.054: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 08:03:10.055: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 08:03:12.055: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528175, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 08:03:18.091: INFO: Waited 4.02141064s for the sample-apiserver to be ready to handle requests.
I0305 08:03:19.161687      26 request.go:621] Throttling request took 1.008262663s, request: GET:https://10.96.0.1:443/apis/coordination.k8s.io/v1beta1?timeout=32s
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:03:20.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3056" for this suite.

â€¢ [SLOW TEST:25.952 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":277,"completed":46,"skipped":790,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:03:20.934: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1085
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar  5 08:03:23.767: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1085 pod-service-account-663058d4-715a-4422-93bc-1a545802bffe -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar  5 08:03:24.566: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1085 pod-service-account-663058d4-715a-4422-93bc-1a545802bffe -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar  5 08:03:24.863: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1085 pod-service-account-663058d4-715a-4422-93bc-1a545802bffe -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:03:25.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1085" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":277,"completed":47,"skipped":798,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:03:25.268: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4615
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:03:29.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4615" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":277,"completed":48,"skipped":835,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:03:29.750: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3419
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  5 08:03:30.432: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  5 08:03:32.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528210, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528210, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528210, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528210, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 08:03:35.530: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:03:35.539: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6489-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:03:37.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3419" for this suite.
STEP: Destroying namespace "webhook-3419-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:7.645 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":277,"completed":49,"skipped":837,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:03:37.398: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1063
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Mar  5 08:03:37.968: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:03:41.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1063" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":277,"completed":50,"skipped":851,"failed":0}

------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:03:41.308: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3364
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-b3354deb-ecdc-44d2-aba5-80c90ed90e23
STEP: Creating a pod to test consume secrets
Mar  5 08:03:41.580: INFO: Waiting up to 5m0s for pod "pod-secrets-327e7f1a-3745-4dc4-a3b1-741cc1df956c" in namespace "secrets-3364" to be "Succeeded or Failed"
Mar  5 08:03:41.598: INFO: Pod "pod-secrets-327e7f1a-3745-4dc4-a3b1-741cc1df956c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.246051ms
Mar  5 08:03:43.604: INFO: Pod "pod-secrets-327e7f1a-3745-4dc4-a3b1-741cc1df956c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012039028s
STEP: Saw pod success
Mar  5 08:03:43.605: INFO: Pod "pod-secrets-327e7f1a-3745-4dc4-a3b1-741cc1df956c" satisfied condition "Succeeded or Failed"
Mar  5 08:03:43.741: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-secrets-327e7f1a-3745-4dc4-a3b1-741cc1df956c container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 08:03:43.822: INFO: Waiting for pod pod-secrets-327e7f1a-3745-4dc4-a3b1-741cc1df956c to disappear
Mar  5 08:03:43.832: INFO: Pod pod-secrets-327e7f1a-3745-4dc4-a3b1-741cc1df956c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:03:43.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3364" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":51,"skipped":851,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:03:43.859: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8499
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar  5 08:03:44.315: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8499 /api/v1/namespaces/watch-8499/configmaps/e2e-watch-test-watch-closed 06084074-2fe5-4988-b51a-89546864b759 5614706 0 2021-03-05 08:03:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-05 08:03:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  5 08:03:44.315: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8499 /api/v1/namespaces/watch-8499/configmaps/e2e-watch-test-watch-closed 06084074-2fe5-4988-b51a-89546864b759 5614708 0 2021-03-05 08:03:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-05 08:03:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar  5 08:03:44.351: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8499 /api/v1/namespaces/watch-8499/configmaps/e2e-watch-test-watch-closed 06084074-2fe5-4988-b51a-89546864b759 5614710 0 2021-03-05 08:03:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-05 08:03:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  5 08:03:44.352: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8499 /api/v1/namespaces/watch-8499/configmaps/e2e-watch-test-watch-closed 06084074-2fe5-4988-b51a-89546864b759 5614711 0 2021-03-05 08:03:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-05 08:03:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:03:44.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8499" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":277,"completed":52,"skipped":852,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:03:44.387: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4781
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  5 08:03:44.724: INFO: Number of nodes with available pods: 0
Mar  5 08:03:44.724: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 08:03:45.745: INFO: Number of nodes with available pods: 0
Mar  5 08:03:45.745: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 08:03:46.744: INFO: Number of nodes with available pods: 3
Mar  5 08:03:46.744: INFO: Node devops-control-plane-2 is running more than one daemon pod
Mar  5 08:03:47.744: INFO: Number of nodes with available pods: 6
Mar  5 08:03:47.744: INFO: Number of running nodes: 6, number of available pods: 6
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar  5 08:03:47.796: INFO: Number of nodes with available pods: 5
Mar  5 08:03:47.796: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 08:03:48.835: INFO: Number of nodes with available pods: 5
Mar  5 08:03:48.835: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 08:03:49.830: INFO: Number of nodes with available pods: 5
Mar  5 08:03:49.830: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 08:03:50.815: INFO: Number of nodes with available pods: 5
Mar  5 08:03:50.815: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 08:03:51.821: INFO: Number of nodes with available pods: 5
Mar  5 08:03:51.821: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 08:03:52.826: INFO: Number of nodes with available pods: 5
Mar  5 08:03:52.826: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 08:03:53.816: INFO: Number of nodes with available pods: 5
Mar  5 08:03:53.816: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 08:03:54.819: INFO: Number of nodes with available pods: 5
Mar  5 08:03:54.819: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 08:03:55.820: INFO: Number of nodes with available pods: 5
Mar  5 08:03:55.820: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 08:03:56.819: INFO: Number of nodes with available pods: 5
Mar  5 08:03:56.819: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 08:03:57.821: INFO: Number of nodes with available pods: 5
Mar  5 08:03:57.821: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 08:03:58.820: INFO: Number of nodes with available pods: 5
Mar  5 08:03:58.820: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 08:03:59.811: INFO: Number of nodes with available pods: 5
Mar  5 08:03:59.811: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 08:04:00.823: INFO: Number of nodes with available pods: 5
Mar  5 08:04:00.823: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 08:04:01.815: INFO: Number of nodes with available pods: 5
Mar  5 08:04:01.815: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 08:04:02.816: INFO: Number of nodes with available pods: 5
Mar  5 08:04:02.816: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 08:04:03.823: INFO: Number of nodes with available pods: 5
Mar  5 08:04:03.823: INFO: Node devops-control-plane-3 is running more than one daemon pod
Mar  5 08:04:04.826: INFO: Number of nodes with available pods: 6
Mar  5 08:04:04.826: INFO: Number of running nodes: 6, number of available pods: 6
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4781, will wait for the garbage collector to delete the pods
Mar  5 08:04:04.917: INFO: Deleting DaemonSet.extensions daemon-set took: 16.67424ms
Mar  5 08:04:05.021: INFO: Terminating DaemonSet.extensions daemon-set pods took: 103.717929ms
Mar  5 08:04:13.729: INFO: Number of nodes with available pods: 0
Mar  5 08:04:13.730: INFO: Number of running nodes: 0, number of available pods: 0
Mar  5 08:04:13.738: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4781/daemonsets","resourceVersion":"5615244"},"items":null}

Mar  5 08:04:13.745: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4781/pods","resourceVersion":"5615244"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:04:13.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4781" for this suite.

â€¢ [SLOW TEST:29.451 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":277,"completed":53,"skipped":878,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:04:13.841: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3106
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:04:14.307: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2755c64e-f077-42f4-a88c-0c3d30715b83" in namespace "security-context-test-3106" to be "Succeeded or Failed"
Mar  5 08:04:14.314: INFO: Pod "alpine-nnp-false-2755c64e-f077-42f4-a88c-0c3d30715b83": Phase="Pending", Reason="", readiness=false. Elapsed: 7.019944ms
Mar  5 08:04:16.320: INFO: Pod "alpine-nnp-false-2755c64e-f077-42f4-a88c-0c3d30715b83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013618206s
Mar  5 08:04:18.330: INFO: Pod "alpine-nnp-false-2755c64e-f077-42f4-a88c-0c3d30715b83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023297964s
Mar  5 08:04:20.339: INFO: Pod "alpine-nnp-false-2755c64e-f077-42f4-a88c-0c3d30715b83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0320598s
Mar  5 08:04:20.339: INFO: Pod "alpine-nnp-false-2755c64e-f077-42f4-a88c-0c3d30715b83" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:04:20.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3106" for this suite.

â€¢ [SLOW TEST:6.540 seconds]
[k8s.io] Security Context
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":54,"skipped":904,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:04:20.382: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2876
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-3c04fb32-e690-4c97-bda9-393815ac2f17
STEP: Creating a pod to test consume configMaps
Mar  5 08:04:20.606: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dd4e6470-8a1d-4c48-b4a9-2044890e1de1" in namespace "projected-2876" to be "Succeeded or Failed"
Mar  5 08:04:20.617: INFO: Pod "pod-projected-configmaps-dd4e6470-8a1d-4c48-b4a9-2044890e1de1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.859019ms
Mar  5 08:04:22.625: INFO: Pod "pod-projected-configmaps-dd4e6470-8a1d-4c48-b4a9-2044890e1de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017995239s
STEP: Saw pod success
Mar  5 08:04:22.625: INFO: Pod "pod-projected-configmaps-dd4e6470-8a1d-4c48-b4a9-2044890e1de1" satisfied condition "Succeeded or Failed"
Mar  5 08:04:22.637: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-projected-configmaps-dd4e6470-8a1d-4c48-b4a9-2044890e1de1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 08:04:22.674: INFO: Waiting for pod pod-projected-configmaps-dd4e6470-8a1d-4c48-b4a9-2044890e1de1 to disappear
Mar  5 08:04:22.679: INFO: Pod pod-projected-configmaps-dd4e6470-8a1d-4c48-b4a9-2044890e1de1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:04:22.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2876" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":277,"completed":55,"skipped":916,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:04:22.704: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6098
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:04:34.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6098" for this suite.

â€¢ [SLOW TEST:11.370 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":277,"completed":56,"skipped":933,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:04:34.076: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9302
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-05d671e6-ef0c-45cc-b399-d49f44089c14
STEP: Creating a pod to test consume configMaps
Mar  5 08:04:34.317: INFO: Waiting up to 5m0s for pod "pod-configmaps-33841aaa-8ccc-4e06-bb3d-4521bb1bebc1" in namespace "configmap-9302" to be "Succeeded or Failed"
Mar  5 08:04:34.324: INFO: Pod "pod-configmaps-33841aaa-8ccc-4e06-bb3d-4521bb1bebc1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.06013ms
Mar  5 08:04:36.338: INFO: Pod "pod-configmaps-33841aaa-8ccc-4e06-bb3d-4521bb1bebc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021466502s
STEP: Saw pod success
Mar  5 08:04:36.338: INFO: Pod "pod-configmaps-33841aaa-8ccc-4e06-bb3d-4521bb1bebc1" satisfied condition "Succeeded or Failed"
Mar  5 08:04:36.354: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-configmaps-33841aaa-8ccc-4e06-bb3d-4521bb1bebc1 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 08:04:36.402: INFO: Waiting for pod pod-configmaps-33841aaa-8ccc-4e06-bb3d-4521bb1bebc1 to disappear
Mar  5 08:04:36.410: INFO: Pod pod-configmaps-33841aaa-8ccc-4e06-bb3d-4521bb1bebc1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:04:36.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9302" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":277,"completed":57,"skipped":986,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:04:36.428: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3627
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:04:36.658: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar  5 08:04:45.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-3627 create -f -'
Mar  5 08:04:46.496: INFO: stderr: ""
Mar  5 08:04:46.496: INFO: stdout: "e2e-test-crd-publish-openapi-5603-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  5 08:04:46.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-3627 delete e2e-test-crd-publish-openapi-5603-crds test-foo'
Mar  5 08:04:46.634: INFO: stderr: ""
Mar  5 08:04:46.634: INFO: stdout: "e2e-test-crd-publish-openapi-5603-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar  5 08:04:46.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-3627 apply -f -'
Mar  5 08:04:47.255: INFO: stderr: ""
Mar  5 08:04:47.255: INFO: stdout: "e2e-test-crd-publish-openapi-5603-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  5 08:04:47.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-3627 delete e2e-test-crd-publish-openapi-5603-crds test-foo'
Mar  5 08:04:47.400: INFO: stderr: ""
Mar  5 08:04:47.400: INFO: stdout: "e2e-test-crd-publish-openapi-5603-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar  5 08:04:47.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-3627 create -f -'
Mar  5 08:04:47.946: INFO: rc: 1
Mar  5 08:04:47.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-3627 apply -f -'
Mar  5 08:04:48.489: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar  5 08:04:48.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-3627 create -f -'
Mar  5 08:04:49.046: INFO: rc: 1
Mar  5 08:04:49.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-3627 apply -f -'
Mar  5 08:04:49.537: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar  5 08:04:49.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 explain e2e-test-crd-publish-openapi-5603-crds'
Mar  5 08:04:50.083: INFO: stderr: ""
Mar  5 08:04:50.083: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5603-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar  5 08:04:50.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 explain e2e-test-crd-publish-openapi-5603-crds.metadata'
Mar  5 08:04:50.595: INFO: stderr: ""
Mar  5 08:04:50.595: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5603-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar  5 08:04:50.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 explain e2e-test-crd-publish-openapi-5603-crds.spec'
Mar  5 08:04:51.114: INFO: stderr: ""
Mar  5 08:04:51.114: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5603-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar  5 08:04:51.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 explain e2e-test-crd-publish-openapi-5603-crds.spec.bars'
Mar  5 08:04:51.607: INFO: stderr: ""
Mar  5 08:04:51.607: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5603-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar  5 08:04:51.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 explain e2e-test-crd-publish-openapi-5603-crds.spec.bars2'
Mar  5 08:04:52.192: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:05:00.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3627" for this suite.

â€¢ [SLOW TEST:24.148 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":277,"completed":58,"skipped":995,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:05:00.578: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3529
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  5 08:05:00.854: INFO: Waiting up to 5m0s for pod "pod-923041b5-1daf-4222-8634-ccd58389a14e" in namespace "emptydir-3529" to be "Succeeded or Failed"
Mar  5 08:05:00.859: INFO: Pod "pod-923041b5-1daf-4222-8634-ccd58389a14e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.080323ms
Mar  5 08:05:02.868: INFO: Pod "pod-923041b5-1daf-4222-8634-ccd58389a14e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013824177s
Mar  5 08:05:04.879: INFO: Pod "pod-923041b5-1daf-4222-8634-ccd58389a14e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024527106s
STEP: Saw pod success
Mar  5 08:05:04.879: INFO: Pod "pod-923041b5-1daf-4222-8634-ccd58389a14e" satisfied condition "Succeeded or Failed"
Mar  5 08:05:04.884: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-923041b5-1daf-4222-8634-ccd58389a14e container test-container: <nil>
STEP: delete the pod
Mar  5 08:05:04.926: INFO: Waiting for pod pod-923041b5-1daf-4222-8634-ccd58389a14e to disappear
Mar  5 08:05:04.931: INFO: Pod pod-923041b5-1daf-4222-8634-ccd58389a14e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:05:04.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3529" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":59,"skipped":1006,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:05:04.950: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-23
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  5 08:05:05.178: INFO: Waiting up to 5m0s for pod "pod-c9acd71e-d284-4fe1-a545-e6dce8a7694a" in namespace "emptydir-23" to be "Succeeded or Failed"
Mar  5 08:05:05.198: INFO: Pod "pod-c9acd71e-d284-4fe1-a545-e6dce8a7694a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.33665ms
Mar  5 08:05:07.206: INFO: Pod "pod-c9acd71e-d284-4fe1-a545-e6dce8a7694a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027629755s
STEP: Saw pod success
Mar  5 08:05:07.206: INFO: Pod "pod-c9acd71e-d284-4fe1-a545-e6dce8a7694a" satisfied condition "Succeeded or Failed"
Mar  5 08:05:07.214: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-c9acd71e-d284-4fe1-a545-e6dce8a7694a container test-container: <nil>
STEP: delete the pod
Mar  5 08:05:07.258: INFO: Waiting for pod pod-c9acd71e-d284-4fe1-a545-e6dce8a7694a to disappear
Mar  5 08:05:07.264: INFO: Pod pod-c9acd71e-d284-4fe1-a545-e6dce8a7694a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:05:07.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-23" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":60,"skipped":1011,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:05:07.297: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1117
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating api versions
Mar  5 08:05:07.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 api-versions'
Mar  5 08:05:07.625: INFO: stderr: ""
Mar  5 08:05:07.625: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napp.k8s.io/v1beta1\napps/v1\nargoproj.io/v1alpha1\nauditing.kubesphere.io/v1alpha1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncaching.internal.knative.dev/v1alpha1\nceph.rook.io/v1\ncertificates.k8s.io/v1beta1\ncluster.k8s.io/v1alpha1\ncluster.kubesphere.io/v1alpha1\nconfig.istio.io/v1alpha2\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndashboard.tekton.dev/v1alpha1\ndevops.kubesphere.io/v1alpha1\ndevops.kubesphere.io/v1alpha3\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nevents.kubesphere.io/v1alpha1\nextensions/v1beta1\niam.kubesphere.io/v1alpha2\ninstall.istio.io/v1alpha1\ninstaller.kubesphere.io/v1alpha1\njaegertracing.io/v1\nkiali.io/v1alpha1\nlogging.kubesphere.io/v1alpha2\nmonitoring.coreos.com/v1\nmonitoring.kiali.io/v1alpha1\nmonitoring.kubesphere.io/v1alpha1\nnetwork.kubesphere.io/v1alpha1\nnetworking.istio.io/v1alpha3\nnetworking.istio.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\nnotification.kubesphere.io/v1alpha1\nobjectbucket.io/v1alpha1\noperator.tigera.io/v1\npolicy/v1beta1\nquota.kubesphere.io/v1alpha2\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nrbac.istio.io/v1alpha1\nrook.io/v1alpha2\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.istio.io/v1beta1\nservicemesh.kubesphere.io/v1alpha2\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nstorage.kubesphere.io/v1alpha1\ntekton.dev/v1alpha1\ntekton.dev/v1beta1\ntenant.kubesphere.io/v1alpha1\ntenant.kubesphere.io/v1alpha2\ntriggers.tekton.dev/v1alpha1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:05:07.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1117" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":277,"completed":61,"skipped":1034,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:05:07.648: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7781
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-7781
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating statefulset ss in namespace statefulset-7781
Mar  5 08:05:07.926: INFO: Found 0 stateful pods, waiting for 1
Mar  5 08:05:17.936: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar  5 08:05:17.982: INFO: Deleting all statefulset in ns statefulset-7781
Mar  5 08:05:17.987: INFO: Scaling statefulset ss to 0
Mar  5 08:05:28.033: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 08:05:28.045: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:05:28.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7781" for this suite.

â€¢ [SLOW TEST:20.471 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":277,"completed":62,"skipped":1044,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:05:28.122: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4338
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar  5 08:05:28.330: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  5 08:05:33.340: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:05:34.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4338" for this suite.

â€¢ [SLOW TEST:6.272 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":277,"completed":63,"skipped":1085,"failed":0}
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:05:34.394: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:06:00.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4484" for this suite.

â€¢ [SLOW TEST:25.706 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  blackbox test
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
    when starting a container that exits
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":277,"completed":64,"skipped":1085,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:06:00.102: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5913
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  5 08:06:00.314: INFO: Waiting up to 5m0s for pod "pod-5ae88c6e-b492-474a-b7a8-ee261dc63d1f" in namespace "emptydir-5913" to be "Succeeded or Failed"
Mar  5 08:06:00.323: INFO: Pod "pod-5ae88c6e-b492-474a-b7a8-ee261dc63d1f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.428462ms
Mar  5 08:06:02.331: INFO: Pod "pod-5ae88c6e-b492-474a-b7a8-ee261dc63d1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015755319s
Mar  5 08:06:04.340: INFO: Pod "pod-5ae88c6e-b492-474a-b7a8-ee261dc63d1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024331209s
STEP: Saw pod success
Mar  5 08:06:04.340: INFO: Pod "pod-5ae88c6e-b492-474a-b7a8-ee261dc63d1f" satisfied condition "Succeeded or Failed"
Mar  5 08:06:04.347: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-5ae88c6e-b492-474a-b7a8-ee261dc63d1f container test-container: <nil>
STEP: delete the pod
Mar  5 08:06:04.393: INFO: Waiting for pod pod-5ae88c6e-b492-474a-b7a8-ee261dc63d1f to disappear
Mar  5 08:06:04.398: INFO: Pod pod-5ae88c6e-b492-474a-b7a8-ee261dc63d1f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:06:04.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5913" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":65,"skipped":1106,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:06:04.423: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-1001
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar  5 08:06:05.035: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar  5 08:06:07.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528365, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528365, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528365, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528365, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 08:06:10.105: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:06:10.112: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:06:11.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1001" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:7.110 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":277,"completed":66,"skipped":1134,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:06:11.534: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-692
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar  5 08:06:12.372: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:06:22.204: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:06:52.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-692" for this suite.

â€¢ [SLOW TEST:41.345 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":277,"completed":67,"skipped":1212,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:06:52.880: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar  5 08:06:53.162: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4856 /api/v1/namespaces/watch-4856/configmaps/e2e-watch-test-label-changed 35b74490-ac6c-4569-8825-8c07440061ed 5617012 0 2021-03-05 08:06:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-05 08:06:53 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  5 08:06:53.162: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4856 /api/v1/namespaces/watch-4856/configmaps/e2e-watch-test-label-changed 35b74490-ac6c-4569-8825-8c07440061ed 5617013 0 2021-03-05 08:06:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-05 08:06:53 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  5 08:06:53.163: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4856 /api/v1/namespaces/watch-4856/configmaps/e2e-watch-test-label-changed 35b74490-ac6c-4569-8825-8c07440061ed 5617014 0 2021-03-05 08:06:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-05 08:06:53 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar  5 08:07:03.270: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4856 /api/v1/namespaces/watch-4856/configmaps/e2e-watch-test-label-changed 35b74490-ac6c-4569-8825-8c07440061ed 5617091 0 2021-03-05 08:06:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-05 08:07:03 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  5 08:07:03.272: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4856 /api/v1/namespaces/watch-4856/configmaps/e2e-watch-test-label-changed 35b74490-ac6c-4569-8825-8c07440061ed 5617092 0 2021-03-05 08:06:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-05 08:07:03 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  5 08:07:03.272: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4856 /api/v1/namespaces/watch-4856/configmaps/e2e-watch-test-label-changed 35b74490-ac6c-4569-8825-8c07440061ed 5617093 0 2021-03-05 08:06:53 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-05 08:07:03 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:07:03.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4856" for this suite.

â€¢ [SLOW TEST:10.418 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":277,"completed":68,"skipped":1219,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:07:03.300: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6245
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-7be28040-2423-40b5-891c-7524bfb5969e
STEP: Creating a pod to test consume secrets
Mar  5 08:07:03.592: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-32f0c473-7db6-445f-b1af-3b9e611c9950" in namespace "projected-6245" to be "Succeeded or Failed"
Mar  5 08:07:03.623: INFO: Pod "pod-projected-secrets-32f0c473-7db6-445f-b1af-3b9e611c9950": Phase="Pending", Reason="", readiness=false. Elapsed: 30.991117ms
Mar  5 08:07:05.631: INFO: Pod "pod-projected-secrets-32f0c473-7db6-445f-b1af-3b9e611c9950": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038767134s
Mar  5 08:07:07.643: INFO: Pod "pod-projected-secrets-32f0c473-7db6-445f-b1af-3b9e611c9950": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050560947s
STEP: Saw pod success
Mar  5 08:07:07.643: INFO: Pod "pod-projected-secrets-32f0c473-7db6-445f-b1af-3b9e611c9950" satisfied condition "Succeeded or Failed"
Mar  5 08:07:07.651: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-projected-secrets-32f0c473-7db6-445f-b1af-3b9e611c9950 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  5 08:07:07.694: INFO: Waiting for pod pod-projected-secrets-32f0c473-7db6-445f-b1af-3b9e611c9950 to disappear
Mar  5 08:07:07.703: INFO: Pod pod-projected-secrets-32f0c473-7db6-445f-b1af-3b9e611c9950 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:07:07.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6245" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":69,"skipped":1223,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:07:07.729: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  5 08:07:08.481: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  5 08:07:10.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528428, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528428, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528428, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750528428, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 08:07:13.540: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:07:23.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4735" for this suite.
STEP: Destroying namespace "webhook-4735-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:16.324 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":277,"completed":70,"skipped":1232,"failed":0}
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:07:24.053: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9827
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  5 08:07:31.008: INFO: Successfully updated pod "pod-update-activedeadlineseconds-acd100a6-e12c-463f-8ea9-37e4c4eb0acf"
Mar  5 08:07:31.008: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-acd100a6-e12c-463f-8ea9-37e4c4eb0acf" in namespace "pods-9827" to be "terminated due to deadline exceeded"
Mar  5 08:07:31.015: INFO: Pod "pod-update-activedeadlineseconds-acd100a6-e12c-463f-8ea9-37e4c4eb0acf": Phase="Running", Reason="", readiness=true. Elapsed: 7.17433ms
Mar  5 08:07:33.023: INFO: Pod "pod-update-activedeadlineseconds-acd100a6-e12c-463f-8ea9-37e4c4eb0acf": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.014574487s
Mar  5 08:07:33.023: INFO: Pod "pod-update-activedeadlineseconds-acd100a6-e12c-463f-8ea9-37e4c4eb0acf" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:07:33.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9827" for this suite.

â€¢ [SLOW TEST:9.002 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":277,"completed":71,"skipped":1232,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:07:33.056: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8578
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar  5 08:07:33.294: INFO: Waiting up to 5m0s for pod "downward-api-19bafe08-18e4-4f1f-aa65-9128a994f9a4" in namespace "downward-api-8578" to be "Succeeded or Failed"
Mar  5 08:07:33.300: INFO: Pod "downward-api-19bafe08-18e4-4f1f-aa65-9128a994f9a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.20799ms
Mar  5 08:07:35.307: INFO: Pod "downward-api-19bafe08-18e4-4f1f-aa65-9128a994f9a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013429614s
STEP: Saw pod success
Mar  5 08:07:35.307: INFO: Pod "downward-api-19bafe08-18e4-4f1f-aa65-9128a994f9a4" satisfied condition "Succeeded or Failed"
Mar  5 08:07:35.315: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downward-api-19bafe08-18e4-4f1f-aa65-9128a994f9a4 container dapi-container: <nil>
STEP: delete the pod
Mar  5 08:07:35.354: INFO: Waiting for pod downward-api-19bafe08-18e4-4f1f-aa65-9128a994f9a4 to disappear
Mar  5 08:07:35.359: INFO: Pod downward-api-19bafe08-18e4-4f1f-aa65-9128a994f9a4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:07:35.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8578" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":277,"completed":72,"skipped":1251,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:07:35.388: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5749
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:07:35.615: INFO: (0) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 21.68476ms)
Mar  5 08:07:35.623: INFO: (1) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.039712ms)
Mar  5 08:07:35.632: INFO: (2) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.703131ms)
Mar  5 08:07:35.640: INFO: (3) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.093605ms)
Mar  5 08:07:35.649: INFO: (4) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.356611ms)
Mar  5 08:07:35.656: INFO: (5) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.819525ms)
Mar  5 08:07:35.663: INFO: (6) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.632713ms)
Mar  5 08:07:35.669: INFO: (7) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.024077ms)
Mar  5 08:07:35.678: INFO: (8) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.015669ms)
Mar  5 08:07:35.685: INFO: (9) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.936715ms)
Mar  5 08:07:35.692: INFO: (10) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.401052ms)
Mar  5 08:07:35.701: INFO: (11) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.518764ms)
Mar  5 08:07:35.709: INFO: (12) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.725095ms)
Mar  5 08:07:35.719: INFO: (13) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 9.260443ms)
Mar  5 08:07:35.728: INFO: (14) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.583438ms)
Mar  5 08:07:35.738: INFO: (15) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 10.111145ms)
Mar  5 08:07:35.746: INFO: (16) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.609865ms)
Mar  5 08:07:35.756: INFO: (17) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 10.703319ms)
Mar  5 08:07:35.764: INFO: (18) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.100685ms)
Mar  5 08:07:35.772: INFO: (19) /api/v1/nodes/devops-pool1-6c76d44df9-dwtv7/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.084495ms)
[AfterEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:07:35.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5749" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":277,"completed":73,"skipped":1260,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:07:35.801: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-fe51c25a-0fce-4eb5-bb29-cf3438ce1125 in namespace container-probe-2489
Mar  5 08:07:38.074: INFO: Started pod liveness-fe51c25a-0fce-4eb5-bb29-cf3438ce1125 in namespace container-probe-2489
STEP: checking the pod's current state and verifying that restartCount is present
Mar  5 08:07:38.085: INFO: Initial restart count of pod liveness-fe51c25a-0fce-4eb5-bb29-cf3438ce1125 is 0
Mar  5 08:08:02.215: INFO: Restart count of pod container-probe-2489/liveness-fe51c25a-0fce-4eb5-bb29-cf3438ce1125 is now 1 (24.130565662s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:08:02.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2489" for this suite.

â€¢ [SLOW TEST:26.465 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":277,"completed":74,"skipped":1283,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:08:02.267: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7435
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  5 08:08:03.103: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 08:08:06.163: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:08:06.174: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-883-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:08:07.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7435" for this suite.
STEP: Destroying namespace "webhook-7435-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.285 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":277,"completed":75,"skipped":1287,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:08:07.553: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7940
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar  5 08:08:10.886: INFO: Successfully updated pod "adopt-release-55nl6"
STEP: Checking that the Job readopts the Pod
Mar  5 08:08:10.886: INFO: Waiting up to 15m0s for pod "adopt-release-55nl6" in namespace "job-7940" to be "adopted"
Mar  5 08:08:10.894: INFO: Pod "adopt-release-55nl6": Phase="Running", Reason="", readiness=true. Elapsed: 8.194524ms
Mar  5 08:08:12.900: INFO: Pod "adopt-release-55nl6": Phase="Running", Reason="", readiness=true. Elapsed: 2.014399777s
Mar  5 08:08:12.901: INFO: Pod "adopt-release-55nl6" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar  5 08:08:13.420: INFO: Successfully updated pod "adopt-release-55nl6"
STEP: Checking that the Job releases the Pod
Mar  5 08:08:13.421: INFO: Waiting up to 15m0s for pod "adopt-release-55nl6" in namespace "job-7940" to be "released"
Mar  5 08:08:13.489: INFO: Pod "adopt-release-55nl6": Phase="Running", Reason="", readiness=true. Elapsed: 68.366163ms
Mar  5 08:08:13.489: INFO: Pod "adopt-release-55nl6" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:08:13.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7940" for this suite.

â€¢ [SLOW TEST:5.976 seconds]
[sig-apps] Job
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":277,"completed":76,"skipped":1311,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:08:13.531: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5927
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar  5 08:08:13.735: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  5 08:08:13.776: INFO: Waiting for terminating namespaces to be deleted...
Mar  5 08:08:13.784: INFO: 
Logging pods the kubelet thinks is on node devops-control-plane-1 before test
Mar  5 08:08:13.829: INFO: kube-apiserver-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar  5 08:08:13.829: INFO: csi-cephfsplugin-provisioner-c68f789b8-2rdqm from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (6 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container csi-attacher ready: true, restart count 10
Mar  5 08:08:13.829: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:08:13.829: INFO: 	Container csi-provisioner ready: true, restart count 13
Mar  5 08:08:13.829: INFO: 	Container csi-resizer ready: true, restart count 11
Mar  5 08:08:13.829: INFO: 	Container csi-snapshotter ready: true, restart count 19
Mar  5 08:08:13.829: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:08:13.829: INFO: istiod-1-6-10-7d869d7f58-tg6zc from istio-system started at 2021-03-01 07:43:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container discovery ready: true, restart count 1
Mar  5 08:08:13.829: INFO: rook-ceph-mon-d-c99989cc-lrjlz from rook-ceph started at 2021-03-04 08:21:35 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container mon ready: true, restart count 1
Mar  5 08:08:13.829: INFO: harbor-da5q24-harbor-jobservice-b55d6b698-8zwxd from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container jobservice ready: true, restart count 2
Mar  5 08:08:13.829: INFO: rook-ceph-osd-prepare-devops-control-plane-1-fzs7q from rook-ceph started at 2021-03-04 11:34:43 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container provision ready: false, restart count 0
Mar  5 08:08:13.829: INFO: csi-cephfsplugin-8sbwl from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:08:13.829: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:08:13.829: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:08:13.829: INFO: redis-ha-haproxy-5c6559d588-b2n68 from kubesphere-system started at 2021-03-04 08:11:54 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container haproxy ready: true, restart count 2
Mar  5 08:08:13.829: INFO: csi-rbdplugin-l7fcg from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:08:13.829: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:08:13.829: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:08:13.829: INFO: details-v1-dc74fc894-m9kzk from serv-mesh-ex started at 2021-03-01 08:46:17 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container details ready: true, restart count 1
Mar  5 08:08:13.829: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:08:13.829: INFO: harbor-da5q24-harbor-notary-signer-6785bf5b4-6w2dq from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container notary-signer ready: true, restart count 2
Mar  5 08:08:13.829: INFO: kube-proxy-v54pf from kube-system started at 2021-02-23 18:20:31 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:08:13.829: INFO: node-local-dns-27gsv from kube-system started at 2021-02-23 18:22:38 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:08:13.829: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-4n6zm from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:08:13.829: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:08:13.829: INFO: ratings-v1-675894856b-tjjdl from serv-mesh-ex started at 2021-03-01 08:46:17 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:08:13.829: INFO: 	Container ratings ready: true, restart count 1
Mar  5 08:08:13.829: INFO: enginsgungor-tutorial-deployment-late-e4m-3f096c9b3046-jobgsk6b from tutorial-s2i started at 2021-03-02 18:05:58 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:08:13.829: INFO: harbor-da5q24-harbor-database-0 from pipeline-tools started at 2021-03-04 10:01:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container database ready: true, restart count 1
Mar  5 08:08:13.829: INFO: rook-ceph-crashcollector-devops-control-plane-1-66d7bd67c8zzdhg from rook-ceph started at 2021-02-23 18:35:33 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:08:13.829: INFO: kube-scheduler-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container kube-scheduler ready: true, restart count 10
Mar  5 08:08:13.829: INFO: canal-v67nk from kube-system started at 2021-02-23 18:23:29 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:08:13.829: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:08:13.829: INFO: kube-controller-manager-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container kube-controller-manager ready: true, restart count 14
Mar  5 08:08:13.829: INFO: redis-ha-server-2 from kubesphere-system started at 2021-02-24 13:03:31 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container redis ready: true, restart count 1
Mar  5 08:08:13.829: INFO: 	Container sentinel ready: true, restart count 1
Mar  5 08:08:13.829: INFO: fluent-bit-6z2kz from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:08:13.829: INFO: node-exporter-grhwd from kubesphere-monitoring-system started at 2021-03-04 17:34:37 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.829: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:08:13.829: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:08:13.829: INFO: etcd-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.830: INFO: 	Container etcd ready: true, restart count 1
Mar  5 08:08:13.830: INFO: rook-ceph-osd-2-7d8c68d866-pljjp from rook-ceph started at 2021-02-23 18:35:33 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.830: INFO: 	Container osd ready: true, restart count 1
Mar  5 08:08:13.830: INFO: thanos-ruler-k8s-0 from kubesphere-monitoring-system started at 2021-02-24 13:23:29 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.830: INFO: 	Container rules-configmap-reloader ready: true, restart count 1
Mar  5 08:08:13.830: INFO: 	Container thanos-ruler ready: true, restart count 1
Mar  5 08:08:13.830: INFO: 
Logging pods the kubelet thinks is on node devops-control-plane-2 before test
Mar  5 08:08:13.880: INFO: canal-hctnd from kube-system started at 2021-02-23 18:23:44 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:08:13.880: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:08:13.880: INFO: kube-auditing-webhook-deploy-b74bfb885-5cnp5 from kubesphere-logging-system started at 2021-02-24 13:34:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container kube-auditing-webhook ready: true, restart count 1
Mar  5 08:08:13.880: INFO: harbor-da5q24-harbor-clair-5dd889dd4f-4nclv from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container adapter ready: true, restart count 1
Mar  5 08:08:13.880: INFO: 	Container clair ready: true, restart count 5
Mar  5 08:08:13.880: INFO: jaeger-es-index-cleaner-1614815700-7s2kc from istio-system started at 2021-03-03 23:55:05 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container jaeger-es-index-cleaner ready: false, restart count 0
Mar  5 08:08:13.880: INFO: ks-apiserver-5894746f6b-gprr9 from kubesphere-system started at 2021-03-04 18:53:58 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container ks-apiserver ready: true, restart count 0
Mar  5 08:08:13.880: INFO: coredns-6cf46fccfd-vgfsb from kube-system started at 2021-02-23 18:23:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container coredns ready: true, restart count 1
Mar  5 08:08:13.880: INFO: kube-apiserver-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar  5 08:08:13.880: INFO: jaeger-collector-7887b45bf-n8cbz from istio-system started at 2021-03-01 07:45:14 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container jaeger-collector ready: true, restart count 3
Mar  5 08:08:13.880: INFO: elasticsearch-logging-curator-elasticsearch-curator-161473657d4 from kubesphere-logging-system started at 2021-03-03 01:00:02 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Mar  5 08:08:13.880: INFO: thanos-ruler-kubesphere-0 from kubesphere-monitoring-system started at 2021-03-04 07:58:11 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container rules-configmap-reloader ready: true, restart count 1
Mar  5 08:08:13.880: INFO: 	Container thanos-ruler ready: true, restart count 1
Mar  5 08:08:13.880: INFO: redis-ha-server-1 from kubesphere-system started at 2021-02-24 13:03:23 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container redis ready: true, restart count 1
Mar  5 08:08:13.880: INFO: 	Container sentinel ready: true, restart count 1
Mar  5 08:08:13.880: INFO: harbor-da5q24-harbor-nginx-687f459b8d-drvcb from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container nginx ready: true, restart count 2
Mar  5 08:08:13.880: INFO: kube-proxy-vkr26 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:08:13.880: INFO: coredns-6cf46fccfd-lbcwc from kube-system started at 2021-02-23 18:23:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container coredns ready: true, restart count 1
Mar  5 08:08:13.880: INFO: hcloud-cloud-controller-manager-7c5d46cc54-rhlq7 from kube-system started at 2021-02-23 18:23:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container hcloud-cloud-controller-manager ready: true, restart count 2
Mar  5 08:08:13.880: INFO: ks-events-operator-8dbf7fccc-v8zdm from kubesphere-logging-system started at 2021-02-24 13:22:25 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container events-operator ready: true, restart count 1
Mar  5 08:08:13.880: INFO: node-local-dns-4wxxm from kube-system started at 2021-02-23 18:22:39 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:08:13.880: INFO: reviews-v1-549f6b9d47-rttzb from serv-mesh-ex started at 2021-03-01 10:23:35 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:08:13.880: INFO: 	Container reviews ready: true, restart count 1
Mar  5 08:08:13.880: INFO: ks-controller-manager-8556448648-dk5xc from kubesphere-system started at 2021-03-04 18:53:58 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container ks-controller-manager ready: true, restart count 0
Mar  5 08:08:13.880: INFO: rook-ceph-crashcollector-devops-control-plane-2-6686b8d74fqx7lt from rook-ceph started at 2021-02-23 18:34:51 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:08:13.880: INFO: rook-ceph-osd-0-855b585564-645vx from rook-ceph started at 2021-02-23 18:34:51 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container osd ready: true, restart count 1
Mar  5 08:08:13.880: INFO: redis-ha-haproxy-5c6559d588-l22bp from kubesphere-system started at 2021-02-24 13:03:15 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container haproxy ready: true, restart count 2
Mar  5 08:08:13.880: INFO: jaeger-es-index-cleaner-1614729300-p7kmh from istio-system started at 2021-03-02 23:55:03 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container jaeger-es-index-cleaner ready: false, restart count 0
Mar  5 08:08:13.880: INFO: logsidecar-injector-deploy-74c66bfd85-vpkvb from kubesphere-logging-system started at 2021-02-24 13:22:19 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container config-reloader ready: true, restart count 2
Mar  5 08:08:13.880: INFO: 	Container logsidecar-injector ready: true, restart count 2
Mar  5 08:08:13.880: INFO: minio-image-st-668b878c77-4rwrb from pipeline-tools started at 2021-03-03 17:33:04 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container minio ready: true, restart count 1
Mar  5 08:08:13.880: INFO: harbor-da5q24-harbor-trivy-0 from pipeline-tools started at 2021-03-04 10:01:13 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container trivy ready: true, restart count 1
Mar  5 08:08:13.880: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-5bzp2 from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:08:13.880: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:08:13.880: INFO: kube-scheduler-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container kube-scheduler ready: true, restart count 8
Mar  5 08:08:13.880: INFO: etcd-65796969c7-mr9hs from kubesphere-system started at 2021-02-24 13:19:44 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container etcd ready: true, restart count 1
Mar  5 08:08:13.880: INFO: notification-deployment-65547747f7-zxr25 from kubesphere-alerting-system started at 2021-02-24 13:45:42 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container notification ready: true, restart count 1
Mar  5 08:08:13.880: INFO: harbor-da5q24-harbor-portal-84fb8bdb7f-nttwj from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container portal ready: true, restart count 1
Mar  5 08:08:13.880: INFO: node-exporter-cvhfl from kubesphere-monitoring-system started at 2021-03-04 17:34:19 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:08:13.880: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:08:13.880: INFO: etcd-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:23 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container etcd ready: true, restart count 1
Mar  5 08:08:13.880: INFO: csi-rbdplugin-4hzbs from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:08:13.880: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:08:13.880: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:08:13.880: INFO: openldap-1 from kubesphere-system started at 2021-02-24 13:04:13 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container openldap-ha ready: true, restart count 1
Mar  5 08:08:13.880: INFO: ks-installer-74b46bd68d-q6nrh from kubesphere-system started at 2021-03-04 08:15:54 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container installer ready: true, restart count 1
Mar  5 08:08:13.880: INFO: kube-controller-manager-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container kube-controller-manager ready: true, restart count 7
Mar  5 08:08:13.880: INFO: fluent-bit-grhml from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:08:13.880: INFO: ks-events-ruler-698b7899c7-9qnjp from kubesphere-logging-system started at 2021-02-24 13:22:32 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:08:13.880: INFO: 	Container events-ruler ready: true, restart count 1
Mar  5 08:08:13.880: INFO: jaeger-operator-c78679c9f-r9pbh from istio-system started at 2021-03-01 07:44:49 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container jaeger-operator ready: true, restart count 2
Mar  5 08:08:13.880: INFO: harbor-da5q24-harbor-core-67659b6b55-4mcwv from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container core ready: true, restart count 2
Mar  5 08:08:13.880: INFO: harbor-da5q24-harbor-notary-server-7fc788c84b-bdpf9 from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container notary-server ready: true, restart count 3
Mar  5 08:08:13.880: INFO: harbor-da5q24-harbor-chartmuseum-874964bb8-7dqmz from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container chartmuseum ready: true, restart count 1
Mar  5 08:08:13.880: INFO: csi-cephfsplugin-l8vgz from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:08:13.880: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:08:13.880: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:08:13.880: INFO: ks-console-fb7f5895-rxb4f from kubesphere-system started at 2021-02-24 13:04:07 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container ks-console ready: true, restart count 1
Mar  5 08:08:13.880: INFO: harbor-da5q24-harbor-registry-56df8d46bf-2s6ct from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container registry ready: true, restart count 1
Mar  5 08:08:13.880: INFO: 	Container registryctl ready: true, restart count 1
Mar  5 08:08:13.880: INFO: gogs-gogs-test-http from proteus started at 2021-03-03 07:35:46 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container wget ready: false, restart count 0
Mar  5 08:08:13.880: INFO: rook-ceph-osd-prepare-devops-control-plane-2-l69rc from rook-ceph started at 2021-03-04 11:34:45 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.880: INFO: 	Container provision ready: false, restart count 0
Mar  5 08:08:13.880: INFO: 
Logging pods the kubelet thinks is on node devops-control-plane-3 before test
Mar  5 08:08:13.933: INFO: kube-proxy-nrg4j from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:08:13.933: INFO: csi-cephfsplugin-287cb from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:08:13.933: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:08:13.933: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:08:13.933: INFO: ks-console-fb7f5895-5pmpn from kubesphere-system started at 2021-02-24 13:04:07 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container ks-console ready: true, restart count 1
Mar  5 08:08:13.933: INFO: csi-rbdplugin-provisioner-5bc97cdbb9-zjbws from rook-ceph started at 2021-03-04 08:15:54 +0000 UTC (6 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container csi-attacher ready: true, restart count 1
Mar  5 08:08:13.933: INFO: 	Container csi-provisioner ready: true, restart count 1
Mar  5 08:08:13.933: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:08:13.933: INFO: 	Container csi-resizer ready: true, restart count 1
Mar  5 08:08:13.933: INFO: 	Container csi-snapshotter ready: true, restart count 1
Mar  5 08:08:13.933: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:08:13.933: INFO: rook-ceph-osd-prepare-devops-control-plane-3-mnfq2 from rook-ceph started at 2021-03-04 11:34:47 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container provision ready: false, restart count 0
Mar  5 08:08:13.933: INFO: gogs-gogs-test-ssh from proteus started at 2021-03-03 07:35:46 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container wget ready: false, restart count 0
Mar  5 08:08:13.933: INFO: kube-controller-manager-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container kube-controller-manager ready: true, restart count 11
Mar  5 08:08:13.933: INFO: kube-scheduler-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container kube-scheduler ready: true, restart count 13
Mar  5 08:08:13.933: INFO: fluentbit-operator-7c56d66dd-7b4df from kubesphere-logging-system started at 2021-02-24 13:20:01 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container fluentbit-operator ready: true, restart count 2
Mar  5 08:08:13.933: INFO: istio-ingressgateway-76df6567c6-tndjv from istio-system started at 2021-03-01 07:44:17 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:08:13.933: INFO: ks-controller-manager-8556448648-c4spd from kubesphere-system started at 2021-03-04 18:54:03 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container ks-controller-manager ready: true, restart count 0
Mar  5 08:08:13.933: INFO: machine-controller-webhook-747c599b5c-k7xw4 from kube-system started at 2021-02-23 18:23:36 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container machine-controller-webhook ready: true, restart count 1
Mar  5 08:08:13.933: INFO: notification-manager-deployment-7c8df68d94-5dv72 from kubesphere-monitoring-system started at 2021-02-24 13:05:51 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container notification-manager ready: true, restart count 1
Mar  5 08:08:13.933: INFO: jaeger-query-b546558bf-frn98 from istio-system started at 2021-03-01 07:45:14 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container jaeger-agent ready: true, restart count 1
Mar  5 08:08:13.933: INFO: 	Container jaeger-query ready: true, restart count 1
Mar  5 08:08:13.933: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2021-03-04 17:34:29 +0000 UTC (3 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container prometheus ready: true, restart count 1
Mar  5 08:08:13.933: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  5 08:08:13.933: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  5 08:08:13.933: INFO: node-local-dns-twq6t from kube-system started at 2021-02-23 18:22:38 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:08:13.933: INFO: rook-ceph-crashcollector-devops-control-plane-3-587c5dc7b8mt27z from rook-ceph started at 2021-02-23 18:34:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:08:13.933: INFO: redis-ha-server-0 from kubesphere-system started at 2021-02-24 13:03:18 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container redis ready: true, restart count 1
Mar  5 08:08:13.933: INFO: 	Container sentinel ready: true, restart count 1
Mar  5 08:08:13.933: INFO: fluent-bit-s284n from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:08:13.933: INFO: kube-auditing-operator-6ddc8db4b-vfxqc from kubesphere-logging-system started at 2021-02-24 13:34:03 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container kube-auditing-operator ready: true, restart count 1
Mar  5 08:08:13.933: INFO: notification-db-init-job-mqs2r from kubesphere-alerting-system started at 2021-03-03 17:34:05 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container notification-db-init ready: false, restart count 0
Mar  5 08:08:13.933: INFO: notification-db-ctrl-job-7kzd5 from kubesphere-alerting-system started at 2021-03-03 17:34:15 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container notification-db-ctrl ready: false, restart count 0
Mar  5 08:08:13.933: INFO: without-dockerfile-v1-7dc7678956-mh5vw from serv-mesh-ex started at 2021-03-04 08:15:54 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container container-2a87d6 ready: true, restart count 1
Mar  5 08:08:13.933: INFO: etcd-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:36 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container etcd ready: true, restart count 1
Mar  5 08:08:13.933: INFO: redis-ha-haproxy-5c6559d588-qhqhx from kubesphere-system started at 2021-02-24 13:03:15 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container haproxy ready: true, restart count 1
Mar  5 08:08:13.933: INFO: logsidecar-injector-deploy-74c66bfd85-62xbg from kubesphere-logging-system started at 2021-02-24 13:22:19 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:08:13.933: INFO: 	Container logsidecar-injector ready: true, restart count 1
Mar  5 08:08:13.933: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-wp8ts from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:08:13.933: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:08:13.933: INFO: kiali-operator-58b8765b9c-6qh7r from istio-system started at 2021-03-01 07:44:57 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container operator ready: true, restart count 1
Mar  5 08:08:13.933: INFO: kube-apiserver-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar  5 08:08:13.933: INFO: csi-rbdplugin-7zr2t from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:08:13.933: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:08:13.933: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:08:13.933: INFO: node-exporter-6fsk8 from kubesphere-monitoring-system started at 2021-03-04 17:34:28 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:08:13.933: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:08:13.933: INFO: rook-ceph-osd-1-845f4b4486-5phnd from rook-ceph started at 2021-02-23 18:34:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container osd ready: true, restart count 1
Mar  5 08:08:13.933: INFO: openldap-0 from kubesphere-system started at 2021-02-24 13:03:27 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container openldap-ha ready: true, restart count 1
Mar  5 08:08:13.933: INFO: calico-kube-controllers-589975f454-whrcl from kube-system started at 2021-02-23 18:23:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container calico-kube-controllers ready: true, restart count 1
Mar  5 08:08:13.933: INFO: canal-r57rn from kube-system started at 2021-02-23 18:23:39 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:08:13.933: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:08:13.933: INFO: ks-events-ruler-698b7899c7-rjqqv from kubesphere-logging-system started at 2021-02-24 13:22:32 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:08:13.933: INFO: 	Container events-ruler ready: true, restart count 1
Mar  5 08:08:13.933: INFO: machine-controller-74bbdbb8b8-7r4nj from kube-system started at 2021-02-23 18:23:35 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container machine-controller ready: true, restart count 27
Mar  5 08:08:13.933: INFO: minio-7bfdb5968b-cnd55 from kubesphere-system started at 2021-02-24 14:10:33 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container minio ready: true, restart count 1
Mar  5 08:08:13.933: INFO: thanos-ruler-kubesphere-1 from kubesphere-monitoring-system started at 2021-03-04 07:58:11 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container rules-configmap-reloader ready: true, restart count 1
Mar  5 08:08:13.933: INFO: 	Container thanos-ruler ready: true, restart count 1
Mar  5 08:08:13.933: INFO: ks-apiserver-5894746f6b-8p8vg from kubesphere-system started at 2021-03-04 18:54:02 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.933: INFO: 	Container ks-apiserver ready: true, restart count 0
Mar  5 08:08:13.933: INFO: 
Logging pods the kubelet thinks is on node devops-pool1-6c76d44df9-8tgs6 before test
Mar  5 08:08:13.967: INFO: gogs-gogs-7467fdcf8f-mckdc from default started at 2021-03-04 13:06:22 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container gogs ready: true, restart count 0
Mar  5 08:08:13.967: INFO: adopt-release-nckhj from job-7940 started at 2021-03-05 08:08:13 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container c ready: false, restart count 0
Mar  5 08:08:13.967: INFO: csi-cephfsplugin-9k9tq from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container csi-cephfsplugin ready: true, restart count 8
Mar  5 08:08:13.967: INFO: 	Container driver-registrar ready: true, restart count 8
Mar  5 08:08:13.967: INFO: 	Container liveness-prometheus ready: true, restart count 8
Mar  5 08:08:13.967: INFO: elasticsearch-logging-discovery-1 from kubesphere-logging-system started at 2021-03-04 11:51:15 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  5 08:08:13.967: INFO: snapshot-controller-0 from kube-system started at 2021-03-04 11:51:26 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  5 08:08:13.967: INFO: s2ioperator-0 from kubesphere-devops-system started at 2021-03-04 18:50:25 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container manager ready: true, restart count 0
Mar  5 08:08:13.967: INFO: elasticsearch-logging-data-2 from kubesphere-logging-system started at 2021-03-04 11:51:19 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  5 08:08:13.967: INFO: gogs-postgresql-0 from default started at 2021-03-04 13:06:21 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container gogs-postgresql ready: true, restart count 0
Mar  5 08:08:13.967: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-9ffgc from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:08:13.967: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:08:13.967: INFO: csi-rbdplugin-9km6t from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container csi-rbdplugin ready: true, restart count 8
Mar  5 08:08:13.967: INFO: 	Container driver-registrar ready: true, restart count 8
Mar  5 08:08:13.967: INFO: 	Container liveness-prometheus ready: true, restart count 8
Mar  5 08:08:13.967: INFO: alertmanager-main-0 from kubesphere-monitoring-system started at 2021-03-04 11:52:06 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container alertmanager ready: true, restart count 0
Mar  5 08:08:13.967: INFO: 	Container config-reloader ready: true, restart count 0
Mar  5 08:08:13.967: INFO: thanos-ruler-k8s-1 from kubesphere-monitoring-system started at 2021-03-04 11:51:27 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  5 08:08:13.967: INFO: 	Container thanos-ruler ready: true, restart count 0
Mar  5 08:08:13.967: INFO: elasticsearch-logging-curator-elasticsearch-curator-1614905cqqk from kubesphere-logging-system started at 2021-03-05 01:00:08 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Mar  5 08:08:13.967: INFO: sonobuoy from sonobuoy started at 2021-03-05 07:46:37 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  5 08:08:13.967: INFO: adopt-release-g9x79 from job-7940 started at 2021-03-05 08:08:08 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container c ready: true, restart count 0
Mar  5 08:08:13.967: INFO: canal-rm2l9 from kube-system started at 2021-02-23 18:25:59 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container calico-node ready: true, restart count 8
Mar  5 08:08:13.967: INFO: 	Container kube-flannel ready: true, restart count 8
Mar  5 08:08:13.967: INFO: jaeger-es-index-cleaner-1614902100-xcqvw from istio-system started at 2021-03-04 23:55:01 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container jaeger-es-index-cleaner ready: false, restart count 0
Mar  5 08:08:13.967: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2021-03-04 17:35:33 +0000 UTC (3 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container prometheus ready: true, restart count 1
Mar  5 08:08:13.967: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  5 08:08:13.967: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  5 08:08:13.967: INFO: openpitrix-hyperpitrix-deployment-6d48d87c6c-2xhgh from openpitrix-system started at 2021-03-04 18:51:18 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container hyperpitrix ready: true, restart count 0
Mar  5 08:08:13.967: INFO: hyperpitrix-generate-kubeconfig-9f4fb from openpitrix-system started at 2021-03-04 18:51:36 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container hyperpitrix-generate-kubeconfig ready: false, restart count 0
Mar  5 08:08:13.967: INFO: 	Container hyperpitrix-migrate-runtime ready: false, restart count 0
Mar  5 08:08:13.967: INFO: adopt-release-55nl6 from job-7940 started at 2021-03-05 08:08:08 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container c ready: true, restart count 0
Mar  5 08:08:13.967: INFO: node-local-dns-kh572 from kube-system started at 2021-02-23 18:25:59 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container node-cache ready: true, restart count 8
Mar  5 08:08:13.967: INFO: node-exporter-824lt from kubesphere-monitoring-system started at 2021-03-04 17:34:42 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:08:13.967: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:08:13.967: INFO: kube-proxy-jxx7v from kube-system started at 2021-02-23 18:25:59 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container kube-proxy ready: true, restart count 8
Mar  5 08:08:13.967: INFO: ks-sample-dev-5c5d97c6cc-c9b48 from kubesphere-offline-dev started at 2021-03-04 17:48:16 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container js-sample ready: true, restart count 0
Mar  5 08:08:13.967: INFO: fluent-bit-2vc49 from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:13.967: INFO: 	Container fluent-bit ready: true, restart count 8
Mar  5 08:08:13.967: INFO: 
Logging pods the kubelet thinks is on node devops-pool1-6c76d44df9-dwtv7 before test
Mar  5 08:08:14.006: INFO: csi-cephfsplugin-nxcmz from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:08:14.006: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:08:14.006: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:08:14.006: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:08:14.006: INFO: fluent-bit-s42x9 from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.006: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:08:14.006: INFO: hyperpitrix-release-app-job-tqln9 from openpitrix-system started at 2021-03-04 18:51:28 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.006: INFO: 	Container hyperpitrix-release-app-job ready: false, restart count 0
Mar  5 08:08:14.006: INFO: enginsgungor-hello-deployment-latest--6eg-3afd31aa2f90-jobgl2pn from serv-mesh-ex started at 2021-03-01 19:15:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.006: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:08:14.006: INFO: enginsgungor-tutorial-deployment-late-9pz-042c363e1509-job598vw from tutorial-s2i started at 2021-03-02 18:10:13 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.006: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:08:14.006: INFO: elasticsearch-logging-data-1 from kubesphere-logging-system started at 2021-02-24 13:20:42 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.006: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 08:08:14.006: INFO: ks-controller-manager-8556448648-cbt26 from kubesphere-system started at 2021-03-04 18:53:55 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.006: INFO: 	Container ks-controller-manager ready: true, restart count 1
Mar  5 08:08:14.006: INFO: enginsgungor-hello-world-latest-ug1-ejy-f941edb3103d-job-pr5nr from serv-mesh-ex started at 2021-03-01 18:50:41 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.006: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:08:14.006: INFO: elasticsearch-logging-curator-elasticsearch-curator-161481wvfqt from kubesphere-logging-system started at 2021-03-04 01:00:01 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.006: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Mar  5 08:08:14.006: INFO: node-exporter-f6kh5 from kubesphere-monitoring-system started at 2021-03-04 17:34:32 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:14.006: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:08:14.006: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:08:14.006: INFO: sonobuoy-e2e-job-41e333abf2c348c1 from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:14.006: INFO: 	Container e2e ready: true, restart count 0
Mar  5 08:08:14.006: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:08:14.006: INFO: rook-ceph-mgr-a-55fb9d48b6-clq94 from rook-ceph started at 2021-02-23 18:34:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.006: INFO: 	Container mgr ready: true, restart count 2
Mar  5 08:08:14.006: INFO: kube-proxy-2czsl from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.006: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:08:14.006: INFO: csi-cephfsplugin-provisioner-c68f789b8-k89fp from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (6 container statuses recorded)
Mar  5 08:08:14.006: INFO: 	Container csi-attacher ready: true, restart count 14
Mar  5 08:08:14.007: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:08:14.007: INFO: 	Container csi-provisioner ready: true, restart count 14
Mar  5 08:08:14.007: INFO: 	Container csi-resizer ready: true, restart count 12
Mar  5 08:08:14.007: INFO: 	Container csi-snapshotter ready: true, restart count 13
Mar  5 08:08:14.007: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:08:14.007: INFO: ks-apiserver-5894746f6b-2dq28 from kubesphere-system started at 2021-03-04 18:53:54 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container ks-apiserver ready: true, restart count 0
Mar  5 08:08:14.007: INFO: ks-console-fb7f5895-vvxbg from kubesphere-system started at 2021-02-25 07:43:56 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container ks-console ready: true, restart count 1
Mar  5 08:08:14.007: INFO: tutorial-deployment-v1-5549b8b9d8-t8t9z from tutorial-s2i started at 2021-03-02 18:10:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container container-t1u32h ready: true, restart count 1
Mar  5 08:08:14.007: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-f7ttl from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:08:14.007: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:08:14.007: INFO: rook-ceph-mon-a-85cd5968bc-hq6sb from rook-ceph started at 2021-02-23 18:33:25 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container mon ready: true, restart count 1
Mar  5 08:08:14.007: INFO: alertmanager-main-1 from kubesphere-monitoring-system started at 2021-02-24 13:05:28 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container alertmanager ready: true, restart count 1
Mar  5 08:08:14.007: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:08:14.007: INFO: elasticsearch-logging-discovery-2 from kubesphere-logging-system started at 2021-02-24 13:21:07 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 08:08:14.007: INFO: canal-mfdhb from kube-system started at 2021-02-23 18:26:00 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:08:14.007: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:08:14.007: INFO: node-local-dns-8s8tk from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:08:14.007: INFO: rook-ceph-crashcollector-devops-pool1-6c76d44df9-dwtv7-7dfdvvgh from rook-ceph started at 2021-02-23 18:34:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:08:14.007: INFO: rook-ceph-operator-6fb9f456fc-8g9z2 from rook-ceph started at 2021-02-23 18:29:52 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container rook-ceph-operator ready: true, restart count 1
Mar  5 08:08:14.007: INFO: csi-rbdplugin-nhvm9 from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:08:14.007: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:08:14.007: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:08:14.007: INFO: enginsgungor-hello-deployment-latest--0dq-14b4f67d0b7e-jobptskv from serv-mesh-ex started at 2021-03-01 19:28:59 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:08:14.007: INFO: ks-sample-dev-7bb944b987-hnlc4 from kubesphere-sample-dev started at 2021-03-03 11:03:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container js-sample ready: true, restart count 1
Mar  5 08:08:14.007: INFO: notification-manager-operator-6958786cd6-26xxr from kubesphere-monitoring-system started at 2021-03-04 08:15:54 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Mar  5 08:08:14.007: INFO: 	Container notification-manager-operator ready: true, restart count 2
Mar  5 08:08:14.007: INFO: kube-auditing-webhook-deploy-b74bfb885-qf48j from kubesphere-logging-system started at 2021-02-24 13:34:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container kube-auditing-webhook ready: true, restart count 1
Mar  5 08:08:14.007: INFO: kiali-5fc8bfd66b-wjrqr from istio-system started at 2021-03-01 07:45:58 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container kiali ready: true, restart count 1
Mar  5 08:08:14.007: INFO: harbor-da5q24-harbor-redis-0 from pipeline-tools started at 2021-03-04 10:01:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.007: INFO: 	Container redis ready: true, restart count 1
Mar  5 08:08:14.007: INFO: 
Logging pods the kubelet thinks is on node devops-pool1-6c76d44df9-njb8n before test
Mar  5 08:08:14.068: INFO: kube-proxy-bpngj from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.068: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:08:14.068: INFO: default-http-backend-857d7b6856-gt6hh from kubesphere-controls-system started at 2021-02-24 13:03:47 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.068: INFO: 	Container default-http-backend ready: true, restart count 1
Mar  5 08:08:14.068: INFO: node-local-dns-5qx5m from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.068: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:08:14.068: INFO: rook-ceph-crashcollector-devops-pool1-6c76d44df9-njb8n-cfbd6xbn from rook-ceph started at 2021-02-23 18:33:46 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.068: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:08:14.068: INFO: prometheus-operator-84d58bf775-q2drc from kubesphere-monitoring-system started at 2021-02-24 13:05:18 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:14.068: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Mar  5 08:08:14.068: INFO: 	Container prometheus-operator ready: true, restart count 1
Mar  5 08:08:14.068: INFO: notification-manager-deployment-7c8df68d94-7vd2s from kubesphere-monitoring-system started at 2021-02-24 13:05:51 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.068: INFO: 	Container notification-manager ready: true, restart count 1
Mar  5 08:08:14.068: INFO: mysql-7f64d9f584-49fnb from kubesphere-system started at 2021-02-24 13:19:43 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.068: INFO: 	Container mysql ready: true, restart count 1
Mar  5 08:08:14.068: INFO: rook-ceph-tools-64bd84c8b5-hd7mk from rook-ceph started at 2021-02-25 07:43:01 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.068: INFO: 	Container rook-ceph-tools ready: true, restart count 1
Mar  5 08:08:14.068: INFO: csi-cephfsplugin-t7l65 from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:08:14.068: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:08:14.068: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:08:14.069: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:08:14.069: INFO: elasticsearch-logging-data-0 from kubesphere-logging-system started at 2021-02-24 13:19:49 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 08:08:14.069: INFO: canal-tp2fn from kube-system started at 2021-02-23 18:26:00 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:08:14.069: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:08:14.069: INFO: kube-state-metrics-95c974544-drf59 from kubesphere-monitoring-system started at 2021-02-24 13:05:20 +0000 UTC (3 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 1
Mar  5 08:08:14.069: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 1
Mar  5 08:08:14.069: INFO: 	Container kube-state-metrics ready: true, restart count 1
Mar  5 08:08:14.069: INFO: hellojs-v1-5f4d98c7df-snl82 from serv-mesh-ex started at 2021-03-02 06:09:42 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container container-3nysic ready: true, restart count 1
Mar  5 08:08:14.069: INFO: csi-rbdplugin-5n26w from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:08:14.069: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:08:14.069: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:08:14.069: INFO: kubectl-admin-5d98567c78-m72x5 from kubesphere-controls-system started at 2021-02-24 13:06:05 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container kubectl ready: true, restart count 1
Mar  5 08:08:14.069: INFO: enginsgungor-hello-deployment-latest--fsk-bb68efbac665-jobm9zxh from serv-mesh-ex started at 2021-03-02 06:09:25 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:08:14.069: INFO: node-exporter-fhq4b from kubesphere-monitoring-system started at 2021-03-04 17:34:24 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:08:14.069: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:08:14.069: INFO: rook-ceph-mon-c-764d6d7649-wcbn5 from rook-ceph started at 2021-02-23 18:33:46 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container mon ready: true, restart count 1
Mar  5 08:08:14.069: INFO: elasticsearch-logging-discovery-0 from kubesphere-logging-system started at 2021-02-24 13:19:50 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 08:08:14.069: INFO: fluent-bit-cqbf6 from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:08:14.069: INFO: ks-jenkins-5bf5cbf449-tzqjz from kubesphere-devops-system started at 2021-02-28 09:58:29 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container ks-jenkins ready: true, restart count 1
Mar  5 08:08:14.069: INFO: enginsgungor-hello-deployment-latest--cnc-39caa4291750-jobgnx27 from serv-mesh-ex started at 2021-03-01 19:24:09 +0000 UTC (1 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:08:14.069: INFO: productpage-v1-795bd6db76-trctn from serv-mesh-ex started at 2021-03-04 08:15:54 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:08:14.069: INFO: 	Container productpage ready: true, restart count 1
Mar  5 08:08:14.069: INFO: alertmanager-main-2 from kubesphere-monitoring-system started at 2021-02-24 13:05:28 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container alertmanager ready: true, restart count 1
Mar  5 08:08:14.069: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:08:14.069: INFO: ks-events-exporter-5bc4d9f496-zm5tx from kubesphere-logging-system started at 2021-02-24 13:22:33 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:08:14.069: INFO: 	Container events-exporter ready: true, restart count 1
Mar  5 08:08:14.069: INFO: kubesphere-router-serv-mesh-ex-585558c74b-45mf8 from kubesphere-controls-system started at 2021-03-01 08:45:33 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:08:14.069: INFO: 	Container nginx-ingress-controller ready: true, restart count 1
Mar  5 08:08:14.069: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-xprqf from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:08:14.069: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:08:14.069: INFO: csi-rbdplugin-provisioner-5bc97cdbb9-nlb2z from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (6 container statuses recorded)
Mar  5 08:08:14.069: INFO: 	Container csi-attacher ready: true, restart count 21
Mar  5 08:08:14.069: INFO: 	Container csi-provisioner ready: true, restart count 12
Mar  5 08:08:14.069: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:08:14.069: INFO: 	Container csi-resizer ready: true, restart count 11
Mar  5 08:08:14.069: INFO: 	Container csi-snapshotter ready: true, restart count 13
Mar  5 08:08:14.069: INFO: 	Container liveness-prometheus ready: true, restart count 1
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: verifying the node has the label node devops-control-plane-1
STEP: verifying the node has the label node devops-control-plane-2
STEP: verifying the node has the label node devops-control-plane-3
STEP: verifying the node has the label node devops-pool1-6c76d44df9-8tgs6
STEP: verifying the node has the label node devops-pool1-6c76d44df9-dwtv7
STEP: verifying the node has the label node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod gogs-gogs-7467fdcf8f-mckdc requesting resource cpu=0m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod gogs-postgresql-0 requesting resource cpu=250m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod istio-ingressgateway-76df6567c6-tndjv requesting resource cpu=100m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod istiod-1-6-10-7d869d7f58-tg6zc requesting resource cpu=500m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod jaeger-collector-7887b45bf-n8cbz requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod jaeger-operator-c78679c9f-r9pbh requesting resource cpu=100m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod jaeger-query-b546558bf-frn98 requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod kiali-5fc8bfd66b-wjrqr requesting resource cpu=0m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod kiali-operator-58b8765b9c-6qh7r requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod adopt-release-55nl6 requesting resource cpu=0m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod adopt-release-g9x79 requesting resource cpu=0m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod adopt-release-nckhj requesting resource cpu=0m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod calico-kube-controllers-589975f454-whrcl requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod canal-hctnd requesting resource cpu=250m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod canal-mfdhb requesting resource cpu=250m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod canal-r57rn requesting resource cpu=250m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod canal-rm2l9 requesting resource cpu=250m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod canal-tp2fn requesting resource cpu=250m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod canal-v67nk requesting resource cpu=250m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod coredns-6cf46fccfd-lbcwc requesting resource cpu=100m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod coredns-6cf46fccfd-vgfsb requesting resource cpu=100m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod etcd-devops-control-plane-1 requesting resource cpu=0m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod etcd-devops-control-plane-2 requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod etcd-devops-control-plane-3 requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod hcloud-cloud-controller-manager-7c5d46cc54-rhlq7 requesting resource cpu=100m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod kube-apiserver-devops-control-plane-1 requesting resource cpu=250m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod kube-apiserver-devops-control-plane-2 requesting resource cpu=250m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod kube-apiserver-devops-control-plane-3 requesting resource cpu=250m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod kube-controller-manager-devops-control-plane-1 requesting resource cpu=200m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod kube-controller-manager-devops-control-plane-2 requesting resource cpu=200m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod kube-controller-manager-devops-control-plane-3 requesting resource cpu=200m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod kube-proxy-2czsl requesting resource cpu=0m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod kube-proxy-bpngj requesting resource cpu=0m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod kube-proxy-jxx7v requesting resource cpu=0m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod kube-proxy-nrg4j requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod kube-proxy-v54pf requesting resource cpu=0m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod kube-proxy-vkr26 requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod kube-scheduler-devops-control-plane-1 requesting resource cpu=100m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod kube-scheduler-devops-control-plane-2 requesting resource cpu=100m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod kube-scheduler-devops-control-plane-3 requesting resource cpu=100m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod machine-controller-74bbdbb8b8-7r4nj requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod machine-controller-webhook-747c599b5c-k7xw4 requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod node-local-dns-27gsv requesting resource cpu=25m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod node-local-dns-4wxxm requesting resource cpu=25m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod node-local-dns-5qx5m requesting resource cpu=25m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod node-local-dns-8s8tk requesting resource cpu=25m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod node-local-dns-kh572 requesting resource cpu=25m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod node-local-dns-twq6t requesting resource cpu=25m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod snapshot-controller-0 requesting resource cpu=0m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod notification-deployment-65547747f7-zxr25 requesting resource cpu=10m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod default-http-backend-857d7b6856-gt6hh requesting resource cpu=10m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod kubectl-admin-5d98567c78-m72x5 requesting resource cpu=0m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod kubesphere-router-serv-mesh-ex-585558c74b-45mf8 requesting resource cpu=100m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod ks-jenkins-5bf5cbf449-tzqjz requesting resource cpu=100m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod s2ioperator-0 requesting resource cpu=100m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod elasticsearch-logging-data-0 requesting resource cpu=25m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod elasticsearch-logging-data-1 requesting resource cpu=25m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod elasticsearch-logging-data-2 requesting resource cpu=25m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod elasticsearch-logging-discovery-0 requesting resource cpu=25m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod elasticsearch-logging-discovery-1 requesting resource cpu=25m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod elasticsearch-logging-discovery-2 requesting resource cpu=25m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod fluent-bit-2vc49 requesting resource cpu=0m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod fluent-bit-6z2kz requesting resource cpu=0m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod fluent-bit-cqbf6 requesting resource cpu=0m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod fluent-bit-grhml requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod fluent-bit-s284n requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod fluent-bit-s42x9 requesting resource cpu=0m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod fluentbit-operator-7c56d66dd-7b4df requesting resource cpu=100m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod ks-events-exporter-5bc4d9f496-zm5tx requesting resource cpu=120m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod ks-events-operator-8dbf7fccc-v8zdm requesting resource cpu=20m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod ks-events-ruler-698b7899c7-9qnjp requesting resource cpu=150m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod ks-events-ruler-698b7899c7-rjqqv requesting resource cpu=150m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod kube-auditing-operator-6ddc8db4b-vfxqc requesting resource cpu=20m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod kube-auditing-webhook-deploy-b74bfb885-5cnp5 requesting resource cpu=20m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod kube-auditing-webhook-deploy-b74bfb885-qf48j requesting resource cpu=20m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod logsidecar-injector-deploy-74c66bfd85-62xbg requesting resource cpu=20m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod logsidecar-injector-deploy-74c66bfd85-vpkvb requesting resource cpu=20m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod alertmanager-main-0 requesting resource cpu=20m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod alertmanager-main-1 requesting resource cpu=20m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod alertmanager-main-2 requesting resource cpu=20m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod kube-state-metrics-95c974544-drf59 requesting resource cpu=120m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod node-exporter-6fsk8 requesting resource cpu=112m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod node-exporter-824lt requesting resource cpu=112m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod node-exporter-cvhfl requesting resource cpu=112m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod node-exporter-f6kh5 requesting resource cpu=112m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod node-exporter-fhq4b requesting resource cpu=112m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod node-exporter-grhwd requesting resource cpu=112m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod notification-manager-deployment-7c8df68d94-5dv72 requesting resource cpu=5m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod notification-manager-deployment-7c8df68d94-7vd2s requesting resource cpu=5m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod notification-manager-operator-6958786cd6-26xxr requesting resource cpu=10m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod prometheus-k8s-0 requesting resource cpu=200m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod prometheus-k8s-1 requesting resource cpu=200m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod prometheus-operator-84d58bf775-q2drc requesting resource cpu=110m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod thanos-ruler-k8s-0 requesting resource cpu=100m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod thanos-ruler-k8s-1 requesting resource cpu=100m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod thanos-ruler-kubesphere-0 requesting resource cpu=100m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod thanos-ruler-kubesphere-1 requesting resource cpu=100m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod ks-sample-dev-5c5d97c6cc-c9b48 requesting resource cpu=100m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod ks-sample-dev-7bb944b987-hnlc4 requesting resource cpu=100m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod etcd-65796969c7-mr9hs requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod ks-apiserver-5894746f6b-2dq28 requesting resource cpu=20m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod ks-apiserver-5894746f6b-8p8vg requesting resource cpu=20m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod ks-apiserver-5894746f6b-gprr9 requesting resource cpu=20m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod ks-console-fb7f5895-5pmpn requesting resource cpu=20m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod ks-console-fb7f5895-rxb4f requesting resource cpu=20m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod ks-console-fb7f5895-vvxbg requesting resource cpu=20m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod ks-controller-manager-8556448648-c4spd requesting resource cpu=30m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod ks-controller-manager-8556448648-cbt26 requesting resource cpu=30m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod ks-controller-manager-8556448648-dk5xc requesting resource cpu=30m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod ks-installer-74b46bd68d-q6nrh requesting resource cpu=20m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod minio-7bfdb5968b-cnd55 requesting resource cpu=250m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod mysql-7f64d9f584-49fnb requesting resource cpu=0m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod openldap-0 requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod openldap-1 requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod redis-ha-haproxy-5c6559d588-b2n68 requesting resource cpu=0m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod redis-ha-haproxy-5c6559d588-l22bp requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod redis-ha-haproxy-5c6559d588-qhqhx requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod redis-ha-server-0 requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod redis-ha-server-1 requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod redis-ha-server-2 requesting resource cpu=0m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod openpitrix-hyperpitrix-deployment-6d48d87c6c-2xhgh requesting resource cpu=100m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod harbor-da5q24-harbor-chartmuseum-874964bb8-7dqmz requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod harbor-da5q24-harbor-clair-5dd889dd4f-4nclv requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod harbor-da5q24-harbor-core-67659b6b55-4mcwv requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod harbor-da5q24-harbor-database-0 requesting resource cpu=0m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod harbor-da5q24-harbor-jobservice-b55d6b698-8zwxd requesting resource cpu=0m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod harbor-da5q24-harbor-nginx-687f459b8d-drvcb requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod harbor-da5q24-harbor-notary-server-7fc788c84b-bdpf9 requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod harbor-da5q24-harbor-notary-signer-6785bf5b4-6w2dq requesting resource cpu=0m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod harbor-da5q24-harbor-portal-84fb8bdb7f-nttwj requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod harbor-da5q24-harbor-redis-0 requesting resource cpu=0m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod harbor-da5q24-harbor-registry-56df8d46bf-2s6ct requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod harbor-da5q24-harbor-trivy-0 requesting resource cpu=200m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod minio-image-st-668b878c77-4rwrb requesting resource cpu=250m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod csi-cephfsplugin-287cb requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod csi-cephfsplugin-8sbwl requesting resource cpu=0m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod csi-cephfsplugin-9k9tq requesting resource cpu=0m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod csi-cephfsplugin-l8vgz requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod csi-cephfsplugin-nxcmz requesting resource cpu=0m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod csi-cephfsplugin-provisioner-c68f789b8-2rdqm requesting resource cpu=0m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod csi-cephfsplugin-provisioner-c68f789b8-k89fp requesting resource cpu=0m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod csi-cephfsplugin-t7l65 requesting resource cpu=0m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod csi-rbdplugin-4hzbs requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod csi-rbdplugin-5n26w requesting resource cpu=0m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod csi-rbdplugin-7zr2t requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod csi-rbdplugin-9km6t requesting resource cpu=0m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod csi-rbdplugin-l7fcg requesting resource cpu=0m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod csi-rbdplugin-nhvm9 requesting resource cpu=0m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod csi-rbdplugin-provisioner-5bc97cdbb9-nlb2z requesting resource cpu=0m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod csi-rbdplugin-provisioner-5bc97cdbb9-zjbws requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod rook-ceph-crashcollector-devops-control-plane-1-66d7bd67c8zzdhg requesting resource cpu=0m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod rook-ceph-crashcollector-devops-control-plane-2-6686b8d74fqx7lt requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod rook-ceph-crashcollector-devops-control-plane-3-587c5dc7b8mt27z requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod rook-ceph-crashcollector-devops-pool1-6c76d44df9-dwtv7-7dfdvvgh requesting resource cpu=0m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod rook-ceph-crashcollector-devops-pool1-6c76d44df9-njb8n-cfbd6xbn requesting resource cpu=0m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod rook-ceph-mgr-a-55fb9d48b6-clq94 requesting resource cpu=0m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod rook-ceph-mon-a-85cd5968bc-hq6sb requesting resource cpu=0m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod rook-ceph-mon-c-764d6d7649-wcbn5 requesting resource cpu=0m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod rook-ceph-mon-d-c99989cc-lrjlz requesting resource cpu=0m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod rook-ceph-operator-6fb9f456fc-8g9z2 requesting resource cpu=0m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod rook-ceph-osd-0-855b585564-645vx requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod rook-ceph-osd-1-845f4b4486-5phnd requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod rook-ceph-osd-2-7d8c68d866-pljjp requesting resource cpu=0m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod rook-ceph-tools-64bd84c8b5-hd7mk requesting resource cpu=0m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod details-v1-dc74fc894-m9kzk requesting resource cpu=110m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod hellojs-v1-5f4d98c7df-snl82 requesting resource cpu=0m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod productpage-v1-795bd6db76-trctn requesting resource cpu=110m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod ratings-v1-675894856b-tjjdl requesting resource cpu=110m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod reviews-v1-549f6b9d47-rttzb requesting resource cpu=110m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod without-dockerfile-v1-7dc7678956-mh5vw requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod sonobuoy requesting resource cpu=0m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod sonobuoy-e2e-job-41e333abf2c348c1 requesting resource cpu=0m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-4n6zm requesting resource cpu=0m on Node devops-control-plane-1
Mar  5 08:08:14.452: INFO: Pod sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-5bzp2 requesting resource cpu=0m on Node devops-control-plane-2
Mar  5 08:08:14.452: INFO: Pod sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-9ffgc requesting resource cpu=0m on Node devops-pool1-6c76d44df9-8tgs6
Mar  5 08:08:14.452: INFO: Pod sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-f7ttl requesting resource cpu=0m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.452: INFO: Pod sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-wp8ts requesting resource cpu=0m on Node devops-control-plane-3
Mar  5 08:08:14.452: INFO: Pod sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-xprqf requesting resource cpu=0m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.452: INFO: Pod tutorial-deployment-v1-5549b8b9d8-t8t9z requesting resource cpu=0m on Node devops-pool1-6c76d44df9-dwtv7
STEP: Starting Pods to consume most of the cluster CPU.
Mar  5 08:08:14.452: INFO: Creating a pod which consumes cpu=2060m on Node devops-pool1-6c76d44df9-dwtv7
Mar  5 08:08:14.474: INFO: Creating a pod which consumes cpu=1727m on Node devops-pool1-6c76d44df9-njb8n
Mar  5 08:08:14.492: INFO: Creating a pod which consumes cpu=1570m on Node devops-control-plane-1
Mar  5 08:08:14.510: INFO: Creating a pod which consumes cpu=1185m on Node devops-control-plane-2
Mar  5 08:08:14.540: INFO: Creating a pod which consumes cpu=1433m on Node devops-control-plane-3
Mar  5 08:08:14.572: INFO: Creating a pod which consumes cpu=1605m on Node devops-pool1-6c76d44df9-8tgs6
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1fb09ff7-4b4d-4ff2-8e97-a63b07cb1987.166963d1e994d93d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5927/filler-pod-1fb09ff7-4b4d-4ff2-8e97-a63b07cb1987 to devops-control-plane-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1fb09ff7-4b4d-4ff2-8e97-a63b07cb1987.166963d22c12bcf8], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1fb09ff7-4b4d-4ff2-8e97-a63b07cb1987.166963d22fdab07d], Reason = [Created], Message = [Created container filler-pod-1fb09ff7-4b4d-4ff2-8e97-a63b07cb1987]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1fb09ff7-4b4d-4ff2-8e97-a63b07cb1987.166963d238bd4459], Reason = [Started], Message = [Started container filler-pod-1fb09ff7-4b4d-4ff2-8e97-a63b07cb1987]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8019adee-c18a-4d58-a94d-730d7c093d19.166963d1e988252a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5927/filler-pod-8019adee-c18a-4d58-a94d-730d7c093d19 to devops-control-plane-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8019adee-c18a-4d58-a94d-730d7c093d19.166963d2354062d9], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8019adee-c18a-4d58-a94d-730d7c093d19.166963d239bd2f94], Reason = [Created], Message = [Created container filler-pod-8019adee-c18a-4d58-a94d-730d7c093d19]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8019adee-c18a-4d58-a94d-730d7c093d19.166963d24516cd1f], Reason = [Started], Message = [Started container filler-pod-8019adee-c18a-4d58-a94d-730d7c093d19]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a6a7af23-e9ed-4509-91d4-73533a7eb7cb.166963d1e3458638], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5927/filler-pod-a6a7af23-e9ed-4509-91d4-73533a7eb7cb to devops-pool1-6c76d44df9-dwtv7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a6a7af23-e9ed-4509-91d4-73533a7eb7cb.166963d22585a569], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a6a7af23-e9ed-4509-91d4-73533a7eb7cb.166963d22a0364df], Reason = [Created], Message = [Created container filler-pod-a6a7af23-e9ed-4509-91d4-73533a7eb7cb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a6a7af23-e9ed-4509-91d4-73533a7eb7cb.166963d233e7b8b4], Reason = [Started], Message = [Started container filler-pod-a6a7af23-e9ed-4509-91d4-73533a7eb7cb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a9d5bf7b-1226-4848-9843-79f08127005c.166963d1e98fad66], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5927/filler-pod-a9d5bf7b-1226-4848-9843-79f08127005c to devops-control-plane-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a9d5bf7b-1226-4848-9843-79f08127005c.166963d22888a48c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a9d5bf7b-1226-4848-9843-79f08127005c.166963d22dbfe55b], Reason = [Created], Message = [Created container filler-pod-a9d5bf7b-1226-4848-9843-79f08127005c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a9d5bf7b-1226-4848-9843-79f08127005c.166963d2373ee33f], Reason = [Started], Message = [Started container filler-pod-a9d5bf7b-1226-4848-9843-79f08127005c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-abf81e6f-2a7f-4c05-a961-fab4f63e14cd.166963d1eab9cf90], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5927/filler-pod-abf81e6f-2a7f-4c05-a961-fab4f63e14cd to devops-pool1-6c76d44df9-8tgs6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-abf81e6f-2a7f-4c05-a961-fab4f63e14cd.166963d221141e51], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-abf81e6f-2a7f-4c05-a961-fab4f63e14cd.166963d2246c0efa], Reason = [Created], Message = [Created container filler-pod-abf81e6f-2a7f-4c05-a961-fab4f63e14cd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-abf81e6f-2a7f-4c05-a961-fab4f63e14cd.166963d22c570859], Reason = [Started], Message = [Started container filler-pod-abf81e6f-2a7f-4c05-a961-fab4f63e14cd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c090e560-5f46-4d3d-b194-dc0b4945b02f.166963d1e90ee976], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5927/filler-pod-c090e560-5f46-4d3d-b194-dc0b4945b02f to devops-pool1-6c76d44df9-njb8n]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c090e560-5f46-4d3d-b194-dc0b4945b02f.166963d22611f01c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c090e560-5f46-4d3d-b194-dc0b4945b02f.166963d229857017], Reason = [Created], Message = [Created container filler-pod-c090e560-5f46-4d3d-b194-dc0b4945b02f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c090e560-5f46-4d3d-b194-dc0b4945b02f.166963d231a37287], Reason = [Started], Message = [Started container filler-pod-c090e560-5f46-4d3d-b194-dc0b4945b02f]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.166963d2dffe1d36], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.166963d2e1beee7a], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 Insufficient cpu.]
STEP: removing the label node off the node devops-pool1-6c76d44df9-njb8n
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node devops-control-plane-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node devops-control-plane-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node devops-control-plane-3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node devops-pool1-6c76d44df9-8tgs6
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node devops-pool1-6c76d44df9-dwtv7
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:08:19.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5927" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

â€¢ [SLOW TEST:6.415 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":277,"completed":77,"skipped":1324,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:08:19.947: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7039
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test env composition
Mar  5 08:08:20.156: INFO: Waiting up to 5m0s for pod "var-expansion-2faca02a-be04-4da8-a5e9-bc60f040e798" in namespace "var-expansion-7039" to be "Succeeded or Failed"
Mar  5 08:08:20.163: INFO: Pod "var-expansion-2faca02a-be04-4da8-a5e9-bc60f040e798": Phase="Pending", Reason="", readiness=false. Elapsed: 6.115063ms
Mar  5 08:08:22.173: INFO: Pod "var-expansion-2faca02a-be04-4da8-a5e9-bc60f040e798": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017017428s
STEP: Saw pod success
Mar  5 08:08:22.174: INFO: Pod "var-expansion-2faca02a-be04-4da8-a5e9-bc60f040e798" satisfied condition "Succeeded or Failed"
Mar  5 08:08:22.183: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod var-expansion-2faca02a-be04-4da8-a5e9-bc60f040e798 container dapi-container: <nil>
STEP: delete the pod
Mar  5 08:08:22.235: INFO: Waiting for pod var-expansion-2faca02a-be04-4da8-a5e9-bc60f040e798 to disappear
Mar  5 08:08:22.241: INFO: Pod var-expansion-2faca02a-be04-4da8-a5e9-bc60f040e798 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:08:22.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7039" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":277,"completed":78,"skipped":1327,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:08:22.276: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-8300
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar  5 08:08:24.548: INFO: &Pod{ObjectMeta:{send-events-3a16f232-57e0-49ff-bccd-a3a9b4bc3f41  events-8300 /api/v1/namespaces/events-8300/pods/send-events-3a16f232-57e0-49ff-bccd-a3a9b4bc3f41 f5d20017-35f8-4fdc-8cfa-d75bb30f22ca 5618206 0 2021-03-05 08:08:22 +0000 UTC <nil> <nil> map[name:foo time:491952950] map[cni.projectcalico.org/podIP:10.244.5.206/32 cni.projectcalico.org/podIPs:10.244.5.206/32] [] []  [{e2e.test Update v1 2021-03-05 08:08:22 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 116 105 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 112 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 114 116 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 99 111 110 116 97 105 110 101 114 80 111 114 116 92 34 58 56 48 44 92 34 112 114 111 116 111 99 111 108 92 34 58 92 34 84 67 80 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 99 111 110 116 97 105 110 101 114 80 111 114 116 34 58 123 125 44 34 102 58 112 114 111 116 111 99 111 108 34 58 123 125 125 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2021-03-05 08:08:23 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2021-03-05 08:08:24 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 53 46 50 48 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c2vk4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c2vk4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c2vk4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-dwtv7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 08:08:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 08:08:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 08:08:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 08:08:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.6,PodIP:10.244.5.206,StartTime:2021-03-05 08:08:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-05 08:08:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://495bda921d989e7a91e1ef805d91f608f6fcdd2a1c31ce913d0315e2cd57e12a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar  5 08:08:26.560: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar  5 08:08:28.568: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:08:28.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8300" for this suite.

â€¢ [SLOW TEST:6.360 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":277,"completed":79,"skipped":1368,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:08:28.637: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2630
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Mar  5 08:08:28.814: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:08:33.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2630" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":277,"completed":80,"skipped":1396,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:08:33.324: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8483
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:08:49.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8483" for this suite.

â€¢ [SLOW TEST:16.475 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":277,"completed":81,"skipped":1398,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:08:49.799: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1209
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-1209
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  5 08:08:49.977: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  5 08:08:50.199: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  5 08:08:52.207: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 08:08:54.206: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 08:08:56.205: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 08:08:58.208: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 08:09:00.209: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 08:09:02.206: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 08:09:04.208: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 08:09:06.206: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 08:09:08.207: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  5 08:09:08.221: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  5 08:09:08.237: INFO: The status of Pod netserver-2 is Running (Ready = true)
Mar  5 08:09:08.254: INFO: The status of Pod netserver-3 is Running (Ready = true)
Mar  5 08:09:08.266: INFO: The status of Pod netserver-4 is Running (Ready = true)
Mar  5 08:09:08.287: INFO: The status of Pod netserver-5 is Running (Ready = true)
STEP: Creating test pods
Mar  5 08:09:12.338: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.97:8080/dial?request=hostname&protocol=http&host=10.244.0.236&port=8080&tries=1'] Namespace:pod-network-test-1209 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:09:12.339: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:09:12.583: INFO: Waiting for responses: map[]
Mar  5 08:09:12.590: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.97:8080/dial?request=hostname&protocol=http&host=10.244.1.76&port=8080&tries=1'] Namespace:pod-network-test-1209 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:09:12.590: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:09:12.766: INFO: Waiting for responses: map[]
Mar  5 08:09:12.774: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.97:8080/dial?request=hostname&protocol=http&host=10.244.2.11&port=8080&tries=1'] Namespace:pod-network-test-1209 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:09:12.774: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:09:12.992: INFO: Waiting for responses: map[]
Mar  5 08:09:13.001: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.97:8080/dial?request=hostname&protocol=http&host=10.244.3.96&port=8080&tries=1'] Namespace:pod-network-test-1209 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:09:13.001: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:09:13.202: INFO: Waiting for responses: map[]
Mar  5 08:09:13.208: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.97:8080/dial?request=hostname&protocol=http&host=10.244.5.207&port=8080&tries=1'] Namespace:pod-network-test-1209 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:09:13.208: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:09:13.420: INFO: Waiting for responses: map[]
Mar  5 08:09:13.428: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.97:8080/dial?request=hostname&protocol=http&host=10.244.4.176&port=8080&tries=1'] Namespace:pod-network-test-1209 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:09:13.428: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:09:13.659: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:09:13.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1209" for this suite.

â€¢ [SLOW TEST:23.900 seconds]
[sig-network] Networking
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":277,"completed":82,"skipped":1402,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:09:13.701: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4899
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service multi-endpoint-test in namespace services-4899
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4899 to expose endpoints map[]
Mar  5 08:09:13.946: INFO: successfully validated that service multi-endpoint-test in namespace services-4899 exposes endpoints map[] (10.99819ms elapsed)
STEP: Creating pod pod1 in namespace services-4899
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4899 to expose endpoints map[pod1:[100]]
Mar  5 08:09:16.008: INFO: successfully validated that service multi-endpoint-test in namespace services-4899 exposes endpoints map[pod1:[100]] (2.045025006s elapsed)
STEP: Creating pod pod2 in namespace services-4899
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4899 to expose endpoints map[pod1:[100] pod2:[101]]
Mar  5 08:09:18.100: INFO: successfully validated that service multi-endpoint-test in namespace services-4899 exposes endpoints map[pod1:[100] pod2:[101]] (2.071141567s elapsed)
STEP: Deleting pod pod1 in namespace services-4899
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4899 to expose endpoints map[pod2:[101]]
Mar  5 08:09:19.147: INFO: successfully validated that service multi-endpoint-test in namespace services-4899 exposes endpoints map[pod2:[101]] (1.028706922s elapsed)
STEP: Deleting pod pod2 in namespace services-4899
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4899 to expose endpoints map[]
Mar  5 08:09:20.175: INFO: successfully validated that service multi-endpoint-test in namespace services-4899 exposes endpoints map[] (1.017401551s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:09:20.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4899" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:6.578 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":277,"completed":83,"skipped":1409,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:09:20.282: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1326
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-88954b13-715a-4db5-95b9-347a226286da
STEP: Creating a pod to test consume configMaps
Mar  5 08:09:20.517: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-809e3b7b-cc56-4948-8160-06e963db9b4b" in namespace "projected-1326" to be "Succeeded or Failed"
Mar  5 08:09:20.528: INFO: Pod "pod-projected-configmaps-809e3b7b-cc56-4948-8160-06e963db9b4b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.563244ms
Mar  5 08:09:22.545: INFO: Pod "pod-projected-configmaps-809e3b7b-cc56-4948-8160-06e963db9b4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0285532s
STEP: Saw pod success
Mar  5 08:09:22.545: INFO: Pod "pod-projected-configmaps-809e3b7b-cc56-4948-8160-06e963db9b4b" satisfied condition "Succeeded or Failed"
Mar  5 08:09:22.552: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-projected-configmaps-809e3b7b-cc56-4948-8160-06e963db9b4b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 08:09:22.596: INFO: Waiting for pod pod-projected-configmaps-809e3b7b-cc56-4948-8160-06e963db9b4b to disappear
Mar  5 08:09:22.601: INFO: Pod pod-projected-configmaps-809e3b7b-cc56-4948-8160-06e963db9b4b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:09:22.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1326" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":84,"skipped":1426,"failed":0}

------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:09:22.621: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-9fdbcca6-75ee-4c56-87ae-9b93425205de in namespace container-probe-2
Mar  5 08:09:24.896: INFO: Started pod liveness-9fdbcca6-75ee-4c56-87ae-9b93425205de in namespace container-probe-2
STEP: checking the pod's current state and verifying that restartCount is present
Mar  5 08:09:24.903: INFO: Initial restart count of pod liveness-9fdbcca6-75ee-4c56-87ae-9b93425205de is 0
Mar  5 08:09:42.982: INFO: Restart count of pod container-probe-2/liveness-9fdbcca6-75ee-4c56-87ae-9b93425205de is now 1 (18.079653835s elapsed)
Mar  5 08:10:03.080: INFO: Restart count of pod container-probe-2/liveness-9fdbcca6-75ee-4c56-87ae-9b93425205de is now 2 (38.177303082s elapsed)
Mar  5 08:10:23.171: INFO: Restart count of pod container-probe-2/liveness-9fdbcca6-75ee-4c56-87ae-9b93425205de is now 3 (58.268080913s elapsed)
Mar  5 08:10:43.248: INFO: Restart count of pod container-probe-2/liveness-9fdbcca6-75ee-4c56-87ae-9b93425205de is now 4 (1m18.345264765s elapsed)
Mar  5 08:11:57.611: INFO: Restart count of pod container-probe-2/liveness-9fdbcca6-75ee-4c56-87ae-9b93425205de is now 5 (2m32.708284606s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:11:57.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2" for this suite.

â€¢ [SLOW TEST:155.045 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":277,"completed":85,"skipped":1426,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:11:57.675: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4003
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating replication controller my-hostname-basic-b5d87855-1c9e-4d51-950f-3fe27c5b80b5
Mar  5 08:11:57.884: INFO: Pod name my-hostname-basic-b5d87855-1c9e-4d51-950f-3fe27c5b80b5: Found 0 pods out of 1
Mar  5 08:12:02.893: INFO: Pod name my-hostname-basic-b5d87855-1c9e-4d51-950f-3fe27c5b80b5: Found 1 pods out of 1
Mar  5 08:12:02.893: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b5d87855-1c9e-4d51-950f-3fe27c5b80b5" are running
Mar  5 08:12:02.900: INFO: Pod "my-hostname-basic-b5d87855-1c9e-4d51-950f-3fe27c5b80b5-44cxc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-05 08:11:57 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-05 08:11:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-05 08:11:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-05 08:11:57 +0000 UTC Reason: Message:}])
Mar  5 08:12:02.900: INFO: Trying to dial the pod
Mar  5 08:12:07.929: INFO: Controller my-hostname-basic-b5d87855-1c9e-4d51-950f-3fe27c5b80b5: Got expected result from replica 1 [my-hostname-basic-b5d87855-1c9e-4d51-950f-3fe27c5b80b5-44cxc]: "my-hostname-basic-b5d87855-1c9e-4d51-950f-3fe27c5b80b5-44cxc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:12:07.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4003" for this suite.

â€¢ [SLOW TEST:10.290 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":277,"completed":86,"skipped":1458,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:12:07.966: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3039
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar  5 08:12:08.200: INFO: Created pod &Pod{ObjectMeta:{dns-3039  dns-3039 /api/v1/namespaces/dns-3039/pods/dns-3039 927e1be9-3af1-4e41-8d89-635eb2b522ef 5620269 0 2021-03-05 08:12:08 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-03-05 08:12:08 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 67 111 110 102 105 103 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 115 101 114 118 101 114 115 34 58 123 125 44 34 102 58 115 101 97 114 99 104 101 115 34 58 123 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nr5cr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nr5cr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nr5cr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  5 08:12:08.208: INFO: The status of Pod dns-3039 is Pending, waiting for it to be Running (with Ready = true)
Mar  5 08:12:10.216: INFO: The status of Pod dns-3039 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Mar  5 08:12:10.217: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3039 PodName:dns-3039 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:12:10.217: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Verifying customized DNS server is configured on pod...
Mar  5 08:12:10.459: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3039 PodName:dns-3039 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:12:10.459: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:12:10.690: INFO: Deleting pod dns-3039...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:12:10.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3039" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":277,"completed":87,"skipped":1470,"failed":0}
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:12:10.744: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7287
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-l2fh8 in namespace proxy-7287
I0305 08:12:10.987598      26 runners.go:190] Created replication controller with name: proxy-service-l2fh8, namespace: proxy-7287, replica count: 1
I0305 08:12:12.038434      26 runners.go:190] proxy-service-l2fh8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0305 08:12:13.038810      26 runners.go:190] proxy-service-l2fh8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0305 08:12:14.039206      26 runners.go:190] proxy-service-l2fh8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 08:12:15.040358      26 runners.go:190] proxy-service-l2fh8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 08:12:16.040799      26 runners.go:190] proxy-service-l2fh8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 08:12:17.041154      26 runners.go:190] proxy-service-l2fh8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 08:12:18.041490      26 runners.go:190] proxy-service-l2fh8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 08:12:19.041893      26 runners.go:190] proxy-service-l2fh8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0305 08:12:20.042212      26 runners.go:190] proxy-service-l2fh8 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  5 08:12:20.050: INFO: setup took 9.109920612s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar  5 08:12:20.063: INFO: (0) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 12.247613ms)
Mar  5 08:12:20.066: INFO: (0) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 15.844725ms)
Mar  5 08:12:20.067: INFO: (0) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 15.67671ms)
Mar  5 08:12:20.067: INFO: (0) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 16.450154ms)
Mar  5 08:12:20.075: INFO: (0) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 24.314648ms)
Mar  5 08:12:20.075: INFO: (0) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 24.499403ms)
Mar  5 08:12:20.076: INFO: (0) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 25.59089ms)
Mar  5 08:12:20.077: INFO: (0) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 26.191089ms)
Mar  5 08:12:20.077: INFO: (0) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 26.303967ms)
Mar  5 08:12:20.077: INFO: (0) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 25.997218ms)
Mar  5 08:12:20.077: INFO: (0) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 27.083394ms)
Mar  5 08:12:20.079: INFO: (0) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 28.11898ms)
Mar  5 08:12:20.080: INFO: (0) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 29.191404ms)
Mar  5 08:12:20.080: INFO: (0) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 29.039337ms)
Mar  5 08:12:20.080: INFO: (0) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 29.496261ms)
Mar  5 08:12:20.081: INFO: (0) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 29.906862ms)
Mar  5 08:12:20.090: INFO: (1) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 8.493366ms)
Mar  5 08:12:20.094: INFO: (1) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 12.549366ms)
Mar  5 08:12:20.094: INFO: (1) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 12.50659ms)
Mar  5 08:12:20.094: INFO: (1) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 12.476391ms)
Mar  5 08:12:20.094: INFO: (1) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 12.658353ms)
Mar  5 08:12:20.094: INFO: (1) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 12.744279ms)
Mar  5 08:12:20.095: INFO: (1) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 13.141475ms)
Mar  5 08:12:20.095: INFO: (1) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 13.40753ms)
Mar  5 08:12:20.095: INFO: (1) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 13.435435ms)
Mar  5 08:12:20.096: INFO: (1) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 14.812085ms)
Mar  5 08:12:20.101: INFO: (1) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 19.66269ms)
Mar  5 08:12:20.102: INFO: (1) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 20.845614ms)
Mar  5 08:12:20.104: INFO: (1) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 22.721149ms)
Mar  5 08:12:20.105: INFO: (1) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 23.433351ms)
Mar  5 08:12:20.105: INFO: (1) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 22.961505ms)
Mar  5 08:12:20.105: INFO: (1) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 23.135207ms)
Mar  5 08:12:20.113: INFO: (2) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 7.61966ms)
Mar  5 08:12:20.115: INFO: (2) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 10.439423ms)
Mar  5 08:12:20.116: INFO: (2) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 10.066959ms)
Mar  5 08:12:20.116: INFO: (2) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 8.646419ms)
Mar  5 08:12:20.116: INFO: (2) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 9.631668ms)
Mar  5 08:12:20.116: INFO: (2) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 9.301723ms)
Mar  5 08:12:20.116: INFO: (2) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 10.803913ms)
Mar  5 08:12:20.117: INFO: (2) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 9.302816ms)
Mar  5 08:12:20.117: INFO: (2) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 9.884171ms)
Mar  5 08:12:20.117: INFO: (2) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 10.687667ms)
Mar  5 08:12:20.124: INFO: (2) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 17.069295ms)
Mar  5 08:12:20.127: INFO: (2) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 19.529955ms)
Mar  5 08:12:20.127: INFO: (2) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 20.798248ms)
Mar  5 08:12:20.127: INFO: (2) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 19.822612ms)
Mar  5 08:12:20.127: INFO: (2) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 20.219151ms)
Mar  5 08:12:20.127: INFO: (2) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 20.036916ms)
Mar  5 08:12:20.140: INFO: (3) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 12.243692ms)
Mar  5 08:12:20.140: INFO: (3) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 12.180859ms)
Mar  5 08:12:20.141: INFO: (3) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 13.546039ms)
Mar  5 08:12:20.141: INFO: (3) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 13.353174ms)
Mar  5 08:12:20.141: INFO: (3) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 13.268239ms)
Mar  5 08:12:20.142: INFO: (3) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 13.0832ms)
Mar  5 08:12:20.142: INFO: (3) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 13.62113ms)
Mar  5 08:12:20.142: INFO: (3) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 13.343448ms)
Mar  5 08:12:20.142: INFO: (3) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 13.976096ms)
Mar  5 08:12:20.143: INFO: (3) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 14.604512ms)
Mar  5 08:12:20.144: INFO: (3) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 15.94492ms)
Mar  5 08:12:20.145: INFO: (3) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 17.334481ms)
Mar  5 08:12:20.146: INFO: (3) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 17.251433ms)
Mar  5 08:12:20.146: INFO: (3) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 18.007819ms)
Mar  5 08:12:20.146: INFO: (3) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 18.350038ms)
Mar  5 08:12:20.146: INFO: (3) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 17.759206ms)
Mar  5 08:12:20.155: INFO: (4) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 8.260804ms)
Mar  5 08:12:20.155: INFO: (4) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 8.451988ms)
Mar  5 08:12:20.158: INFO: (4) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 11.282914ms)
Mar  5 08:12:20.158: INFO: (4) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 11.039869ms)
Mar  5 08:12:20.158: INFO: (4) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 11.440588ms)
Mar  5 08:12:20.158: INFO: (4) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 11.438955ms)
Mar  5 08:12:20.158: INFO: (4) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 11.466453ms)
Mar  5 08:12:20.159: INFO: (4) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 12.727555ms)
Mar  5 08:12:20.160: INFO: (4) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 12.993626ms)
Mar  5 08:12:20.160: INFO: (4) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 13.415942ms)
Mar  5 08:12:20.162: INFO: (4) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 15.096508ms)
Mar  5 08:12:20.162: INFO: (4) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 15.243151ms)
Mar  5 08:12:20.162: INFO: (4) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 14.891574ms)
Mar  5 08:12:20.162: INFO: (4) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 15.076436ms)
Mar  5 08:12:20.162: INFO: (4) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 15.664764ms)
Mar  5 08:12:20.163: INFO: (4) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 16.14915ms)
Mar  5 08:12:20.176: INFO: (5) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 12.80035ms)
Mar  5 08:12:20.176: INFO: (5) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 12.894005ms)
Mar  5 08:12:20.176: INFO: (5) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 13.313248ms)
Mar  5 08:12:20.176: INFO: (5) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 13.128056ms)
Mar  5 08:12:20.176: INFO: (5) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 13.221533ms)
Mar  5 08:12:20.177: INFO: (5) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 13.597777ms)
Mar  5 08:12:20.177: INFO: (5) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 14.242167ms)
Mar  5 08:12:20.177: INFO: (5) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 14.265963ms)
Mar  5 08:12:20.177: INFO: (5) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 13.938434ms)
Mar  5 08:12:20.177: INFO: (5) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 13.925332ms)
Mar  5 08:12:20.177: INFO: (5) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 13.765579ms)
Mar  5 08:12:20.177: INFO: (5) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 13.864293ms)
Mar  5 08:12:20.179: INFO: (5) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 15.358115ms)
Mar  5 08:12:20.179: INFO: (5) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 15.894548ms)
Mar  5 08:12:20.179: INFO: (5) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 16.260154ms)
Mar  5 08:12:20.179: INFO: (5) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 16.356185ms)
Mar  5 08:12:20.188: INFO: (6) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 6.338505ms)
Mar  5 08:12:20.190: INFO: (6) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 9.141117ms)
Mar  5 08:12:20.191: INFO: (6) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 10.206302ms)
Mar  5 08:12:20.191: INFO: (6) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 10.159428ms)
Mar  5 08:12:20.191: INFO: (6) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 9.997121ms)
Mar  5 08:12:20.191: INFO: (6) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 9.812648ms)
Mar  5 08:12:20.191: INFO: (6) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 9.556453ms)
Mar  5 08:12:20.191: INFO: (6) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 10.31341ms)
Mar  5 08:12:20.192: INFO: (6) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 10.427365ms)
Mar  5 08:12:20.193: INFO: (6) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 13.153125ms)
Mar  5 08:12:20.194: INFO: (6) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 12.943819ms)
Mar  5 08:12:20.196: INFO: (6) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 14.698663ms)
Mar  5 08:12:20.196: INFO: (6) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 14.719569ms)
Mar  5 08:12:20.196: INFO: (6) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 16.315684ms)
Mar  5 08:12:20.196: INFO: (6) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 15.033477ms)
Mar  5 08:12:20.196: INFO: (6) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 15.5436ms)
Mar  5 08:12:20.206: INFO: (7) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 8.499924ms)
Mar  5 08:12:20.206: INFO: (7) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 9.076808ms)
Mar  5 08:12:20.207: INFO: (7) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 9.700681ms)
Mar  5 08:12:20.207: INFO: (7) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 9.475707ms)
Mar  5 08:12:20.207: INFO: (7) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 9.333431ms)
Mar  5 08:12:20.207: INFO: (7) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 9.427927ms)
Mar  5 08:12:20.207: INFO: (7) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 9.858128ms)
Mar  5 08:12:20.208: INFO: (7) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 10.868192ms)
Mar  5 08:12:20.208: INFO: (7) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 10.253165ms)
Mar  5 08:12:20.208: INFO: (7) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 10.825826ms)
Mar  5 08:12:20.208: INFO: (7) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 10.695012ms)
Mar  5 08:12:20.208: INFO: (7) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 11.17668ms)
Mar  5 08:12:20.208: INFO: (7) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 10.459173ms)
Mar  5 08:12:20.210: INFO: (7) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 12.037158ms)
Mar  5 08:12:20.210: INFO: (7) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 11.953925ms)
Mar  5 08:12:20.210: INFO: (7) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 12.153658ms)
Mar  5 08:12:20.218: INFO: (8) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 8.239999ms)
Mar  5 08:12:20.227: INFO: (8) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 15.986097ms)
Mar  5 08:12:20.227: INFO: (8) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 16.37483ms)
Mar  5 08:12:20.227: INFO: (8) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 15.627487ms)
Mar  5 08:12:20.227: INFO: (8) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 15.874905ms)
Mar  5 08:12:20.227: INFO: (8) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 16.233797ms)
Mar  5 08:12:20.227: INFO: (8) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 16.561989ms)
Mar  5 08:12:20.228: INFO: (8) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 17.327799ms)
Mar  5 08:12:20.228: INFO: (8) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 16.884192ms)
Mar  5 08:12:20.228: INFO: (8) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 17.12699ms)
Mar  5 08:12:20.242: INFO: (8) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 31.247089ms)
Mar  5 08:12:20.245: INFO: (8) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 34.672222ms)
Mar  5 08:12:20.245: INFO: (8) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 34.621434ms)
Mar  5 08:12:20.245: INFO: (8) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 34.63316ms)
Mar  5 08:12:20.245: INFO: (8) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 34.828998ms)
Mar  5 08:12:20.246: INFO: (8) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 34.337733ms)
Mar  5 08:12:20.262: INFO: (9) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 16.196675ms)
Mar  5 08:12:20.266: INFO: (9) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 20.014285ms)
Mar  5 08:12:20.266: INFO: (9) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 19.737289ms)
Mar  5 08:12:20.266: INFO: (9) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 20.229917ms)
Mar  5 08:12:20.266: INFO: (9) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 20.13054ms)
Mar  5 08:12:20.266: INFO: (9) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 20.089604ms)
Mar  5 08:12:20.266: INFO: (9) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 20.468215ms)
Mar  5 08:12:20.266: INFO: (9) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 20.326783ms)
Mar  5 08:12:20.266: INFO: (9) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 20.248039ms)
Mar  5 08:12:20.266: INFO: (9) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 20.158755ms)
Mar  5 08:12:20.268: INFO: (9) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 21.830435ms)
Mar  5 08:12:20.272: INFO: (9) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 25.736442ms)
Mar  5 08:12:20.272: INFO: (9) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 25.614268ms)
Mar  5 08:12:20.272: INFO: (9) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 25.669908ms)
Mar  5 08:12:20.272: INFO: (9) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 25.720116ms)
Mar  5 08:12:20.272: INFO: (9) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 25.76978ms)
Mar  5 08:12:20.287: INFO: (10) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 15.300108ms)
Mar  5 08:12:20.287: INFO: (10) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 15.604419ms)
Mar  5 08:12:20.287: INFO: (10) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 15.607529ms)
Mar  5 08:12:20.288: INFO: (10) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 16.638814ms)
Mar  5 08:12:20.289: INFO: (10) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 16.545002ms)
Mar  5 08:12:20.289: INFO: (10) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 16.577061ms)
Mar  5 08:12:20.292: INFO: (10) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 20.370415ms)
Mar  5 08:12:20.292: INFO: (10) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 20.449282ms)
Mar  5 08:12:20.293: INFO: (10) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 20.839885ms)
Mar  5 08:12:20.293: INFO: (10) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 21.040993ms)
Mar  5 08:12:20.293: INFO: (10) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 21.177153ms)
Mar  5 08:12:20.297: INFO: (10) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 25.209241ms)
Mar  5 08:12:20.297: INFO: (10) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 25.153959ms)
Mar  5 08:12:20.299: INFO: (10) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 26.683628ms)
Mar  5 08:12:20.299: INFO: (10) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 26.748818ms)
Mar  5 08:12:20.299: INFO: (10) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 27.339795ms)
Mar  5 08:12:20.316: INFO: (11) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 15.815515ms)
Mar  5 08:12:20.316: INFO: (11) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 16.346361ms)
Mar  5 08:12:20.316: INFO: (11) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 16.355638ms)
Mar  5 08:12:20.316: INFO: (11) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 16.841057ms)
Mar  5 08:12:20.316: INFO: (11) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 16.274739ms)
Mar  5 08:12:20.316: INFO: (11) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 16.508355ms)
Mar  5 08:12:20.316: INFO: (11) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 16.562507ms)
Mar  5 08:12:20.316: INFO: (11) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 16.422158ms)
Mar  5 08:12:20.316: INFO: (11) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 16.699766ms)
Mar  5 08:12:20.316: INFO: (11) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 16.50644ms)
Mar  5 08:12:20.316: INFO: (11) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 16.930506ms)
Mar  5 08:12:20.319: INFO: (11) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 19.157092ms)
Mar  5 08:12:20.319: INFO: (11) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 19.681393ms)
Mar  5 08:12:20.320: INFO: (11) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 19.778531ms)
Mar  5 08:12:20.320: INFO: (11) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 19.52218ms)
Mar  5 08:12:20.320: INFO: (11) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 19.400931ms)
Mar  5 08:12:20.334: INFO: (12) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 14.082094ms)
Mar  5 08:12:20.334: INFO: (12) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 13.911332ms)
Mar  5 08:12:20.334: INFO: (12) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 14.135558ms)
Mar  5 08:12:20.335: INFO: (12) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 14.430534ms)
Mar  5 08:12:20.334: INFO: (12) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 14.274106ms)
Mar  5 08:12:20.334: INFO: (12) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 14.14363ms)
Mar  5 08:12:20.335: INFO: (12) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 14.822297ms)
Mar  5 08:12:20.335: INFO: (12) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 13.742597ms)
Mar  5 08:12:20.335: INFO: (12) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 14.723491ms)
Mar  5 08:12:20.335: INFO: (12) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 14.347714ms)
Mar  5 08:12:20.334: INFO: (12) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 14.546017ms)
Mar  5 08:12:20.335: INFO: (12) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 14.388237ms)
Mar  5 08:12:20.336: INFO: (12) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 14.646897ms)
Mar  5 08:12:20.336: INFO: (12) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 15.215778ms)
Mar  5 08:12:20.336: INFO: (12) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 15.628226ms)
Mar  5 08:12:20.336: INFO: (12) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 15.238483ms)
Mar  5 08:12:20.344: INFO: (13) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 7.665571ms)
Mar  5 08:12:20.344: INFO: (13) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 7.903857ms)
Mar  5 08:12:20.348: INFO: (13) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 11.395461ms)
Mar  5 08:12:20.348: INFO: (13) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 11.695575ms)
Mar  5 08:12:20.348: INFO: (13) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 12.314142ms)
Mar  5 08:12:20.348: INFO: (13) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 11.988068ms)
Mar  5 08:12:20.348: INFO: (13) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 11.719251ms)
Mar  5 08:12:20.348: INFO: (13) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 11.769207ms)
Mar  5 08:12:20.348: INFO: (13) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 11.619301ms)
Mar  5 08:12:20.348: INFO: (13) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 11.611549ms)
Mar  5 08:12:20.348: INFO: (13) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 11.946864ms)
Mar  5 08:12:20.350: INFO: (13) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 12.865805ms)
Mar  5 08:12:20.350: INFO: (13) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 12.897875ms)
Mar  5 08:12:20.350: INFO: (13) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 13.0643ms)
Mar  5 08:12:20.350: INFO: (13) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 13.085323ms)
Mar  5 08:12:20.351: INFO: (13) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 14.387181ms)
Mar  5 08:12:20.357: INFO: (14) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 5.586992ms)
Mar  5 08:12:20.358: INFO: (14) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 5.663944ms)
Mar  5 08:12:20.359: INFO: (14) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 7.022763ms)
Mar  5 08:12:20.359: INFO: (14) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 6.999287ms)
Mar  5 08:12:20.359: INFO: (14) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 7.462262ms)
Mar  5 08:12:20.361: INFO: (14) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 8.562971ms)
Mar  5 08:12:20.361: INFO: (14) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 8.428049ms)
Mar  5 08:12:20.361: INFO: (14) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 8.791686ms)
Mar  5 08:12:20.361: INFO: (14) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 8.663529ms)
Mar  5 08:12:20.363: INFO: (14) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 10.877702ms)
Mar  5 08:12:20.363: INFO: (14) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 11.504263ms)
Mar  5 08:12:20.365: INFO: (14) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 13.16651ms)
Mar  5 08:12:20.365: INFO: (14) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 13.479492ms)
Mar  5 08:12:20.367: INFO: (14) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 15.635739ms)
Mar  5 08:12:20.367: INFO: (14) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 15.535639ms)
Mar  5 08:12:20.367: INFO: (14) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 15.031635ms)
Mar  5 08:12:20.377: INFO: (15) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 9.55995ms)
Mar  5 08:12:20.378: INFO: (15) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 9.814719ms)
Mar  5 08:12:20.378: INFO: (15) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 10.270482ms)
Mar  5 08:12:20.380: INFO: (15) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 11.85857ms)
Mar  5 08:12:20.380: INFO: (15) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 11.978668ms)
Mar  5 08:12:20.381: INFO: (15) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 12.813414ms)
Mar  5 08:12:20.381: INFO: (15) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 12.608714ms)
Mar  5 08:12:20.381: INFO: (15) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 13.079115ms)
Mar  5 08:12:20.381: INFO: (15) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 12.842496ms)
Mar  5 08:12:20.382: INFO: (15) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 14.196952ms)
Mar  5 08:12:20.383: INFO: (15) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 14.650805ms)
Mar  5 08:12:20.383: INFO: (15) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 14.577588ms)
Mar  5 08:12:20.384: INFO: (15) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 15.979178ms)
Mar  5 08:12:20.385: INFO: (15) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 16.626183ms)
Mar  5 08:12:20.385: INFO: (15) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 17.034791ms)
Mar  5 08:12:20.385: INFO: (15) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 16.613848ms)
Mar  5 08:12:20.399: INFO: (16) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 13.801098ms)
Mar  5 08:12:20.399: INFO: (16) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 13.794083ms)
Mar  5 08:12:20.399: INFO: (16) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 13.20526ms)
Mar  5 08:12:20.399: INFO: (16) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 14.247734ms)
Mar  5 08:12:20.400: INFO: (16) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 13.6596ms)
Mar  5 08:12:20.400: INFO: (16) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 14.722057ms)
Mar  5 08:12:20.400: INFO: (16) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 14.454654ms)
Mar  5 08:12:20.400: INFO: (16) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 13.665542ms)
Mar  5 08:12:20.400: INFO: (16) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 13.878528ms)
Mar  5 08:12:20.400: INFO: (16) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 13.934586ms)
Mar  5 08:12:20.400: INFO: (16) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 14.288104ms)
Mar  5 08:12:20.402: INFO: (16) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 15.429102ms)
Mar  5 08:12:20.402: INFO: (16) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 16.032618ms)
Mar  5 08:12:20.402: INFO: (16) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 15.912659ms)
Mar  5 08:12:20.402: INFO: (16) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 16.884865ms)
Mar  5 08:12:20.403: INFO: (16) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 17.524169ms)
Mar  5 08:12:20.411: INFO: (17) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 7.400939ms)
Mar  5 08:12:20.411: INFO: (17) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 7.563055ms)
Mar  5 08:12:20.411: INFO: (17) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 7.950274ms)
Mar  5 08:12:20.411: INFO: (17) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 7.913318ms)
Mar  5 08:12:20.411: INFO: (17) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 7.565673ms)
Mar  5 08:12:20.412: INFO: (17) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 8.892889ms)
Mar  5 08:12:20.413: INFO: (17) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 9.422272ms)
Mar  5 08:12:20.413: INFO: (17) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 9.343441ms)
Mar  5 08:12:20.413: INFO: (17) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 8.945857ms)
Mar  5 08:12:20.413: INFO: (17) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 9.221945ms)
Mar  5 08:12:20.413: INFO: (17) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 9.307878ms)
Mar  5 08:12:20.413: INFO: (17) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 9.662812ms)
Mar  5 08:12:20.414: INFO: (17) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 10.468096ms)
Mar  5 08:12:20.414: INFO: (17) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 10.81106ms)
Mar  5 08:12:20.415: INFO: (17) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 11.115851ms)
Mar  5 08:12:20.415: INFO: (17) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 11.40146ms)
Mar  5 08:12:20.425: INFO: (18) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 8.977904ms)
Mar  5 08:12:20.426: INFO: (18) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 10.907449ms)
Mar  5 08:12:20.426: INFO: (18) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 9.595782ms)
Mar  5 08:12:20.426: INFO: (18) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 10.113652ms)
Mar  5 08:12:20.426: INFO: (18) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 9.730144ms)
Mar  5 08:12:20.426: INFO: (18) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 9.950893ms)
Mar  5 08:12:20.428: INFO: (18) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 12.092227ms)
Mar  5 08:12:20.430: INFO: (18) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 14.550128ms)
Mar  5 08:12:20.440: INFO: (18) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 24.588542ms)
Mar  5 08:12:20.451: INFO: (18) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 36.05427ms)
Mar  5 08:12:20.451: INFO: (18) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 35.359635ms)
Mar  5 08:12:20.453: INFO: (18) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 37.111461ms)
Mar  5 08:12:20.457: INFO: (18) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 41.925186ms)
Mar  5 08:12:20.457: INFO: (18) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 41.178014ms)
Mar  5 08:12:20.457: INFO: (18) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 41.63155ms)
Mar  5 08:12:20.458: INFO: (18) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 43.021541ms)
Mar  5 08:12:20.468: INFO: (19) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">test<... (200; 9.249382ms)
Mar  5 08:12:20.470: INFO: (19) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 11.17429ms)
Mar  5 08:12:20.473: INFO: (19) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 14.216581ms)
Mar  5 08:12:20.473: INFO: (19) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:443/proxy/tlsrewritem... (200; 14.593753ms)
Mar  5 08:12:20.473: INFO: (19) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4/proxy/rewriteme">test</a> (200; 14.558736ms)
Mar  5 08:12:20.474: INFO: (19) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:162/proxy/: bar (200; 14.478601ms)
Mar  5 08:12:20.475: INFO: (19) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:460/proxy/: tls baz (200; 15.738019ms)
Mar  5 08:12:20.475: INFO: (19) /api/v1/namespaces/proxy-7287/pods/proxy-service-l2fh8-4c4h4:160/proxy/: foo (200; 15.680005ms)
Mar  5 08:12:20.479: INFO: (19) /api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/: <a href="/api/v1/namespaces/proxy-7287/pods/http:proxy-service-l2fh8-4c4h4:1080/proxy/rewriteme">... (200; 20.305645ms)
Mar  5 08:12:20.479: INFO: (19) /api/v1/namespaces/proxy-7287/pods/https:proxy-service-l2fh8-4c4h4:462/proxy/: tls qux (200; 20.191523ms)
Mar  5 08:12:20.480: INFO: (19) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname2/proxy/: bar (200; 20.548462ms)
Mar  5 08:12:20.482: INFO: (19) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname2/proxy/: tls qux (200; 23.135408ms)
Mar  5 08:12:20.484: INFO: (19) /api/v1/namespaces/proxy-7287/services/http:proxy-service-l2fh8:portname1/proxy/: foo (200; 25.151735ms)
Mar  5 08:12:20.484: INFO: (19) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname2/proxy/: bar (200; 25.255177ms)
Mar  5 08:12:20.484: INFO: (19) /api/v1/namespaces/proxy-7287/services/proxy-service-l2fh8:portname1/proxy/: foo (200; 25.174576ms)
Mar  5 08:12:20.487: INFO: (19) /api/v1/namespaces/proxy-7287/services/https:proxy-service-l2fh8:tlsportname1/proxy/: tls baz (200; 27.839174ms)
STEP: deleting ReplicationController proxy-service-l2fh8 in namespace proxy-7287, will wait for the garbage collector to delete the pods
Mar  5 08:12:20.566: INFO: Deleting ReplicationController proxy-service-l2fh8 took: 20.006907ms
Mar  5 08:12:21.666: INFO: Terminating ReplicationController proxy-service-l2fh8 pods took: 1.100501964s
[AfterEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:12:23.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7287" for this suite.

â€¢ [SLOW TEST:13.053 seconds]
[sig-network] Proxy
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":277,"completed":88,"skipped":1476,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:12:23.800: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1249
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 08:12:24.175: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b8ac047-e06d-4a34-bb11-34eec2b7cceb" in namespace "downward-api-1249" to be "Succeeded or Failed"
Mar  5 08:12:24.181: INFO: Pod "downwardapi-volume-4b8ac047-e06d-4a34-bb11-34eec2b7cceb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.265983ms
Mar  5 08:12:26.188: INFO: Pod "downwardapi-volume-4b8ac047-e06d-4a34-bb11-34eec2b7cceb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012995738s
STEP: Saw pod success
Mar  5 08:12:26.188: INFO: Pod "downwardapi-volume-4b8ac047-e06d-4a34-bb11-34eec2b7cceb" satisfied condition "Succeeded or Failed"
Mar  5 08:12:26.196: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-4b8ac047-e06d-4a34-bb11-34eec2b7cceb container client-container: <nil>
STEP: delete the pod
Mar  5 08:12:26.251: INFO: Waiting for pod downwardapi-volume-4b8ac047-e06d-4a34-bb11-34eec2b7cceb to disappear
Mar  5 08:12:26.261: INFO: Pod downwardapi-volume-4b8ac047-e06d-4a34-bb11-34eec2b7cceb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:12:26.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1249" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":89,"skipped":1481,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:12:26.306: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3821
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  5 08:12:26.514: INFO: Waiting up to 5m0s for pod "pod-af086125-26ff-4825-843d-43b9d86645a7" in namespace "emptydir-3821" to be "Succeeded or Failed"
Mar  5 08:12:26.520: INFO: Pod "pod-af086125-26ff-4825-843d-43b9d86645a7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.717001ms
Mar  5 08:12:28.526: INFO: Pod "pod-af086125-26ff-4825-843d-43b9d86645a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01245076s
STEP: Saw pod success
Mar  5 08:12:28.527: INFO: Pod "pod-af086125-26ff-4825-843d-43b9d86645a7" satisfied condition "Succeeded or Failed"
Mar  5 08:12:28.532: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-af086125-26ff-4825-843d-43b9d86645a7 container test-container: <nil>
STEP: delete the pod
Mar  5 08:12:28.576: INFO: Waiting for pod pod-af086125-26ff-4825-843d-43b9d86645a7 to disappear
Mar  5 08:12:28.584: INFO: Pod pod-af086125-26ff-4825-843d-43b9d86645a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:12:28.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3821" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":90,"skipped":1498,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:12:28.617: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2790
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Mar  5 08:12:28.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 create -f - --namespace=kubectl-2790'
Mar  5 08:12:29.461: INFO: stderr: ""
Mar  5 08:12:29.461: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar  5 08:12:30.474: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  5 08:12:30.474: INFO: Found 0 / 1
Mar  5 08:12:31.473: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  5 08:12:31.473: INFO: Found 0 / 1
Mar  5 08:12:32.475: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  5 08:12:32.475: INFO: Found 1 / 1
Mar  5 08:12:32.475: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar  5 08:12:32.481: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  5 08:12:32.481: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  5 08:12:32.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 patch pod agnhost-master-lwhkc --namespace=kubectl-2790 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  5 08:12:32.636: INFO: stderr: ""
Mar  5 08:12:32.636: INFO: stdout: "pod/agnhost-master-lwhkc patched\n"
STEP: checking annotations
Mar  5 08:12:32.642: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  5 08:12:32.643: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:12:32.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2790" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":277,"completed":91,"skipped":1498,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:12:32.665: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5188
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0305 08:12:42.967152      26 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  5 08:12:42.967: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:12:42.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5188" for this suite.

â€¢ [SLOW TEST:10.329 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":277,"completed":92,"skipped":1500,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:12:42.994: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-287
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:12:43.178: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:12:50.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-287" for this suite.

â€¢ [SLOW TEST:7.505 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":277,"completed":93,"skipped":1513,"failed":0}
SS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:12:50.499: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7842
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:12:50.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7842" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":277,"completed":94,"skipped":1515,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:12:50.940: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2874
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 08:12:51.989: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f881f6b-228f-44a2-a014-ba5a31a295eb" in namespace "downward-api-2874" to be "Succeeded or Failed"
Mar  5 08:12:52.006: INFO: Pod "downwardapi-volume-5f881f6b-228f-44a2-a014-ba5a31a295eb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.792369ms
Mar  5 08:12:54.018: INFO: Pod "downwardapi-volume-5f881f6b-228f-44a2-a014-ba5a31a295eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028486177s
STEP: Saw pod success
Mar  5 08:12:54.018: INFO: Pod "downwardapi-volume-5f881f6b-228f-44a2-a014-ba5a31a295eb" satisfied condition "Succeeded or Failed"
Mar  5 08:12:54.025: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-5f881f6b-228f-44a2-a014-ba5a31a295eb container client-container: <nil>
STEP: delete the pod
Mar  5 08:12:54.161: INFO: Waiting for pod downwardapi-volume-5f881f6b-228f-44a2-a014-ba5a31a295eb to disappear
Mar  5 08:12:54.167: INFO: Pod downwardapi-volume-5f881f6b-228f-44a2-a014-ba5a31a295eb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:12:54.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2874" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":95,"skipped":1527,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:12:54.196: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 08:12:54.473: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12517e82-2924-4424-ba99-42e34d1a8499" in namespace "downward-api-6773" to be "Succeeded or Failed"
Mar  5 08:12:54.488: INFO: Pod "downwardapi-volume-12517e82-2924-4424-ba99-42e34d1a8499": Phase="Pending", Reason="", readiness=false. Elapsed: 15.296808ms
Mar  5 08:12:56.496: INFO: Pod "downwardapi-volume-12517e82-2924-4424-ba99-42e34d1a8499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022685269s
STEP: Saw pod success
Mar  5 08:12:56.496: INFO: Pod "downwardapi-volume-12517e82-2924-4424-ba99-42e34d1a8499" satisfied condition "Succeeded or Failed"
Mar  5 08:12:56.503: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-12517e82-2924-4424-ba99-42e34d1a8499 container client-container: <nil>
STEP: delete the pod
Mar  5 08:12:56.568: INFO: Waiting for pod downwardapi-volume-12517e82-2924-4424-ba99-42e34d1a8499 to disappear
Mar  5 08:12:56.591: INFO: Pod downwardapi-volume-12517e82-2924-4424-ba99-42e34d1a8499 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:12:56.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6773" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":277,"completed":96,"skipped":1527,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:12:57.191: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6599
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  5 08:12:57.450: INFO: Waiting up to 5m0s for pod "pod-b26096e7-d1b0-45ff-a30e-51865032ec57" in namespace "emptydir-6599" to be "Succeeded or Failed"
Mar  5 08:12:57.466: INFO: Pod "pod-b26096e7-d1b0-45ff-a30e-51865032ec57": Phase="Pending", Reason="", readiness=false. Elapsed: 15.103528ms
Mar  5 08:12:59.477: INFO: Pod "pod-b26096e7-d1b0-45ff-a30e-51865032ec57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026141614s
STEP: Saw pod success
Mar  5 08:12:59.477: INFO: Pod "pod-b26096e7-d1b0-45ff-a30e-51865032ec57" satisfied condition "Succeeded or Failed"
Mar  5 08:12:59.484: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-b26096e7-d1b0-45ff-a30e-51865032ec57 container test-container: <nil>
STEP: delete the pod
Mar  5 08:12:59.536: INFO: Waiting for pod pod-b26096e7-d1b0-45ff-a30e-51865032ec57 to disappear
Mar  5 08:12:59.544: INFO: Pod pod-b26096e7-d1b0-45ff-a30e-51865032ec57 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:12:59.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6599" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":97,"skipped":1595,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:12:59.573: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1813
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Mar  5 08:13:02.331: INFO: Successfully updated pod "labelsupdatefd5888d6-2b5f-4487-bb7e-c7fa0437eda8"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:13:04.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1813" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":277,"completed":98,"skipped":1600,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:13:04.437: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1881
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-8f178246-8df9-4780-9ea5-d02fc0cee99d
STEP: Creating a pod to test consume secrets
Mar  5 08:13:04.668: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-66c6dbe8-14d3-4790-bff2-88adcf0c93da" in namespace "projected-1881" to be "Succeeded or Failed"
Mar  5 08:13:04.672: INFO: Pod "pod-projected-secrets-66c6dbe8-14d3-4790-bff2-88adcf0c93da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.090594ms
Mar  5 08:13:06.682: INFO: Pod "pod-projected-secrets-66c6dbe8-14d3-4790-bff2-88adcf0c93da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014584294s
Mar  5 08:13:08.690: INFO: Pod "pod-projected-secrets-66c6dbe8-14d3-4790-bff2-88adcf0c93da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022514016s
STEP: Saw pod success
Mar  5 08:13:08.690: INFO: Pod "pod-projected-secrets-66c6dbe8-14d3-4790-bff2-88adcf0c93da" satisfied condition "Succeeded or Failed"
Mar  5 08:13:08.696: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-projected-secrets-66c6dbe8-14d3-4790-bff2-88adcf0c93da container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  5 08:13:08.750: INFO: Waiting for pod pod-projected-secrets-66c6dbe8-14d3-4790-bff2-88adcf0c93da to disappear
Mar  5 08:13:08.755: INFO: Pod pod-projected-secrets-66c6dbe8-14d3-4790-bff2-88adcf0c93da no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:13:08.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1881" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":99,"skipped":1602,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:13:08.778: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-projected-ddvr
STEP: Creating a pod to test atomic-volume-subpath
Mar  5 08:13:08.994: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-ddvr" in namespace "subpath-9736" to be "Succeeded or Failed"
Mar  5 08:13:09.000: INFO: Pod "pod-subpath-test-projected-ddvr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033144ms
Mar  5 08:13:11.009: INFO: Pod "pod-subpath-test-projected-ddvr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015589937s
Mar  5 08:13:13.019: INFO: Pod "pod-subpath-test-projected-ddvr": Phase="Running", Reason="", readiness=true. Elapsed: 4.02488545s
Mar  5 08:13:15.024: INFO: Pod "pod-subpath-test-projected-ddvr": Phase="Running", Reason="", readiness=true. Elapsed: 6.030526939s
Mar  5 08:13:17.031: INFO: Pod "pod-subpath-test-projected-ddvr": Phase="Running", Reason="", readiness=true. Elapsed: 8.037335082s
Mar  5 08:13:19.039: INFO: Pod "pod-subpath-test-projected-ddvr": Phase="Running", Reason="", readiness=true. Elapsed: 10.045189498s
Mar  5 08:13:21.048: INFO: Pod "pod-subpath-test-projected-ddvr": Phase="Running", Reason="", readiness=true. Elapsed: 12.053786511s
Mar  5 08:13:23.055: INFO: Pod "pod-subpath-test-projected-ddvr": Phase="Running", Reason="", readiness=true. Elapsed: 14.061235797s
Mar  5 08:13:25.063: INFO: Pod "pod-subpath-test-projected-ddvr": Phase="Running", Reason="", readiness=true. Elapsed: 16.068983383s
Mar  5 08:13:27.071: INFO: Pod "pod-subpath-test-projected-ddvr": Phase="Running", Reason="", readiness=true. Elapsed: 18.077345766s
Mar  5 08:13:29.080: INFO: Pod "pod-subpath-test-projected-ddvr": Phase="Running", Reason="", readiness=true. Elapsed: 20.085804285s
Mar  5 08:13:31.089: INFO: Pod "pod-subpath-test-projected-ddvr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.095607161s
STEP: Saw pod success
Mar  5 08:13:31.089: INFO: Pod "pod-subpath-test-projected-ddvr" satisfied condition "Succeeded or Failed"
Mar  5 08:13:31.103: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-subpath-test-projected-ddvr container test-container-subpath-projected-ddvr: <nil>
STEP: delete the pod
Mar  5 08:13:31.169: INFO: Waiting for pod pod-subpath-test-projected-ddvr to disappear
Mar  5 08:13:31.176: INFO: Pod pod-subpath-test-projected-ddvr no longer exists
STEP: Deleting pod pod-subpath-test-projected-ddvr
Mar  5 08:13:31.176: INFO: Deleting pod "pod-subpath-test-projected-ddvr" in namespace "subpath-9736"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:13:31.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9736" for this suite.

â€¢ [SLOW TEST:22.430 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":277,"completed":100,"skipped":1605,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:13:31.210: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-3515
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating server pod server in namespace prestop-3515
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3515
STEP: Deleting pre-stop pod
Mar  5 08:13:40.519: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:13:40.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3515" for this suite.

â€¢ [SLOW TEST:9.348 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":277,"completed":101,"skipped":1630,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:13:40.560: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8784
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Mar  5 08:14:20.830: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0305 08:14:20.830759      26 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:14:20.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8784" for this suite.

â€¢ [SLOW TEST:40.317 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":277,"completed":102,"skipped":1664,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:14:20.880: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4606
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-c49a69e8-74b0-4dc0-aaaf-32cc22406f23
STEP: Creating a pod to test consume secrets
Mar  5 08:14:21.330: INFO: Waiting up to 5m0s for pod "pod-secrets-9dbfbf00-2907-4def-975d-5a0371bd455c" in namespace "secrets-4606" to be "Succeeded or Failed"
Mar  5 08:14:21.335: INFO: Pod "pod-secrets-9dbfbf00-2907-4def-975d-5a0371bd455c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.496389ms
Mar  5 08:14:23.343: INFO: Pod "pod-secrets-9dbfbf00-2907-4def-975d-5a0371bd455c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01286605s
STEP: Saw pod success
Mar  5 08:14:23.343: INFO: Pod "pod-secrets-9dbfbf00-2907-4def-975d-5a0371bd455c" satisfied condition "Succeeded or Failed"
Mar  5 08:14:23.348: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-secrets-9dbfbf00-2907-4def-975d-5a0371bd455c container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 08:14:23.388: INFO: Waiting for pod pod-secrets-9dbfbf00-2907-4def-975d-5a0371bd455c to disappear
Mar  5 08:14:23.393: INFO: Pod pod-secrets-9dbfbf00-2907-4def-975d-5a0371bd455c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:14:23.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4606" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":103,"skipped":1688,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:14:23.414: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1169
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Mar  5 08:14:23.593: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:15:04.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1169" for this suite.

â€¢ [SLOW TEST:41.530 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":277,"completed":104,"skipped":1692,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:15:04.946: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8188
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on node default medium
Mar  5 08:15:05.152: INFO: Waiting up to 5m0s for pod "pod-d1bcbe99-3c5a-4773-a686-96ea219fcea7" in namespace "emptydir-8188" to be "Succeeded or Failed"
Mar  5 08:15:05.158: INFO: Pod "pod-d1bcbe99-3c5a-4773-a686-96ea219fcea7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.746184ms
Mar  5 08:15:07.166: INFO: Pod "pod-d1bcbe99-3c5a-4773-a686-96ea219fcea7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013913201s
Mar  5 08:15:09.173: INFO: Pod "pod-d1bcbe99-3c5a-4773-a686-96ea219fcea7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020763699s
STEP: Saw pod success
Mar  5 08:15:09.173: INFO: Pod "pod-d1bcbe99-3c5a-4773-a686-96ea219fcea7" satisfied condition "Succeeded or Failed"
Mar  5 08:15:09.181: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-d1bcbe99-3c5a-4773-a686-96ea219fcea7 container test-container: <nil>
STEP: delete the pod
Mar  5 08:15:09.216: INFO: Waiting for pod pod-d1bcbe99-3c5a-4773-a686-96ea219fcea7 to disappear
Mar  5 08:15:09.221: INFO: Pod pod-d1bcbe99-3c5a-4773-a686-96ea219fcea7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:15:09.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8188" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":105,"skipped":1765,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:15:09.294: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3602
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:15:09.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3602" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
â€¢{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":277,"completed":106,"skipped":1771,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:15:09.515: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8092
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-bb2a284a-5271-4c2c-9bf0-da324a8b20b0
STEP: Creating a pod to test consume secrets
Mar  5 08:15:09.729: INFO: Waiting up to 5m0s for pod "pod-secrets-75fe463c-e993-4c3b-955e-fa3fc342ad86" in namespace "secrets-8092" to be "Succeeded or Failed"
Mar  5 08:15:09.750: INFO: Pod "pod-secrets-75fe463c-e993-4c3b-955e-fa3fc342ad86": Phase="Pending", Reason="", readiness=false. Elapsed: 21.146758ms
Mar  5 08:15:11.757: INFO: Pod "pod-secrets-75fe463c-e993-4c3b-955e-fa3fc342ad86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027591781s
STEP: Saw pod success
Mar  5 08:15:11.757: INFO: Pod "pod-secrets-75fe463c-e993-4c3b-955e-fa3fc342ad86" satisfied condition "Succeeded or Failed"
Mar  5 08:15:11.764: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-secrets-75fe463c-e993-4c3b-955e-fa3fc342ad86 container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 08:15:11.807: INFO: Waiting for pod pod-secrets-75fe463c-e993-4c3b-955e-fa3fc342ad86 to disappear
Mar  5 08:15:11.812: INFO: Pod pod-secrets-75fe463c-e993-4c3b-955e-fa3fc342ad86 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:15:11.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8092" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":107,"skipped":1778,"failed":0}
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:15:11.847: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1649
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3214
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3437
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:15:28.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1649" for this suite.
STEP: Destroying namespace "nsdeletetest-3214" for this suite.
Mar  5 08:15:28.620: INFO: Namespace nsdeletetest-3214 was already deleted
STEP: Destroying namespace "nsdeletetest-3437" for this suite.

â€¢ [SLOW TEST:16.785 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":277,"completed":108,"skipped":1780,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:15:28.634: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7911
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  5 08:15:29.201: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 08:15:32.262: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:15:32.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7911" for this suite.
STEP: Destroying namespace "webhook-7911-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":277,"completed":109,"skipped":1837,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:15:32.527: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  5 08:15:35.364: INFO: Successfully updated pod "pod-update-5f401f90-90b9-4bda-aa79-8307682d51a9"
STEP: verifying the updated pod is in kubernetes
Mar  5 08:15:35.379: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:15:35.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8577" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":277,"completed":110,"skipped":1847,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:15:35.413: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4436
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-b2090a99-d1b1-4808-a77d-66789d97c641
STEP: Creating secret with name s-test-opt-upd-be504181-86a1-4b88-ba23-bb46969e5f38
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-b2090a99-d1b1-4808-a77d-66789d97c641
STEP: Updating secret s-test-opt-upd-be504181-86a1-4b88-ba23-bb46969e5f38
STEP: Creating secret with name s-test-opt-create-2b16df28-7cfa-4d12-80f0-2ca954fbbe5f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:16:56.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4436" for this suite.

â€¢ [SLOW TEST:81.358 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":111,"skipped":1867,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:16:56.778: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5260
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-5260
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Mar  5 08:16:57.006: INFO: Found 0 stateful pods, waiting for 3
Mar  5 08:17:07.016: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 08:17:07.016: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 08:17:07.016: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar  5 08:17:07.062: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar  5 08:17:17.132: INFO: Updating stateful set ss2
Mar  5 08:17:17.155: INFO: Waiting for Pod statefulset-5260/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Mar  5 08:17:27.235: INFO: Found 2 stateful pods, waiting for 3
Mar  5 08:17:37.244: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 08:17:37.244: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 08:17:37.244: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar  5 08:17:37.285: INFO: Updating stateful set ss2
Mar  5 08:17:37.297: INFO: Waiting for Pod statefulset-5260/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  5 08:17:47.311: INFO: Waiting for Pod statefulset-5260/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  5 08:17:57.354: INFO: Updating stateful set ss2
Mar  5 08:17:57.369: INFO: Waiting for StatefulSet statefulset-5260/ss2 to complete update
Mar  5 08:17:57.370: INFO: Waiting for Pod statefulset-5260/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar  5 08:18:07.386: INFO: Deleting all statefulset in ns statefulset-5260
Mar  5 08:18:07.397: INFO: Scaling statefulset ss2 to 0
Mar  5 08:18:27.445: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 08:18:27.451: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:18:27.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5260" for this suite.

â€¢ [SLOW TEST:90.750 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":277,"completed":112,"skipped":1886,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:18:27.543: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1035
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-6e540a1a-6d7c-49c8-ac20-df1ba45f48db
STEP: Creating a pod to test consume secrets
Mar  5 08:18:27.771: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9e06e571-1eb3-4f3f-b8af-9a2585575f58" in namespace "projected-1035" to be "Succeeded or Failed"
Mar  5 08:18:27.777: INFO: Pod "pod-projected-secrets-9e06e571-1eb3-4f3f-b8af-9a2585575f58": Phase="Pending", Reason="", readiness=false. Elapsed: 5.684179ms
Mar  5 08:18:29.785: INFO: Pod "pod-projected-secrets-9e06e571-1eb3-4f3f-b8af-9a2585575f58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013676609s
STEP: Saw pod success
Mar  5 08:18:29.785: INFO: Pod "pod-projected-secrets-9e06e571-1eb3-4f3f-b8af-9a2585575f58" satisfied condition "Succeeded or Failed"
Mar  5 08:18:29.795: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-projected-secrets-9e06e571-1eb3-4f3f-b8af-9a2585575f58 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  5 08:18:29.864: INFO: Waiting for pod pod-projected-secrets-9e06e571-1eb3-4f3f-b8af-9a2585575f58 to disappear
Mar  5 08:18:29.871: INFO: Pod pod-projected-secrets-9e06e571-1eb3-4f3f-b8af-9a2585575f58 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:18:29.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1035" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":113,"skipped":1946,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:18:29.898: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6409
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  5 08:18:30.525: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  5 08:18:32.551: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529110, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529110, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529110, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529110, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 08:18:35.596: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:18:35.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6409" for this suite.
STEP: Destroying namespace "webhook-6409-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.225 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":277,"completed":114,"skipped":1971,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:18:36.125: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4407
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  5 08:18:36.368: INFO: Waiting up to 5m0s for pod "pod-dfb53806-3b0f-4c1c-85fa-0856b2734cd5" in namespace "emptydir-4407" to be "Succeeded or Failed"
Mar  5 08:18:36.374: INFO: Pod "pod-dfb53806-3b0f-4c1c-85fa-0856b2734cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.079224ms
Mar  5 08:18:38.385: INFO: Pod "pod-dfb53806-3b0f-4c1c-85fa-0856b2734cd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015585926s
STEP: Saw pod success
Mar  5 08:18:38.385: INFO: Pod "pod-dfb53806-3b0f-4c1c-85fa-0856b2734cd5" satisfied condition "Succeeded or Failed"
Mar  5 08:18:38.391: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-dfb53806-3b0f-4c1c-85fa-0856b2734cd5 container test-container: <nil>
STEP: delete the pod
Mar  5 08:18:38.460: INFO: Waiting for pod pod-dfb53806-3b0f-4c1c-85fa-0856b2734cd5 to disappear
Mar  5 08:18:38.464: INFO: Pod pod-dfb53806-3b0f-4c1c-85fa-0856b2734cd5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:18:38.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4407" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":115,"skipped":1973,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:18:38.483: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-391
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar  5 08:18:43.800: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:18:44.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-391" for this suite.

â€¢ [SLOW TEST:6.386 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":277,"completed":116,"skipped":1991,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:18:44.872: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1053
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Mar  5 08:18:49.665: INFO: Successfully updated pod "annotationupdatea9d22773-32dc-490e-b913-795a7f21acd1"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:18:51.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1053" for this suite.

â€¢ [SLOW TEST:6.852 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":277,"completed":117,"skipped":2041,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:18:51.726: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-4161
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Mar  5 08:18:51.967: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  5 08:19:52.222: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:19:52.234: INFO: Starting informer...
STEP: Starting pod...
Mar  5 08:19:52.468: INFO: Pod is running on devops-pool1-6c76d44df9-8tgs6. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Mar  5 08:19:52.532: INFO: Pod wasn't evicted. Proceeding
Mar  5 08:19:52.532: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Mar  5 08:21:07.624: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:21:07.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4161" for this suite.

â€¢ [SLOW TEST:135.936 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":277,"completed":118,"skipped":2063,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:21:07.677: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6276
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:21:14.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6276" for this suite.

â€¢ [SLOW TEST:7.318 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":277,"completed":119,"skipped":2073,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:21:15.006: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-9997
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Mar  5 08:21:15.240: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Mar  5 08:21:15.253: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  5 08:21:15.253: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Mar  5 08:21:15.275: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  5 08:21:15.275: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Mar  5 08:21:15.295: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar  5 08:21:15.296: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Mar  5 08:21:22.622: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:21:22.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-9997" for this suite.

â€¢ [SLOW TEST:7.696 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":277,"completed":120,"skipped":2092,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:21:22.706: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9904
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-92a6a425-7cab-414b-b015-06c6d386f2a9
STEP: Creating a pod to test consume configMaps
Mar  5 08:21:22.995: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5c4af19d-0d49-4219-91ca-0becc180b679" in namespace "projected-9904" to be "Succeeded or Failed"
Mar  5 08:21:23.012: INFO: Pod "pod-projected-configmaps-5c4af19d-0d49-4219-91ca-0becc180b679": Phase="Pending", Reason="", readiness=false. Elapsed: 17.210266ms
Mar  5 08:21:25.022: INFO: Pod "pod-projected-configmaps-5c4af19d-0d49-4219-91ca-0becc180b679": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026986183s
Mar  5 08:21:27.030: INFO: Pod "pod-projected-configmaps-5c4af19d-0d49-4219-91ca-0becc180b679": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035373583s
STEP: Saw pod success
Mar  5 08:21:27.030: INFO: Pod "pod-projected-configmaps-5c4af19d-0d49-4219-91ca-0becc180b679" satisfied condition "Succeeded or Failed"
Mar  5 08:21:27.036: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-projected-configmaps-5c4af19d-0d49-4219-91ca-0becc180b679 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 08:21:27.098: INFO: Waiting for pod pod-projected-configmaps-5c4af19d-0d49-4219-91ca-0becc180b679 to disappear
Mar  5 08:21:27.115: INFO: Pod pod-projected-configmaps-5c4af19d-0d49-4219-91ca-0becc180b679 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:21:27.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9904" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":121,"skipped":2132,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:21:27.184: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1280
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-7ea1e3f9-2140-4a16-9849-610dd201af55
STEP: Creating a pod to test consume secrets
Mar  5 08:21:27.422: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-36e8d485-9eef-488d-b1ce-7a4c293821f7" in namespace "projected-1280" to be "Succeeded or Failed"
Mar  5 08:21:27.428: INFO: Pod "pod-projected-secrets-36e8d485-9eef-488d-b1ce-7a4c293821f7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.363696ms
Mar  5 08:21:29.437: INFO: Pod "pod-projected-secrets-36e8d485-9eef-488d-b1ce-7a4c293821f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014905775s
STEP: Saw pod success
Mar  5 08:21:29.439: INFO: Pod "pod-projected-secrets-36e8d485-9eef-488d-b1ce-7a4c293821f7" satisfied condition "Succeeded or Failed"
Mar  5 08:21:29.446: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-projected-secrets-36e8d485-9eef-488d-b1ce-7a4c293821f7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  5 08:21:29.492: INFO: Waiting for pod pod-projected-secrets-36e8d485-9eef-488d-b1ce-7a4c293821f7 to disappear
Mar  5 08:21:29.497: INFO: Pod pod-projected-secrets-36e8d485-9eef-488d-b1ce-7a4c293821f7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:21:29.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1280" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":122,"skipped":2134,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:21:29.533: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9346
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9346
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9346
STEP: creating replication controller externalsvc in namespace services-9346
I0305 08:21:29.851020      26 runners.go:190] Created replication controller with name: externalsvc, namespace: services-9346, replica count: 2
I0305 08:21:32.901789      26 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar  5 08:21:32.969: INFO: Creating new exec pod
Mar  5 08:21:35.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=services-9346 execpod65tbt -- /bin/sh -x -c nslookup nodeport-service'
Mar  5 08:21:35.809: INFO: stderr: "+ nslookup nodeport-service\n"
Mar  5 08:21:35.809: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-9346.svc.cluster.local\tcanonical name = externalsvc.services-9346.svc.cluster.local.\nName:\texternalsvc.services-9346.svc.cluster.local\nAddress: 10.101.115.253\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9346, will wait for the garbage collector to delete the pods
Mar  5 08:21:35.909: INFO: Deleting ReplicationController externalsvc took: 14.738568ms
Mar  5 08:21:36.009: INFO: Terminating ReplicationController externalsvc pods took: 100.296869ms
Mar  5 08:21:40.276: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:21:40.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9346" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:10.812 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":277,"completed":123,"skipped":2168,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:21:40.351: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-600
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  5 08:21:41.266: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  5 08:21:43.300: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529301, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529301, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529301, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529301, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 08:21:46.344: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:21:46.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-600" for this suite.
STEP: Destroying namespace "webhook-600-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.664 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":277,"completed":124,"skipped":2192,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:21:47.016: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8473
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-162708f9-9ad4-455a-b902-09d4ea5fc90e
STEP: Creating a pod to test consume secrets
Mar  5 08:21:47.253: INFO: Waiting up to 5m0s for pod "pod-secrets-0ab02e9a-ce77-44f7-be4c-48966a8ac8c5" in namespace "secrets-8473" to be "Succeeded or Failed"
Mar  5 08:21:47.270: INFO: Pod "pod-secrets-0ab02e9a-ce77-44f7-be4c-48966a8ac8c5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.303816ms
Mar  5 08:21:49.278: INFO: Pod "pod-secrets-0ab02e9a-ce77-44f7-be4c-48966a8ac8c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025301655s
Mar  5 08:21:51.291: INFO: Pod "pod-secrets-0ab02e9a-ce77-44f7-be4c-48966a8ac8c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038081771s
STEP: Saw pod success
Mar  5 08:21:51.291: INFO: Pod "pod-secrets-0ab02e9a-ce77-44f7-be4c-48966a8ac8c5" satisfied condition "Succeeded or Failed"
Mar  5 08:21:51.300: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-secrets-0ab02e9a-ce77-44f7-be4c-48966a8ac8c5 container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 08:21:51.364: INFO: Waiting for pod pod-secrets-0ab02e9a-ce77-44f7-be4c-48966a8ac8c5 to disappear
Mar  5 08:21:51.376: INFO: Pod pod-secrets-0ab02e9a-ce77-44f7-be4c-48966a8ac8c5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:21:51.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8473" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":125,"skipped":2203,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:21:51.407: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8400
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-2cb5b524-1907-4e23-8449-52a71c17122b
STEP: Creating a pod to test consume configMaps
Mar  5 08:21:51.627: INFO: Waiting up to 5m0s for pod "pod-configmaps-d415d622-e1e5-437e-bc71-fecd39b64b85" in namespace "configmap-8400" to be "Succeeded or Failed"
Mar  5 08:21:51.636: INFO: Pod "pod-configmaps-d415d622-e1e5-437e-bc71-fecd39b64b85": Phase="Pending", Reason="", readiness=false. Elapsed: 8.566256ms
Mar  5 08:21:53.643: INFO: Pod "pod-configmaps-d415d622-e1e5-437e-bc71-fecd39b64b85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015898569s
Mar  5 08:21:55.651: INFO: Pod "pod-configmaps-d415d622-e1e5-437e-bc71-fecd39b64b85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023661405s
STEP: Saw pod success
Mar  5 08:21:55.651: INFO: Pod "pod-configmaps-d415d622-e1e5-437e-bc71-fecd39b64b85" satisfied condition "Succeeded or Failed"
Mar  5 08:21:55.659: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-configmaps-d415d622-e1e5-437e-bc71-fecd39b64b85 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 08:21:55.709: INFO: Waiting for pod pod-configmaps-d415d622-e1e5-437e-bc71-fecd39b64b85 to disappear
Mar  5 08:21:55.715: INFO: Pod pod-configmaps-d415d622-e1e5-437e-bc71-fecd39b64b85 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:21:55.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8400" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":126,"skipped":2224,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:21:55.750: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service nodeport-test with type=NodePort in namespace services-7391
STEP: creating replication controller nodeport-test in namespace services-7391
I0305 08:21:56.057554      26 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-7391, replica count: 2
Mar  5 08:21:59.108: INFO: Creating new exec pod
I0305 08:21:59.108510      26 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  5 08:22:04.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=services-7391 execpod5lh58 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar  5 08:22:04.573: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar  5 08:22:04.573: INFO: stdout: ""
Mar  5 08:22:04.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=services-7391 execpod5lh58 -- /bin/sh -x -c nc -zv -t -w 2 10.108.231.186 80'
Mar  5 08:22:04.903: INFO: stderr: "+ nc -zv -t -w 2 10.108.231.186 80\nConnection to 10.108.231.186 80 port [tcp/http] succeeded!\n"
Mar  5 08:22:04.903: INFO: stdout: ""
Mar  5 08:22:04.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=services-7391 execpod5lh58 -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.6 31417'
Mar  5 08:22:05.226: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.6 31417\nConnection to 192.168.0.6 31417 port [tcp/31417] succeeded!\n"
Mar  5 08:22:05.227: INFO: stdout: ""
Mar  5 08:22:05.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=services-7391 execpod5lh58 -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.5 31417'
Mar  5 08:22:05.601: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.5 31417\nConnection to 192.168.0.5 31417 port [tcp/31417] succeeded!\n"
Mar  5 08:22:05.601: INFO: stdout: ""
Mar  5 08:22:05.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=services-7391 execpod5lh58 -- /bin/sh -x -c nc -zv -t -w 2 157.90.125.65 31417'
Mar  5 08:22:05.920: INFO: stderr: "+ nc -zv -t -w 2 157.90.125.65 31417\nConnection to 157.90.125.65 31417 port [tcp/31417] succeeded!\n"
Mar  5 08:22:05.920: INFO: stdout: ""
Mar  5 08:22:05.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=services-7391 execpod5lh58 -- /bin/sh -x -c nc -zv -t -w 2 159.69.2.45 31417'
Mar  5 08:22:06.274: INFO: stderr: "+ nc -zv -t -w 2 159.69.2.45 31417\nConnection to 159.69.2.45 31417 port [tcp/31417] succeeded!\n"
Mar  5 08:22:06.274: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:22:06.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7391" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:10.563 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":277,"completed":127,"skipped":2279,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:22:06.313: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-5129
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Mar  5 08:22:06.507: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  5 08:23:06.759: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:23:06.770: INFO: Starting informer...
STEP: Starting pods...
Mar  5 08:23:07.001: INFO: Pod1 is running on devops-pool1-6c76d44df9-8tgs6. Tainting Node
Mar  5 08:23:09.244: INFO: Pod2 is running on devops-pool1-6c76d44df9-8tgs6. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Mar  5 08:23:20.788: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar  5 08:23:36.198: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:23:36.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5129" for this suite.

â€¢ [SLOW TEST:90.013 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":277,"completed":128,"skipped":2287,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:23:36.327: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-9828
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:23:36.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9828" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":277,"completed":129,"skipped":2301,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:23:36.555: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8231
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
Mar  5 08:23:37.298: INFO: created pod pod-service-account-defaultsa
Mar  5 08:23:37.298: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  5 08:23:37.310: INFO: created pod pod-service-account-mountsa
Mar  5 08:23:37.310: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  5 08:23:37.332: INFO: created pod pod-service-account-nomountsa
Mar  5 08:23:37.332: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  5 08:23:37.347: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  5 08:23:37.347: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  5 08:23:37.359: INFO: created pod pod-service-account-mountsa-mountspec
Mar  5 08:23:37.359: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  5 08:23:37.369: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  5 08:23:37.369: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  5 08:23:37.380: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  5 08:23:37.380: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  5 08:23:37.398: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  5 08:23:37.399: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  5 08:23:37.432: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  5 08:23:37.432: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:23:37.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8231" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":277,"completed":130,"skipped":2304,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:23:37.495: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4188
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Starting the proxy
Mar  5 08:23:37.730: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-378331353 proxy --unix-socket=/tmp/kubectl-proxy-unix483960916/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:23:37.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4188" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":277,"completed":131,"skipped":2395,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:23:37.900: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:23:38.132: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  5 08:23:43.138: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  5 08:23:49.160: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  5 08:23:51.167: INFO: Creating deployment "test-rollover-deployment"
Mar  5 08:23:51.199: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  5 08:23:53.218: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  5 08:23:53.230: INFO: Ensure that both replica sets have 1 created replica
Mar  5 08:23:53.245: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  5 08:23:53.285: INFO: Updating deployment test-rollover-deployment
Mar  5 08:23:53.285: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  5 08:23:55.312: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  5 08:23:55.324: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  5 08:23:55.338: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 08:23:55.338: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529433, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 08:23:57.355: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 08:23:57.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529433, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 08:23:59.352: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 08:23:59.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 08:24:01.355: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 08:24:01.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 08:24:03.352: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 08:24:03.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 08:24:05.351: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 08:24:05.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 08:24:07.358: INFO: all replica sets need to contain the pod-template-hash label
Mar  5 08:24:07.359: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529438, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529431, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 08:24:09.357: INFO: 
Mar  5 08:24:09.357: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar  5 08:24:09.386: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4799 /apis/apps/v1/namespaces/deployment-4799/deployments/test-rollover-deployment 9b20de7d-9db1-4983-a938-8d0d226f98bc 5629356 2 2021-03-05 08:23:51 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-05 08:23:53 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2021-03-05 08:24:08 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004a42948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-05 08:23:51 +0000 UTC,LastTransitionTime:2021-03-05 08:23:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-84f7f6f64b" has successfully progressed.,LastUpdateTime:2021-03-05 08:24:08 +0000 UTC,LastTransitionTime:2021-03-05 08:23:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  5 08:24:09.396: INFO: New ReplicaSet "test-rollover-deployment-84f7f6f64b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-84f7f6f64b  deployment-4799 /apis/apps/v1/namespaces/deployment-4799/replicasets/test-rollover-deployment-84f7f6f64b 5340d094-9198-4d8b-ad3f-bc98251ec541 5629338 2 2021-03-05 08:23:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 9b20de7d-9db1-4983-a938-8d0d226f98bc 0xc004a42fc7 0xc004a42fc8}] []  [{kube-controller-manager Update apps/v1 2021-03-05 08:24:08 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 98 50 48 100 101 55 100 45 57 100 98 49 45 52 57 56 51 45 97 57 51 56 45 56 100 48 100 50 50 54 102 57 56 98 99 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 84f7f6f64b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004a43058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  5 08:24:09.396: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  5 08:24:09.397: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4799 /apis/apps/v1/namespaces/deployment-4799/replicasets/test-rollover-controller 28b5b5da-58d0-434d-8c41-b95d060ca22f 5629355 2 2021-03-05 08:23:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 9b20de7d-9db1-4983-a938-8d0d226f98bc 0xc004a42db7 0xc004a42db8}] []  [{e2e.test Update apps/v1 2021-03-05 08:23:38 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2021-03-05 08:24:08 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 98 50 48 100 101 55 100 45 57 100 98 49 45 52 57 56 51 45 97 57 51 56 45 56 100 48 100 50 50 54 102 57 56 98 99 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004a42e58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  5 08:24:09.397: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-5686c4cfd5  deployment-4799 /apis/apps/v1/namespaces/deployment-4799/replicasets/test-rollover-deployment-5686c4cfd5 b3bb0a6d-431c-4dd7-b931-c607bb37e689 5629008 2 2021-03-05 08:23:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 9b20de7d-9db1-4983-a938-8d0d226f98bc 0xc004a42ec7 0xc004a42ec8}] []  [{kube-controller-manager Update apps/v1 2021-03-05 08:23:53 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 98 50 48 100 101 55 100 45 57 100 98 49 45 52 57 56 51 45 97 57 51 56 45 56 100 48 100 50 50 54 102 57 56 98 99 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 114 101 100 105 115 45 115 108 97 118 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5686c4cfd5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004a42f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  5 08:24:09.408: INFO: Pod "test-rollover-deployment-84f7f6f64b-dt7x7" is available:
&Pod{ObjectMeta:{test-rollover-deployment-84f7f6f64b-dt7x7 test-rollover-deployment-84f7f6f64b- deployment-4799 /api/v1/namespaces/deployment-4799/pods/test-rollover-deployment-84f7f6f64b-dt7x7 971ce975-e632-434f-ab5a-b49268845508 5629091 0 2021-03-05 08:23:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[cni.projectcalico.org/podIP:10.244.3.183/32 cni.projectcalico.org/podIPs:10.244.3.183/32] [{apps/v1 ReplicaSet test-rollover-deployment-84f7f6f64b 5340d094-9198-4d8b-ad3f-bc98251ec541 0xc003a7b627 0xc003a7b628}] []  [{kube-controller-manager Update v1 2021-03-05 08:23:53 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 53 51 52 48 100 48 57 52 45 57 49 57 56 45 52 100 56 98 45 97 100 51 102 45 98 99 57 56 50 53 49 101 99 53 52 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2021-03-05 08:23:54 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2021-03-05 08:23:58 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 51 46 49 56 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2zt79,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2zt79,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2zt79,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-8tgs6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 08:23:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 08:23:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 08:23:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 08:23:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.8,PodIP:10.244.3.183,StartTime:2021-03-05 08:23:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-05 08:23:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://4d0de228919f0bd77b7ab75ed750c8994f7100ae54d4d3f0f7220e4cfda532be,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:24:09.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4799" for this suite.

â€¢ [SLOW TEST:31.538 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":277,"completed":132,"skipped":2422,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:24:09.439: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7205
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-projected-all-test-volume-7714d536-d522-42c6-aac7-4281046ed91c
STEP: Creating secret with name secret-projected-all-test-volume-c27320ea-476b-45e9-b151-3f1d881dce37
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar  5 08:24:09.702: INFO: Waiting up to 5m0s for pod "projected-volume-8580d11c-449e-4d20-81bd-9b23e08016b9" in namespace "projected-7205" to be "Succeeded or Failed"
Mar  5 08:24:09.706: INFO: Pod "projected-volume-8580d11c-449e-4d20-81bd-9b23e08016b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.755054ms
Mar  5 08:24:11.716: INFO: Pod "projected-volume-8580d11c-449e-4d20-81bd-9b23e08016b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013428949s
Mar  5 08:24:13.724: INFO: Pod "projected-volume-8580d11c-449e-4d20-81bd-9b23e08016b9": Phase="Running", Reason="", readiness=true. Elapsed: 4.021648308s
Mar  5 08:24:15.731: INFO: Pod "projected-volume-8580d11c-449e-4d20-81bd-9b23e08016b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028683424s
STEP: Saw pod success
Mar  5 08:24:15.732: INFO: Pod "projected-volume-8580d11c-449e-4d20-81bd-9b23e08016b9" satisfied condition "Succeeded or Failed"
Mar  5 08:24:15.740: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod projected-volume-8580d11c-449e-4d20-81bd-9b23e08016b9 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar  5 08:24:15.844: INFO: Waiting for pod projected-volume-8580d11c-449e-4d20-81bd-9b23e08016b9 to disappear
Mar  5 08:24:15.849: INFO: Pod projected-volume-8580d11c-449e-4d20-81bd-9b23e08016b9 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:24:15.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7205" for this suite.

â€¢ [SLOW TEST:6.440 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":277,"completed":133,"skipped":2444,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:24:15.882: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7511
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:24:20.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7511" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":134,"skipped":2479,"failed":0}

------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:24:20.252: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9620
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9620.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9620.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9620.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9620.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9620.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9620.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9620.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9620.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9620.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9620.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  5 08:24:28.581: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:28.591: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:28.603: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:28.611: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:28.650: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:28.660: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:28.676: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:28.690: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:28.764: INFO: Lookups using dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9620.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9620.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local jessie_udp@dns-test-service-2.dns-9620.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9620.svc.cluster.local]

Mar  5 08:24:33.782: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:33.789: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:33.796: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:33.803: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:33.863: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:33.872: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:33.882: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:33.897: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:33.912: INFO: Lookups using dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9620.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9620.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local jessie_udp@dns-test-service-2.dns-9620.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9620.svc.cluster.local]

Mar  5 08:24:38.778: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:38.788: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:38.798: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:38.825: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:38.855: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:38.866: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:38.884: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:38.894: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:38.924: INFO: Lookups using dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9620.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9620.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local jessie_udp@dns-test-service-2.dns-9620.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9620.svc.cluster.local]

Mar  5 08:24:43.786: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:43.825: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:43.857: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:43.876: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:44.087: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:44.104: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:44.117: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:44.130: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:44.157: INFO: Lookups using dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9620.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9620.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local jessie_udp@dns-test-service-2.dns-9620.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9620.svc.cluster.local]

Mar  5 08:24:48.773: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:48.785: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:48.793: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:48.810: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:48.846: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:48.855: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:48.864: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:48.873: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:48.889: INFO: Lookups using dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9620.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9620.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local jessie_udp@dns-test-service-2.dns-9620.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9620.svc.cluster.local]

Mar  5 08:24:53.781: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:53.800: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:53.808: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:53.817: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:53.838: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:53.854: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:53.861: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:53.867: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9620.svc.cluster.local from pod dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7: the server could not find the requested resource (get pods dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7)
Mar  5 08:24:53.889: INFO: Lookups using dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9620.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9620.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9620.svc.cluster.local jessie_udp@dns-test-service-2.dns-9620.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9620.svc.cluster.local]

Mar  5 08:24:58.870: INFO: DNS probes using dns-9620/dns-test-ff2b6cd5-bf36-4e52-a8c8-8341873e00a7 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:24:58.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9620" for this suite.

â€¢ [SLOW TEST:38.779 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":277,"completed":135,"skipped":2479,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:24:59.070: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7705
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Mar  5 08:25:00.535: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
W0305 08:25:00.534917      26 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar  5 08:25:00.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7705" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":277,"completed":136,"skipped":2530,"failed":0}

------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:25:00.554: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7483
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting the proxy server
Mar  5 08:25:00.778: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-378331353 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:25:00.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7483" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":277,"completed":137,"skipped":2530,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:25:00.928: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6073
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-6073
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  5 08:25:01.117: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  5 08:25:01.320: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  5 08:25:03.341: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  5 08:25:05.334: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 08:25:07.330: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 08:25:09.330: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 08:25:11.332: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 08:25:13.330: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 08:25:15.337: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  5 08:25:17.362: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  5 08:25:17.383: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  5 08:25:19.407: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  5 08:25:19.436: INFO: The status of Pod netserver-2 is Running (Ready = true)
Mar  5 08:25:19.473: INFO: The status of Pod netserver-3 is Running (Ready = true)
Mar  5 08:25:19.512: INFO: The status of Pod netserver-4 is Running (Ready = true)
Mar  5 08:25:19.540: INFO: The status of Pod netserver-5 is Running (Ready = true)
STEP: Creating test pods
Mar  5 08:25:23.671: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.240 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6073 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:25:23.671: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:25:24.940: INFO: Found all expected endpoints: [netserver-0]
Mar  5 08:25:24.946: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.78 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6073 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:25:24.946: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:25:26.136: INFO: Found all expected endpoints: [netserver-1]
Mar  5 08:25:26.143: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.15 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6073 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:25:26.144: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:25:27.374: INFO: Found all expected endpoints: [netserver-2]
Mar  5 08:25:27.382: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.192 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6073 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:25:27.382: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:25:28.553: INFO: Found all expected endpoints: [netserver-3]
Mar  5 08:25:28.560: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.5.215 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6073 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:25:28.560: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:25:29.733: INFO: Found all expected endpoints: [netserver-4]
Mar  5 08:25:29.738: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.4.177 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6073 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:25:29.738: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:25:30.937: INFO: Found all expected endpoints: [netserver-5]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:25:30.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6073" for this suite.

â€¢ [SLOW TEST:30.032 seconds]
[sig-network] Networking
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":138,"skipped":2538,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:25:30.961: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8668
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar  5 08:25:31.171: INFO: Waiting up to 5m0s for pod "downward-api-e2c1f8d8-1196-4c08-816a-4848bb46cfbc" in namespace "downward-api-8668" to be "Succeeded or Failed"
Mar  5 08:25:31.190: INFO: Pod "downward-api-e2c1f8d8-1196-4c08-816a-4848bb46cfbc": Phase="Pending", Reason="", readiness=false. Elapsed: 19.671299ms
Mar  5 08:25:33.200: INFO: Pod "downward-api-e2c1f8d8-1196-4c08-816a-4848bb46cfbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029110815s
STEP: Saw pod success
Mar  5 08:25:33.200: INFO: Pod "downward-api-e2c1f8d8-1196-4c08-816a-4848bb46cfbc" satisfied condition "Succeeded or Failed"
Mar  5 08:25:33.207: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downward-api-e2c1f8d8-1196-4c08-816a-4848bb46cfbc container dapi-container: <nil>
STEP: delete the pod
Mar  5 08:25:33.272: INFO: Waiting for pod downward-api-e2c1f8d8-1196-4c08-816a-4848bb46cfbc to disappear
Mar  5 08:25:33.279: INFO: Pod downward-api-e2c1f8d8-1196-4c08-816a-4848bb46cfbc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:25:33.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8668" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":277,"completed":139,"skipped":2554,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:25:33.324: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Mar  5 08:25:38.072: INFO: Successfully updated pod "labelsupdateb72067f6-029a-4575-a158-5a7f246bdb4f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:25:40.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4872" for this suite.

â€¢ [SLOW TEST:6.804 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":277,"completed":140,"skipped":2621,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:25:40.130: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5630
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Mar  5 08:25:40.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 create -f - --namespace=kubectl-5630'
Mar  5 08:25:40.944: INFO: stderr: ""
Mar  5 08:25:40.944: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  5 08:25:40.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5630'
Mar  5 08:25:41.071: INFO: stderr: ""
Mar  5 08:25:41.071: INFO: stdout: "update-demo-nautilus-sbzzp update-demo-nautilus-xsglf "
Mar  5 08:25:41.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods update-demo-nautilus-sbzzp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5630'
Mar  5 08:25:41.233: INFO: stderr: ""
Mar  5 08:25:41.233: INFO: stdout: ""
Mar  5 08:25:41.233: INFO: update-demo-nautilus-sbzzp is created but not running
Mar  5 08:25:46.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5630'
Mar  5 08:25:46.383: INFO: stderr: ""
Mar  5 08:25:46.383: INFO: stdout: "update-demo-nautilus-sbzzp update-demo-nautilus-xsglf "
Mar  5 08:25:46.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods update-demo-nautilus-sbzzp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5630'
Mar  5 08:25:46.502: INFO: stderr: ""
Mar  5 08:25:46.502: INFO: stdout: "true"
Mar  5 08:25:46.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods update-demo-nautilus-sbzzp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5630'
Mar  5 08:25:46.635: INFO: stderr: ""
Mar  5 08:25:46.635: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 08:25:46.635: INFO: validating pod update-demo-nautilus-sbzzp
Mar  5 08:25:46.646: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 08:25:46.646: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 08:25:46.646: INFO: update-demo-nautilus-sbzzp is verified up and running
Mar  5 08:25:46.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods update-demo-nautilus-xsglf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5630'
Mar  5 08:25:46.784: INFO: stderr: ""
Mar  5 08:25:46.784: INFO: stdout: "true"
Mar  5 08:25:46.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods update-demo-nautilus-xsglf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5630'
Mar  5 08:25:46.920: INFO: stderr: ""
Mar  5 08:25:46.920: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 08:25:46.920: INFO: validating pod update-demo-nautilus-xsglf
Mar  5 08:25:46.928: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 08:25:46.928: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 08:25:46.928: INFO: update-demo-nautilus-xsglf is verified up and running
STEP: using delete to clean up resources
Mar  5 08:25:46.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 delete --grace-period=0 --force -f - --namespace=kubectl-5630'
Mar  5 08:25:47.078: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 08:25:47.079: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  5 08:25:47.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5630'
Mar  5 08:25:47.225: INFO: stderr: "No resources found in kubectl-5630 namespace.\n"
Mar  5 08:25:47.225: INFO: stdout: ""
Mar  5 08:25:47.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods -l name=update-demo --namespace=kubectl-5630 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  5 08:25:47.367: INFO: stderr: ""
Mar  5 08:25:47.367: INFO: stdout: "update-demo-nautilus-sbzzp\nupdate-demo-nautilus-xsglf\n"
Mar  5 08:25:47.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5630'
Mar  5 08:25:48.036: INFO: stderr: "No resources found in kubectl-5630 namespace.\n"
Mar  5 08:25:48.036: INFO: stdout: ""
Mar  5 08:25:48.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods -l name=update-demo --namespace=kubectl-5630 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  5 08:25:48.175: INFO: stderr: ""
Mar  5 08:25:48.175: INFO: stdout: "update-demo-nautilus-sbzzp\nupdate-demo-nautilus-xsglf\n"
Mar  5 08:25:48.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5630'
Mar  5 08:25:48.510: INFO: stderr: "No resources found in kubectl-5630 namespace.\n"
Mar  5 08:25:48.510: INFO: stdout: ""
Mar  5 08:25:48.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods -l name=update-demo --namespace=kubectl-5630 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  5 08:25:48.650: INFO: stderr: ""
Mar  5 08:25:48.650: INFO: stdout: "update-demo-nautilus-sbzzp\nupdate-demo-nautilus-xsglf\n"
Mar  5 08:25:48.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5630'
Mar  5 08:25:49.007: INFO: stderr: "No resources found in kubectl-5630 namespace.\n"
Mar  5 08:25:49.007: INFO: stdout: ""
Mar  5 08:25:49.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods -l name=update-demo --namespace=kubectl-5630 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  5 08:25:49.163: INFO: stderr: ""
Mar  5 08:25:49.163: INFO: stdout: "update-demo-nautilus-sbzzp\nupdate-demo-nautilus-xsglf\n"
Mar  5 08:25:49.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5630'
Mar  5 08:25:49.518: INFO: stderr: "No resources found in kubectl-5630 namespace.\n"
Mar  5 08:25:49.518: INFO: stdout: ""
Mar  5 08:25:49.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods -l name=update-demo --namespace=kubectl-5630 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  5 08:25:49.666: INFO: stderr: ""
Mar  5 08:25:49.667: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:25:49.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5630" for this suite.

â€¢ [SLOW TEST:9.604 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":277,"completed":141,"skipped":2629,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:25:49.737: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9954
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's command
Mar  5 08:25:50.054: INFO: Waiting up to 5m0s for pod "var-expansion-71eaf136-1693-471a-a96d-9df58d38a44d" in namespace "var-expansion-9954" to be "Succeeded or Failed"
Mar  5 08:25:50.061: INFO: Pod "var-expansion-71eaf136-1693-471a-a96d-9df58d38a44d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.147548ms
Mar  5 08:25:52.083: INFO: Pod "var-expansion-71eaf136-1693-471a-a96d-9df58d38a44d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027902996s
Mar  5 08:25:54.091: INFO: Pod "var-expansion-71eaf136-1693-471a-a96d-9df58d38a44d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036424486s
STEP: Saw pod success
Mar  5 08:25:54.092: INFO: Pod "var-expansion-71eaf136-1693-471a-a96d-9df58d38a44d" satisfied condition "Succeeded or Failed"
Mar  5 08:25:54.098: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod var-expansion-71eaf136-1693-471a-a96d-9df58d38a44d container dapi-container: <nil>
STEP: delete the pod
Mar  5 08:25:54.150: INFO: Waiting for pod var-expansion-71eaf136-1693-471a-a96d-9df58d38a44d to disappear
Mar  5 08:25:54.158: INFO: Pod var-expansion-71eaf136-1693-471a-a96d-9df58d38a44d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:25:54.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9954" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":277,"completed":142,"skipped":2655,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:25:54.193: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4980
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating cluster-info
Mar  5 08:25:54.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 cluster-info'
Mar  5 08:25:54.572: INFO: stderr: ""
Mar  5 08:25:54.572: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mKubeDNSUpstream\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns-upstream:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:25:54.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4980" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":277,"completed":143,"skipped":2678,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:25:54.598: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5702
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-5702
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5702
STEP: Creating statefulset with conflicting port in namespace statefulset-5702
STEP: Waiting until pod test-pod will start running in namespace statefulset-5702
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5702
Mar  5 08:25:58.940: INFO: Observed stateful pod in namespace: statefulset-5702, name: ss-0, uid: 463f5585-3876-4c3e-b2cb-506f532d75dd, status phase: Pending. Waiting for statefulset controller to delete.
Mar  5 08:25:59.323: INFO: Observed stateful pod in namespace: statefulset-5702, name: ss-0, uid: 463f5585-3876-4c3e-b2cb-506f532d75dd, status phase: Failed. Waiting for statefulset controller to delete.
Mar  5 08:25:59.339: INFO: Observed stateful pod in namespace: statefulset-5702, name: ss-0, uid: 463f5585-3876-4c3e-b2cb-506f532d75dd, status phase: Failed. Waiting for statefulset controller to delete.
Mar  5 08:25:59.353: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5702
STEP: Removing pod with conflicting port in namespace statefulset-5702
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5702 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar  5 08:26:03.426: INFO: Deleting all statefulset in ns statefulset-5702
Mar  5 08:26:03.434: INFO: Scaling statefulset ss to 0
Mar  5 08:26:13.471: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 08:26:13.479: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:26:13.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5702" for this suite.

â€¢ [SLOW TEST:18.945 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":277,"completed":144,"skipped":2686,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:26:13.551: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6226
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-587bd6e8-b0d5-4a4b-b6f9-1f96c940ca63
STEP: Creating a pod to test consume configMaps
Mar  5 08:26:13.797: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5b1661e6-02bc-48b7-a293-46dc8ad2d93b" in namespace "projected-6226" to be "Succeeded or Failed"
Mar  5 08:26:13.806: INFO: Pod "pod-projected-configmaps-5b1661e6-02bc-48b7-a293-46dc8ad2d93b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.425356ms
Mar  5 08:26:15.813: INFO: Pod "pod-projected-configmaps-5b1661e6-02bc-48b7-a293-46dc8ad2d93b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016556078s
Mar  5 08:26:17.823: INFO: Pod "pod-projected-configmaps-5b1661e6-02bc-48b7-a293-46dc8ad2d93b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026258601s
STEP: Saw pod success
Mar  5 08:26:17.823: INFO: Pod "pod-projected-configmaps-5b1661e6-02bc-48b7-a293-46dc8ad2d93b" satisfied condition "Succeeded or Failed"
Mar  5 08:26:17.830: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-projected-configmaps-5b1661e6-02bc-48b7-a293-46dc8ad2d93b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 08:26:17.866: INFO: Waiting for pod pod-projected-configmaps-5b1661e6-02bc-48b7-a293-46dc8ad2d93b to disappear
Mar  5 08:26:17.875: INFO: Pod pod-projected-configmaps-5b1661e6-02bc-48b7-a293-46dc8ad2d93b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:26:17.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6226" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":145,"skipped":2727,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:26:17.902: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7266
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override arguments
Mar  5 08:26:18.156: INFO: Waiting up to 5m0s for pod "client-containers-1989d978-286a-40ec-8695-8c94d8b9c8cd" in namespace "containers-7266" to be "Succeeded or Failed"
Mar  5 08:26:18.161: INFO: Pod "client-containers-1989d978-286a-40ec-8695-8c94d8b9c8cd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.401154ms
Mar  5 08:26:20.170: INFO: Pod "client-containers-1989d978-286a-40ec-8695-8c94d8b9c8cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014312931s
STEP: Saw pod success
Mar  5 08:26:20.170: INFO: Pod "client-containers-1989d978-286a-40ec-8695-8c94d8b9c8cd" satisfied condition "Succeeded or Failed"
Mar  5 08:26:20.176: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod client-containers-1989d978-286a-40ec-8695-8c94d8b9c8cd container test-container: <nil>
STEP: delete the pod
Mar  5 08:26:20.225: INFO: Waiting for pod client-containers-1989d978-286a-40ec-8695-8c94d8b9c8cd to disappear
Mar  5 08:26:20.232: INFO: Pod client-containers-1989d978-286a-40ec-8695-8c94d8b9c8cd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:26:20.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7266" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":277,"completed":146,"skipped":2760,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:26:20.276: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9802
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-1d01ff42-fe06-45ba-92d8-19538f2d55aa
STEP: Creating a pod to test consume secrets
Mar  5 08:26:20.513: INFO: Waiting up to 5m0s for pod "pod-secrets-338da083-dd65-467b-8285-c06ec95ed0d5" in namespace "secrets-9802" to be "Succeeded or Failed"
Mar  5 08:26:20.519: INFO: Pod "pod-secrets-338da083-dd65-467b-8285-c06ec95ed0d5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.845184ms
Mar  5 08:26:22.528: INFO: Pod "pod-secrets-338da083-dd65-467b-8285-c06ec95ed0d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014867419s
STEP: Saw pod success
Mar  5 08:26:22.530: INFO: Pod "pod-secrets-338da083-dd65-467b-8285-c06ec95ed0d5" satisfied condition "Succeeded or Failed"
Mar  5 08:26:22.537: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-secrets-338da083-dd65-467b-8285-c06ec95ed0d5 container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 08:26:22.574: INFO: Waiting for pod pod-secrets-338da083-dd65-467b-8285-c06ec95ed0d5 to disappear
Mar  5 08:26:22.599: INFO: Pod pod-secrets-338da083-dd65-467b-8285-c06ec95ed0d5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:26:22.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9802" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":147,"skipped":2774,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:26:22.629: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1497
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating secret secrets-1497/secret-test-68ac1af3-e7ba-4723-bf05-1ac0d0c2bbe2
STEP: Creating a pod to test consume secrets
Mar  5 08:26:22.879: INFO: Waiting up to 5m0s for pod "pod-configmaps-a5a38def-04bb-49ab-9869-db80536d28bf" in namespace "secrets-1497" to be "Succeeded or Failed"
Mar  5 08:26:22.886: INFO: Pod "pod-configmaps-a5a38def-04bb-49ab-9869-db80536d28bf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.460423ms
Mar  5 08:26:24.896: INFO: Pod "pod-configmaps-a5a38def-04bb-49ab-9869-db80536d28bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016660924s
STEP: Saw pod success
Mar  5 08:26:24.896: INFO: Pod "pod-configmaps-a5a38def-04bb-49ab-9869-db80536d28bf" satisfied condition "Succeeded or Failed"
Mar  5 08:26:24.904: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-configmaps-a5a38def-04bb-49ab-9869-db80536d28bf container env-test: <nil>
STEP: delete the pod
Mar  5 08:26:24.957: INFO: Waiting for pod pod-configmaps-a5a38def-04bb-49ab-9869-db80536d28bf to disappear
Mar  5 08:26:24.963: INFO: Pod pod-configmaps-a5a38def-04bb-49ab-9869-db80536d28bf no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:26:24.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1497" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":277,"completed":148,"skipped":2824,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:26:24.996: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  5 08:26:25.292: INFO: Waiting up to 5m0s for pod "pod-5d5adf6f-517d-4ca9-ab6c-451e42a20d6f" in namespace "emptydir-5846" to be "Succeeded or Failed"
Mar  5 08:26:25.300: INFO: Pod "pod-5d5adf6f-517d-4ca9-ab6c-451e42a20d6f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.930213ms
Mar  5 08:26:27.316: INFO: Pod "pod-5d5adf6f-517d-4ca9-ab6c-451e42a20d6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023893391s
STEP: Saw pod success
Mar  5 08:26:27.316: INFO: Pod "pod-5d5adf6f-517d-4ca9-ab6c-451e42a20d6f" satisfied condition "Succeeded or Failed"
Mar  5 08:26:27.323: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-5d5adf6f-517d-4ca9-ab6c-451e42a20d6f container test-container: <nil>
STEP: delete the pod
Mar  5 08:26:27.367: INFO: Waiting for pod pod-5d5adf6f-517d-4ca9-ab6c-451e42a20d6f to disappear
Mar  5 08:26:27.374: INFO: Pod pod-5d5adf6f-517d-4ca9-ab6c-451e42a20d6f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:26:27.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5846" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":149,"skipped":2830,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:26:27.400: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-300
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-300, will wait for the garbage collector to delete the pods
Mar  5 08:26:31.688: INFO: Deleting Job.batch foo took: 13.79848ms
Mar  5 08:26:31.788: INFO: Terminating Job.batch foo pods took: 100.58156ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:27:13.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-300" for this suite.

â€¢ [SLOW TEST:46.324 seconds]
[sig-apps] Job
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":277,"completed":150,"skipped":2832,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:27:13.726: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2750
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-2750
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2750
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2750
Mar  5 08:27:13.946: INFO: Found 0 stateful pods, waiting for 1
Mar  5 08:27:23.953: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  5 08:27:23.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-2750 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  5 08:27:24.278: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  5 08:27:24.279: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  5 08:27:24.279: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  5 08:27:24.285: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  5 08:27:34.297: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 08:27:34.298: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 08:27:34.337: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998543s
Mar  5 08:27:35.345: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992891411s
Mar  5 08:27:36.359: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985114821s
Mar  5 08:27:37.366: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.971647864s
Mar  5 08:27:38.376: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.963746145s
Mar  5 08:27:39.384: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.954070365s
Mar  5 08:27:40.405: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.946472266s
Mar  5 08:27:41.414: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.924727191s
Mar  5 08:27:42.425: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.915853841s
Mar  5 08:27:43.436: INFO: Verifying statefulset ss doesn't scale past 1 for another 905.081244ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2750
Mar  5 08:27:44.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-2750 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  5 08:27:44.765: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  5 08:27:44.765: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  5 08:27:44.765: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  5 08:27:44.786: INFO: Found 1 stateful pods, waiting for 3
Mar  5 08:27:54.795: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 08:27:54.795: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 08:27:54.795: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar  5 08:27:54.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-2750 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  5 08:27:55.173: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  5 08:27:55.173: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  5 08:27:55.173: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  5 08:27:55.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-2750 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  5 08:27:55.531: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  5 08:27:55.531: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  5 08:27:55.531: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  5 08:27:55.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-2750 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  5 08:27:55.901: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  5 08:27:55.901: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  5 08:27:55.901: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  5 08:27:55.901: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 08:27:55.908: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar  5 08:28:05.927: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 08:28:05.927: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 08:28:05.927: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 08:28:05.966: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999209s
Mar  5 08:28:06.975: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986115899s
Mar  5 08:28:07.993: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.976901741s
Mar  5 08:28:09.008: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.957216992s
Mar  5 08:28:10.020: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.943844425s
Mar  5 08:28:11.038: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.932814969s
Mar  5 08:28:12.046: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.914774724s
Mar  5 08:28:13.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.905677971s
Mar  5 08:28:14.064: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.895938902s
Mar  5 08:28:15.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 888.117611ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2750
Mar  5 08:28:16.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-2750 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  5 08:28:16.742: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  5 08:28:16.742: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  5 08:28:16.742: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  5 08:28:16.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-2750 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  5 08:28:17.071: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  5 08:28:17.071: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  5 08:28:17.071: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  5 08:28:17.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-2750 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  5 08:28:17.378: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  5 08:28:17.378: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  5 08:28:17.378: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  5 08:28:17.378: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar  5 08:28:37.410: INFO: Deleting all statefulset in ns statefulset-2750
Mar  5 08:28:37.421: INFO: Scaling statefulset ss to 0
Mar  5 08:28:37.442: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 08:28:37.448: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:28:37.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2750" for this suite.

â€¢ [SLOW TEST:83.798 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":277,"completed":151,"skipped":2838,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:28:37.534: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7264
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  5 08:28:39.782: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:28:39.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7264" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":277,"completed":152,"skipped":2857,"failed":0}
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:28:39.838: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4035
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  5 08:28:42.090: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:28:42.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4035" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":153,"skipped":2859,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:28:42.155: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5402
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 08:28:42.402: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cc64f665-fd3c-46be-b6c0-1a0795223203" in namespace "downward-api-5402" to be "Succeeded or Failed"
Mar  5 08:28:42.409: INFO: Pod "downwardapi-volume-cc64f665-fd3c-46be-b6c0-1a0795223203": Phase="Pending", Reason="", readiness=false. Elapsed: 6.17039ms
Mar  5 08:28:44.488: INFO: Pod "downwardapi-volume-cc64f665-fd3c-46be-b6c0-1a0795223203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.084936153s
STEP: Saw pod success
Mar  5 08:28:44.488: INFO: Pod "downwardapi-volume-cc64f665-fd3c-46be-b6c0-1a0795223203" satisfied condition "Succeeded or Failed"
Mar  5 08:28:44.549: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-cc64f665-fd3c-46be-b6c0-1a0795223203 container client-container: <nil>
STEP: delete the pod
Mar  5 08:28:44.622: INFO: Waiting for pod downwardapi-volume-cc64f665-fd3c-46be-b6c0-1a0795223203 to disappear
Mar  5 08:28:44.626: INFO: Pod downwardapi-volume-cc64f665-fd3c-46be-b6c0-1a0795223203 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:28:44.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5402" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":277,"completed":154,"skipped":2866,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:28:44.659: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9865
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:28:44.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9865" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":277,"completed":155,"skipped":2875,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:28:44.959: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6793
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-4pwl
STEP: Creating a pod to test atomic-volume-subpath
Mar  5 08:28:45.202: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4pwl" in namespace "subpath-6793" to be "Succeeded or Failed"
Mar  5 08:28:45.206: INFO: Pod "pod-subpath-test-configmap-4pwl": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079025ms
Mar  5 08:28:47.212: INFO: Pod "pod-subpath-test-configmap-4pwl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010006824s
Mar  5 08:28:49.222: INFO: Pod "pod-subpath-test-configmap-4pwl": Phase="Running", Reason="", readiness=true. Elapsed: 4.019700244s
Mar  5 08:28:51.237: INFO: Pod "pod-subpath-test-configmap-4pwl": Phase="Running", Reason="", readiness=true. Elapsed: 6.034871854s
Mar  5 08:28:53.246: INFO: Pod "pod-subpath-test-configmap-4pwl": Phase="Running", Reason="", readiness=true. Elapsed: 8.043935441s
Mar  5 08:28:55.255: INFO: Pod "pod-subpath-test-configmap-4pwl": Phase="Running", Reason="", readiness=true. Elapsed: 10.053427638s
Mar  5 08:28:57.265: INFO: Pod "pod-subpath-test-configmap-4pwl": Phase="Running", Reason="", readiness=true. Elapsed: 12.063095388s
Mar  5 08:28:59.274: INFO: Pod "pod-subpath-test-configmap-4pwl": Phase="Running", Reason="", readiness=true. Elapsed: 14.072082887s
Mar  5 08:29:01.287: INFO: Pod "pod-subpath-test-configmap-4pwl": Phase="Running", Reason="", readiness=true. Elapsed: 16.084853516s
Mar  5 08:29:03.294: INFO: Pod "pod-subpath-test-configmap-4pwl": Phase="Running", Reason="", readiness=true. Elapsed: 18.092598616s
Mar  5 08:29:05.303: INFO: Pod "pod-subpath-test-configmap-4pwl": Phase="Running", Reason="", readiness=true. Elapsed: 20.101297726s
Mar  5 08:29:07.312: INFO: Pod "pod-subpath-test-configmap-4pwl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.109753851s
STEP: Saw pod success
Mar  5 08:29:07.312: INFO: Pod "pod-subpath-test-configmap-4pwl" satisfied condition "Succeeded or Failed"
Mar  5 08:29:07.321: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-subpath-test-configmap-4pwl container test-container-subpath-configmap-4pwl: <nil>
STEP: delete the pod
Mar  5 08:29:07.375: INFO: Waiting for pod pod-subpath-test-configmap-4pwl to disappear
Mar  5 08:29:07.383: INFO: Pod pod-subpath-test-configmap-4pwl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4pwl
Mar  5 08:29:07.383: INFO: Deleting pod "pod-subpath-test-configmap-4pwl" in namespace "subpath-6793"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:29:07.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6793" for this suite.

â€¢ [SLOW TEST:22.456 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":277,"completed":156,"skipped":2886,"failed":0}
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:29:07.416: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8020
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:29:07.719: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"2f9a8310-366e-4913-9298-430302db9f9c", Controller:(*bool)(0xc0034d066a), BlockOwnerDeletion:(*bool)(0xc0034d066b)}}
Mar  5 08:29:07.734: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"bba878a1-60e9-4934-8124-6177a196b44f", Controller:(*bool)(0xc0051ad7c2), BlockOwnerDeletion:(*bool)(0xc0051ad7c3)}}
Mar  5 08:29:07.750: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"02c7e74f-c6f0-494a-95a4-9f4573338b79", Controller:(*bool)(0xc00696f5fa), BlockOwnerDeletion:(*bool)(0xc00696f5fb)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:29:12.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8020" for this suite.

â€¢ [SLOW TEST:5.398 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":277,"completed":157,"skipped":2886,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:29:12.817: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4891
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:29:13.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4891" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":277,"completed":158,"skipped":2907,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:29:13.092: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4323
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  5 08:29:14.031: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  5 08:29:16.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529754, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529754, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529754, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529754, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 08:29:19.120: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:29:19.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4323" for this suite.
STEP: Destroying namespace "webhook-4323-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.389 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":277,"completed":159,"skipped":2916,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:29:19.482: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4456
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  5 08:29:20.374: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  5 08:29:22.404: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529760, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529760, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529760, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529760, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 08:29:25.462: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:29:25.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4456" for this suite.
STEP: Destroying namespace "webhook-4456-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.146 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":277,"completed":160,"skipped":2918,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:29:25.629: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2915
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-3a5694d0-628a-4b25-9b24-96b7dda2f17e
STEP: Creating configMap with name cm-test-opt-upd-a1c6a97d-45c5-42a9-ac88-2362fb60fa27
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-3a5694d0-628a-4b25-9b24-96b7dda2f17e
STEP: Updating configmap cm-test-opt-upd-a1c6a97d-45c5-42a9-ac88-2362fb60fa27
STEP: Creating configMap with name cm-test-opt-create-661bd876-b943-4f68-8081-597030f679d4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:29:30.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2915" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":161,"skipped":2923,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:29:30.270: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7811
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1418
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  5 08:29:30.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7811'
Mar  5 08:29:30.658: INFO: stderr: ""
Mar  5 08:29:30.658: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1423
Mar  5 08:29:30.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 delete pods e2e-test-httpd-pod --namespace=kubectl-7811'
Mar  5 08:29:33.877: INFO: stderr: ""
Mar  5 08:29:33.877: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:29:33.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7811" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":277,"completed":162,"skipped":2927,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:29:33.907: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-985
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:29:34.142: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:29:35.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-985" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":277,"completed":163,"skipped":2931,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:29:35.516: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-1862
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test hostPath mode
Mar  5 08:29:36.097: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-1862" to be "Succeeded or Failed"
Mar  5 08:29:36.126: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 29.234282ms
Mar  5 08:29:38.152: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054641601s
STEP: Saw pod success
Mar  5 08:29:38.152: INFO: Pod "pod-host-path-test" satisfied condition "Succeeded or Failed"
Mar  5 08:29:38.159: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar  5 08:29:38.494: INFO: Waiting for pod pod-host-path-test to disappear
Mar  5 08:29:38.498: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:29:38.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-1862" for this suite.
â€¢{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":164,"skipped":2943,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:29:38.547: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1977
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:29:38.783: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar  5 08:29:41.917: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:29:41.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1977" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":277,"completed":165,"skipped":2968,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:29:41.986: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1263
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar  5 08:29:43.090: INFO: Pod name wrapped-volume-race-90c0e412-928c-4712-beb7-280ecddef6fe: Found 0 pods out of 5
Mar  5 08:29:48.102: INFO: Pod name wrapped-volume-race-90c0e412-928c-4712-beb7-280ecddef6fe: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-90c0e412-928c-4712-beb7-280ecddef6fe in namespace emptydir-wrapper-1263, will wait for the garbage collector to delete the pods
Mar  5 08:29:58.257: INFO: Deleting ReplicationController wrapped-volume-race-90c0e412-928c-4712-beb7-280ecddef6fe took: 18.647212ms
Mar  5 08:29:58.357: INFO: Terminating ReplicationController wrapped-volume-race-90c0e412-928c-4712-beb7-280ecddef6fe pods took: 100.777017ms
STEP: Creating RC which spawns configmap-volume pods
Mar  5 08:30:05.503: INFO: Pod name wrapped-volume-race-b95404e2-da01-42ec-ace2-c6aaacf5fd99: Found 0 pods out of 5
Mar  5 08:30:10.515: INFO: Pod name wrapped-volume-race-b95404e2-da01-42ec-ace2-c6aaacf5fd99: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b95404e2-da01-42ec-ace2-c6aaacf5fd99 in namespace emptydir-wrapper-1263, will wait for the garbage collector to delete the pods
Mar  5 08:30:20.633: INFO: Deleting ReplicationController wrapped-volume-race-b95404e2-da01-42ec-ace2-c6aaacf5fd99 took: 17.931117ms
Mar  5 08:30:23.034: INFO: Terminating ReplicationController wrapped-volume-race-b95404e2-da01-42ec-ace2-c6aaacf5fd99 pods took: 2.401707555s
STEP: Creating RC which spawns configmap-volume pods
Mar  5 08:30:33.980: INFO: Pod name wrapped-volume-race-6d4b8b64-640f-449c-a359-08d77dc99083: Found 0 pods out of 5
Mar  5 08:30:38.996: INFO: Pod name wrapped-volume-race-6d4b8b64-640f-449c-a359-08d77dc99083: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6d4b8b64-640f-449c-a359-08d77dc99083 in namespace emptydir-wrapper-1263, will wait for the garbage collector to delete the pods
Mar  5 08:30:51.247: INFO: Deleting ReplicationController wrapped-volume-race-6d4b8b64-640f-449c-a359-08d77dc99083 took: 20.320132ms
Mar  5 08:30:53.648: INFO: Terminating ReplicationController wrapped-volume-race-6d4b8b64-640f-449c-a359-08d77dc99083 pods took: 2.400552792s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:31:03.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1263" for this suite.

â€¢ [SLOW TEST:81.350 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":277,"completed":166,"skipped":2997,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:31:03.338: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1333
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:31:14.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1333" for this suite.

â€¢ [SLOW TEST:11.368 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":277,"completed":167,"skipped":3005,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:31:14.707: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2326
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's args
Mar  5 08:31:15.298: INFO: Waiting up to 5m0s for pod "var-expansion-c515c28c-0204-47bd-baca-9032e2e716de" in namespace "var-expansion-2326" to be "Succeeded or Failed"
Mar  5 08:31:15.306: INFO: Pod "var-expansion-c515c28c-0204-47bd-baca-9032e2e716de": Phase="Pending", Reason="", readiness=false. Elapsed: 7.409554ms
Mar  5 08:31:17.317: INFO: Pod "var-expansion-c515c28c-0204-47bd-baca-9032e2e716de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019041622s
Mar  5 08:31:19.328: INFO: Pod "var-expansion-c515c28c-0204-47bd-baca-9032e2e716de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02978721s
STEP: Saw pod success
Mar  5 08:31:19.328: INFO: Pod "var-expansion-c515c28c-0204-47bd-baca-9032e2e716de" satisfied condition "Succeeded or Failed"
Mar  5 08:31:19.348: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod var-expansion-c515c28c-0204-47bd-baca-9032e2e716de container dapi-container: <nil>
STEP: delete the pod
Mar  5 08:31:19.400: INFO: Waiting for pod var-expansion-c515c28c-0204-47bd-baca-9032e2e716de to disappear
Mar  5 08:31:19.409: INFO: Pod var-expansion-c515c28c-0204-47bd-baca-9032e2e716de no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:31:19.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2326" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":277,"completed":168,"skipped":3016,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:31:19.438: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7461
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  5 08:31:19.666: INFO: Waiting up to 5m0s for pod "pod-7a3666dc-93dc-4332-9320-4b8c96277175" in namespace "emptydir-7461" to be "Succeeded or Failed"
Mar  5 08:31:19.671: INFO: Pod "pod-7a3666dc-93dc-4332-9320-4b8c96277175": Phase="Pending", Reason="", readiness=false. Elapsed: 4.172231ms
Mar  5 08:31:21.679: INFO: Pod "pod-7a3666dc-93dc-4332-9320-4b8c96277175": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012107133s
Mar  5 08:31:23.687: INFO: Pod "pod-7a3666dc-93dc-4332-9320-4b8c96277175": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020070729s
STEP: Saw pod success
Mar  5 08:31:23.687: INFO: Pod "pod-7a3666dc-93dc-4332-9320-4b8c96277175" satisfied condition "Succeeded or Failed"
Mar  5 08:31:23.693: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-7a3666dc-93dc-4332-9320-4b8c96277175 container test-container: <nil>
STEP: delete the pod
Mar  5 08:31:23.744: INFO: Waiting for pod pod-7a3666dc-93dc-4332-9320-4b8c96277175 to disappear
Mar  5 08:31:23.753: INFO: Pod pod-7a3666dc-93dc-4332-9320-4b8c96277175 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:31:23.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7461" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":169,"skipped":3021,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:31:23.806: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4628
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:31:24.045: INFO: Creating ReplicaSet my-hostname-basic-fc0e1ac8-7842-402f-8804-79f4fd125823
Mar  5 08:31:24.067: INFO: Pod name my-hostname-basic-fc0e1ac8-7842-402f-8804-79f4fd125823: Found 0 pods out of 1
Mar  5 08:31:29.076: INFO: Pod name my-hostname-basic-fc0e1ac8-7842-402f-8804-79f4fd125823: Found 1 pods out of 1
Mar  5 08:31:29.076: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-fc0e1ac8-7842-402f-8804-79f4fd125823" is running
Mar  5 08:31:29.082: INFO: Pod "my-hostname-basic-fc0e1ac8-7842-402f-8804-79f4fd125823-lwgj5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-05 08:31:24 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-05 08:31:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-05 08:31:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-05 08:31:24 +0000 UTC Reason: Message:}])
Mar  5 08:31:29.083: INFO: Trying to dial the pod
Mar  5 08:31:34.111: INFO: Controller my-hostname-basic-fc0e1ac8-7842-402f-8804-79f4fd125823: Got expected result from replica 1 [my-hostname-basic-fc0e1ac8-7842-402f-8804-79f4fd125823-lwgj5]: "my-hostname-basic-fc0e1ac8-7842-402f-8804-79f4fd125823-lwgj5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:31:34.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4628" for this suite.

â€¢ [SLOW TEST:10.329 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":277,"completed":170,"skipped":3062,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:31:34.135: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7075
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl replace
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar  5 08:31:34.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-7075'
Mar  5 08:31:34.492: INFO: stderr: ""
Mar  5 08:31:34.493: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar  5 08:31:39.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pod e2e-test-httpd-pod --namespace=kubectl-7075 -o json'
Mar  5 08:31:39.919: INFO: stderr: ""
Mar  5 08:31:39.919: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.244.3.230/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.3.230/32\"\n        },\n        \"creationTimestamp\": \"2021-03-05T08:31:34Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-05T08:31:34Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-05T08:31:35Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.244.3.230\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-05T08:31:36Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7075\",\n        \"resourceVersion\": \"5635396\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7075/pods/e2e-test-httpd-pod\",\n        \"uid\": \"4dc8b4e3-51d4-4114-b2bf-5dc3ddebcff3\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-5zf7x\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"devops-pool1-6c76d44df9-8tgs6\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-5zf7x\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-5zf7x\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-05T08:31:34Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-05T08:31:36Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-05T08:31:36Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-05T08:31:34Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://396392aa0e16a94bbca4405ae6ef019de89ebd56828705580643a89f95094254\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-03-05T08:31:35Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.0.8\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.3.230\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.3.230\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-03-05T08:31:34Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar  5 08:31:39.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 replace -f - --namespace=kubectl-7075'
Mar  5 08:31:40.494: INFO: stderr: ""
Mar  5 08:31:40.494: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Mar  5 08:31:40.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 delete pods e2e-test-httpd-pod --namespace=kubectl-7075'
Mar  5 08:31:53.671: INFO: stderr: ""
Mar  5 08:31:53.671: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:31:53.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7075" for this suite.

â€¢ [SLOW TEST:19.558 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1450
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":277,"completed":171,"skipped":3098,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:31:53.694: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5699
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar  5 08:32:04.046: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0305 08:32:04.046455      26 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:32:04.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5699" for this suite.

â€¢ [SLOW TEST:10.383 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":277,"completed":172,"skipped":3117,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:32:04.078: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2832
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 08:32:04.303: INFO: Waiting up to 5m0s for pod "downwardapi-volume-241eec6b-1bfd-4e3f-a16b-241c2705a49d" in namespace "projected-2832" to be "Succeeded or Failed"
Mar  5 08:32:04.310: INFO: Pod "downwardapi-volume-241eec6b-1bfd-4e3f-a16b-241c2705a49d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.101033ms
Mar  5 08:32:06.326: INFO: Pod "downwardapi-volume-241eec6b-1bfd-4e3f-a16b-241c2705a49d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022748483s
STEP: Saw pod success
Mar  5 08:32:06.326: INFO: Pod "downwardapi-volume-241eec6b-1bfd-4e3f-a16b-241c2705a49d" satisfied condition "Succeeded or Failed"
Mar  5 08:32:06.345: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-241eec6b-1bfd-4e3f-a16b-241c2705a49d container client-container: <nil>
STEP: delete the pod
Mar  5 08:32:06.450: INFO: Waiting for pod downwardapi-volume-241eec6b-1bfd-4e3f-a16b-241c2705a49d to disappear
Mar  5 08:32:06.463: INFO: Pod downwardapi-volume-241eec6b-1bfd-4e3f-a16b-241c2705a49d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:32:06.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2832" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":277,"completed":173,"skipped":3134,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:32:06.484: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2499
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:32:06.715: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:32:07.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2499" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":277,"completed":174,"skipped":3149,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:32:07.782: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9193
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with configMap that has name projected-configmap-test-upd-583e7a55-307c-45e5-8c23-8c7fd40274e4
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-583e7a55-307c-45e5-8c23-8c7fd40274e4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:32:13.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9193" for this suite.

â€¢ [SLOW TEST:5.291 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":175,"skipped":3151,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:32:13.074: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3865
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name projected-secret-test-06109bd1-3adc-4164-8680-475764aceb98
STEP: Creating a pod to test consume secrets
Mar  5 08:32:13.301: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8c40d23f-dce3-4a25-b1a7-bf89797cae16" in namespace "projected-3865" to be "Succeeded or Failed"
Mar  5 08:32:13.310: INFO: Pod "pod-projected-secrets-8c40d23f-dce3-4a25-b1a7-bf89797cae16": Phase="Pending", Reason="", readiness=false. Elapsed: 9.390702ms
Mar  5 08:32:15.321: INFO: Pod "pod-projected-secrets-8c40d23f-dce3-4a25-b1a7-bf89797cae16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019833551s
STEP: Saw pod success
Mar  5 08:32:15.321: INFO: Pod "pod-projected-secrets-8c40d23f-dce3-4a25-b1a7-bf89797cae16" satisfied condition "Succeeded or Failed"
Mar  5 08:32:15.327: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-projected-secrets-8c40d23f-dce3-4a25-b1a7-bf89797cae16 container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 08:32:15.374: INFO: Waiting for pod pod-projected-secrets-8c40d23f-dce3-4a25-b1a7-bf89797cae16 to disappear
Mar  5 08:32:15.381: INFO: Pod pod-projected-secrets-8c40d23f-dce3-4a25-b1a7-bf89797cae16 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:32:15.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3865" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":277,"completed":176,"skipped":3157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:32:15.448: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 08:32:15.984: INFO: Waiting up to 5m0s for pod "downwardapi-volume-413ab7a5-df59-4971-87c2-a32d9331023a" in namespace "downward-api-8416" to be "Succeeded or Failed"
Mar  5 08:32:15.997: INFO: Pod "downwardapi-volume-413ab7a5-df59-4971-87c2-a32d9331023a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.653175ms
Mar  5 08:32:18.006: INFO: Pod "downwardapi-volume-413ab7a5-df59-4971-87c2-a32d9331023a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022624076s
STEP: Saw pod success
Mar  5 08:32:18.006: INFO: Pod "downwardapi-volume-413ab7a5-df59-4971-87c2-a32d9331023a" satisfied condition "Succeeded or Failed"
Mar  5 08:32:18.015: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-413ab7a5-df59-4971-87c2-a32d9331023a container client-container: <nil>
STEP: delete the pod
Mar  5 08:32:18.075: INFO: Waiting for pod downwardapi-volume-413ab7a5-df59-4971-87c2-a32d9331023a to disappear
Mar  5 08:32:18.085: INFO: Pod downwardapi-volume-413ab7a5-df59-4971-87c2-a32d9331023a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:32:18.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8416" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":277,"completed":177,"skipped":3185,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:32:18.114: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-9952
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:32:18.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-9952" for this suite.
â€¢{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":277,"completed":178,"skipped":3202,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:32:18.480: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7163
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:32:18.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7163" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":277,"completed":179,"skipped":3203,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:32:18.772: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2831
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar  5 08:32:18.952: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  5 08:32:18.980: INFO: Waiting for terminating namespaces to be deleted...
Mar  5 08:32:18.988: INFO: 
Logging pods the kubelet thinks is on node devops-control-plane-1 before test
Mar  5 08:32:19.046: INFO: harbor-da5q24-harbor-database-0 from pipeline-tools started at 2021-03-04 10:01:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container database ready: true, restart count 1
Mar  5 08:32:19.046: INFO: kube-proxy-v54pf from kube-system started at 2021-02-23 18:20:31 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:32:19.046: INFO: node-local-dns-27gsv from kube-system started at 2021-02-23 18:22:38 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:32:19.046: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-4n6zm from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:32:19.046: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:32:19.046: INFO: ratings-v1-675894856b-tjjdl from serv-mesh-ex started at 2021-03-01 08:46:17 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:32:19.046: INFO: 	Container ratings ready: true, restart count 1
Mar  5 08:32:19.046: INFO: enginsgungor-tutorial-deployment-late-e4m-3f096c9b3046-jobgsk6b from tutorial-s2i started at 2021-03-02 18:05:58 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:32:19.046: INFO: alertmanager-main-0 from kubesphere-monitoring-system started at 2021-03-05 08:23:17 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container alertmanager ready: true, restart count 0
Mar  5 08:32:19.046: INFO: 	Container config-reloader ready: true, restart count 0
Mar  5 08:32:19.046: INFO: rook-ceph-crashcollector-devops-control-plane-1-66d7bd67c8zzdhg from rook-ceph started at 2021-02-23 18:35:33 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:32:19.046: INFO: kube-scheduler-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container kube-scheduler ready: true, restart count 10
Mar  5 08:32:19.046: INFO: canal-v67nk from kube-system started at 2021-02-23 18:23:29 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:32:19.046: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:32:19.046: INFO: kube-controller-manager-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container kube-controller-manager ready: true, restart count 14
Mar  5 08:32:19.046: INFO: redis-ha-server-2 from kubesphere-system started at 2021-02-24 13:03:31 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container redis ready: true, restart count 1
Mar  5 08:32:19.046: INFO: 	Container sentinel ready: true, restart count 1
Mar  5 08:32:19.046: INFO: fluent-bit-6z2kz from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:32:19.046: INFO: rook-ceph-osd-prepare-devops-control-plane-1-c2ck4 from rook-ceph started at 2021-03-05 08:21:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container provision ready: false, restart count 0
Mar  5 08:32:19.046: INFO: node-exporter-grhwd from kubesphere-monitoring-system started at 2021-03-04 17:34:37 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:32:19.046: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:32:19.046: INFO: etcd-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container etcd ready: true, restart count 1
Mar  5 08:32:19.046: INFO: rook-ceph-osd-2-7d8c68d866-pljjp from rook-ceph started at 2021-02-23 18:35:33 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container osd ready: true, restart count 1
Mar  5 08:32:19.046: INFO: thanos-ruler-k8s-0 from kubesphere-monitoring-system started at 2021-02-24 13:23:29 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container rules-configmap-reloader ready: true, restart count 1
Mar  5 08:32:19.046: INFO: 	Container thanos-ruler ready: true, restart count 1
Mar  5 08:32:19.046: INFO: s2ioperator-0 from kubesphere-devops-system started at 2021-03-05 08:23:16 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container manager ready: true, restart count 0
Mar  5 08:32:19.046: INFO: kube-apiserver-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar  5 08:32:19.046: INFO: csi-cephfsplugin-provisioner-c68f789b8-2rdqm from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (6 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container csi-attacher ready: true, restart count 10
Mar  5 08:32:19.046: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:32:19.046: INFO: 	Container csi-provisioner ready: true, restart count 13
Mar  5 08:32:19.046: INFO: 	Container csi-resizer ready: true, restart count 11
Mar  5 08:32:19.046: INFO: 	Container csi-snapshotter ready: true, restart count 19
Mar  5 08:32:19.046: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:32:19.046: INFO: istiod-1-6-10-7d869d7f58-tg6zc from istio-system started at 2021-03-01 07:43:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container discovery ready: true, restart count 1
Mar  5 08:32:19.046: INFO: rook-ceph-mon-d-c99989cc-lrjlz from rook-ceph started at 2021-03-04 08:21:35 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container mon ready: true, restart count 1
Mar  5 08:32:19.046: INFO: harbor-da5q24-harbor-jobservice-b55d6b698-8zwxd from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container jobservice ready: true, restart count 2
Mar  5 08:32:19.046: INFO: csi-cephfsplugin-8sbwl from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:32:19.046: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:32:19.046: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:32:19.046: INFO: redis-ha-haproxy-5c6559d588-b2n68 from kubesphere-system started at 2021-03-04 08:11:54 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container haproxy ready: true, restart count 2
Mar  5 08:32:19.046: INFO: csi-rbdplugin-l7fcg from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:32:19.046: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:32:19.046: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:32:19.046: INFO: details-v1-dc74fc894-m9kzk from serv-mesh-ex started at 2021-03-01 08:46:17 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container details ready: true, restart count 1
Mar  5 08:32:19.046: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:32:19.046: INFO: harbor-da5q24-harbor-notary-signer-6785bf5b4-6w2dq from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.046: INFO: 	Container notary-signer ready: true, restart count 2
Mar  5 08:32:19.046: INFO: 
Logging pods the kubelet thinks is on node devops-control-plane-2 before test
Mar  5 08:32:19.142: INFO: etcd-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:23 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container etcd ready: true, restart count 1
Mar  5 08:32:19.142: INFO: csi-rbdplugin-4hzbs from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:32:19.142: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:32:19.142: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:32:19.142: INFO: openldap-1 from kubesphere-system started at 2021-02-24 13:04:13 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container openldap-ha ready: true, restart count 1
Mar  5 08:32:19.142: INFO: ks-installer-74b46bd68d-q6nrh from kubesphere-system started at 2021-03-04 08:15:54 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container installer ready: true, restart count 1
Mar  5 08:32:19.142: INFO: kube-controller-manager-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container kube-controller-manager ready: true, restart count 7
Mar  5 08:32:19.142: INFO: fluent-bit-grhml from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:32:19.142: INFO: ks-events-ruler-698b7899c7-9qnjp from kubesphere-logging-system started at 2021-02-24 13:22:32 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:32:19.142: INFO: 	Container events-ruler ready: true, restart count 1
Mar  5 08:32:19.142: INFO: jaeger-operator-c78679c9f-r9pbh from istio-system started at 2021-03-01 07:44:49 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container jaeger-operator ready: true, restart count 2
Mar  5 08:32:19.142: INFO: harbor-da5q24-harbor-core-67659b6b55-4mcwv from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container core ready: true, restart count 2
Mar  5 08:32:19.142: INFO: harbor-da5q24-harbor-notary-server-7fc788c84b-bdpf9 from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container notary-server ready: true, restart count 3
Mar  5 08:32:19.142: INFO: harbor-da5q24-harbor-chartmuseum-874964bb8-7dqmz from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container chartmuseum ready: true, restart count 1
Mar  5 08:32:19.142: INFO: csi-cephfsplugin-l8vgz from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:32:19.142: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:32:19.142: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:32:19.142: INFO: ks-console-fb7f5895-rxb4f from kubesphere-system started at 2021-02-24 13:04:07 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container ks-console ready: true, restart count 1
Mar  5 08:32:19.142: INFO: harbor-da5q24-harbor-registry-56df8d46bf-2s6ct from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container registry ready: true, restart count 1
Mar  5 08:32:19.142: INFO: 	Container registryctl ready: true, restart count 1
Mar  5 08:32:19.142: INFO: gogs-gogs-test-http from proteus started at 2021-03-03 07:35:46 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container wget ready: false, restart count 0
Mar  5 08:32:19.142: INFO: canal-hctnd from kube-system started at 2021-02-23 18:23:44 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:32:19.142: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:32:19.142: INFO: kube-auditing-webhook-deploy-b74bfb885-5cnp5 from kubesphere-logging-system started at 2021-02-24 13:34:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container kube-auditing-webhook ready: true, restart count 1
Mar  5 08:32:19.142: INFO: harbor-da5q24-harbor-clair-5dd889dd4f-4nclv from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container adapter ready: true, restart count 1
Mar  5 08:32:19.142: INFO: 	Container clair ready: true, restart count 5
Mar  5 08:32:19.142: INFO: jaeger-es-index-cleaner-1614815700-7s2kc from istio-system started at 2021-03-03 23:55:05 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container jaeger-es-index-cleaner ready: false, restart count 0
Mar  5 08:32:19.142: INFO: ks-apiserver-5894746f6b-gprr9 from kubesphere-system started at 2021-03-04 18:53:58 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container ks-apiserver ready: true, restart count 0
Mar  5 08:32:19.142: INFO: coredns-6cf46fccfd-vgfsb from kube-system started at 2021-02-23 18:23:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container coredns ready: true, restart count 1
Mar  5 08:32:19.142: INFO: kube-apiserver-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar  5 08:32:19.142: INFO: jaeger-collector-7887b45bf-n8cbz from istio-system started at 2021-03-01 07:45:14 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container jaeger-collector ready: true, restart count 3
Mar  5 08:32:19.142: INFO: elasticsearch-logging-curator-elasticsearch-curator-161473657d4 from kubesphere-logging-system started at 2021-03-03 01:00:02 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Mar  5 08:32:19.142: INFO: thanos-ruler-kubesphere-0 from kubesphere-monitoring-system started at 2021-03-04 07:58:11 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container rules-configmap-reloader ready: true, restart count 1
Mar  5 08:32:19.142: INFO: 	Container thanos-ruler ready: true, restart count 1
Mar  5 08:32:19.142: INFO: redis-ha-server-1 from kubesphere-system started at 2021-02-24 13:03:23 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container redis ready: true, restart count 1
Mar  5 08:32:19.142: INFO: 	Container sentinel ready: true, restart count 1
Mar  5 08:32:19.142: INFO: harbor-da5q24-harbor-nginx-687f459b8d-drvcb from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container nginx ready: true, restart count 2
Mar  5 08:32:19.142: INFO: kube-proxy-vkr26 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:32:19.142: INFO: coredns-6cf46fccfd-lbcwc from kube-system started at 2021-02-23 18:23:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container coredns ready: true, restart count 1
Mar  5 08:32:19.142: INFO: hcloud-cloud-controller-manager-7c5d46cc54-rhlq7 from kube-system started at 2021-02-23 18:23:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container hcloud-cloud-controller-manager ready: true, restart count 2
Mar  5 08:32:19.142: INFO: ks-events-operator-8dbf7fccc-v8zdm from kubesphere-logging-system started at 2021-02-24 13:22:25 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container events-operator ready: true, restart count 1
Mar  5 08:32:19.142: INFO: node-local-dns-4wxxm from kube-system started at 2021-02-23 18:22:39 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:32:19.142: INFO: reviews-v1-549f6b9d47-rttzb from serv-mesh-ex started at 2021-03-01 10:23:35 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:32:19.142: INFO: 	Container reviews ready: true, restart count 1
Mar  5 08:32:19.142: INFO: ks-controller-manager-8556448648-dk5xc from kubesphere-system started at 2021-03-04 18:53:58 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container ks-controller-manager ready: true, restart count 0
Mar  5 08:32:19.142: INFO: rook-ceph-crashcollector-devops-control-plane-2-6686b8d74fqx7lt from rook-ceph started at 2021-02-23 18:34:51 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:32:19.142: INFO: rook-ceph-osd-0-855b585564-645vx from rook-ceph started at 2021-02-23 18:34:51 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container osd ready: true, restart count 1
Mar  5 08:32:19.142: INFO: redis-ha-haproxy-5c6559d588-l22bp from kubesphere-system started at 2021-02-24 13:03:15 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container haproxy ready: true, restart count 2
Mar  5 08:32:19.142: INFO: jaeger-es-index-cleaner-1614729300-p7kmh from istio-system started at 2021-03-02 23:55:03 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container jaeger-es-index-cleaner ready: false, restart count 0
Mar  5 08:32:19.142: INFO: logsidecar-injector-deploy-74c66bfd85-vpkvb from kubesphere-logging-system started at 2021-02-24 13:22:19 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container config-reloader ready: true, restart count 2
Mar  5 08:32:19.142: INFO: 	Container logsidecar-injector ready: true, restart count 2
Mar  5 08:32:19.142: INFO: minio-image-st-668b878c77-4rwrb from pipeline-tools started at 2021-03-03 17:33:04 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container minio ready: true, restart count 1
Mar  5 08:32:19.142: INFO: harbor-da5q24-harbor-trivy-0 from pipeline-tools started at 2021-03-04 10:01:13 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container trivy ready: true, restart count 1
Mar  5 08:32:19.142: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-5bzp2 from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:32:19.142: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:32:19.142: INFO: rook-ceph-osd-prepare-devops-control-plane-2-x2r69 from rook-ceph started at 2021-03-05 08:21:02 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container provision ready: false, restart count 0
Mar  5 08:32:19.142: INFO: kube-scheduler-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container kube-scheduler ready: true, restart count 8
Mar  5 08:32:19.142: INFO: etcd-65796969c7-mr9hs from kubesphere-system started at 2021-02-24 13:19:44 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container etcd ready: true, restart count 1
Mar  5 08:32:19.142: INFO: notification-deployment-65547747f7-zxr25 from kubesphere-alerting-system started at 2021-02-24 13:45:42 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container notification ready: true, restart count 1
Mar  5 08:32:19.142: INFO: harbor-da5q24-harbor-portal-84fb8bdb7f-nttwj from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container portal ready: true, restart count 1
Mar  5 08:32:19.142: INFO: node-exporter-cvhfl from kubesphere-monitoring-system started at 2021-03-04 17:34:19 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.142: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:32:19.142: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:32:19.142: INFO: 
Logging pods the kubelet thinks is on node devops-control-plane-3 before test
Mar  5 08:32:19.192: INFO: jaeger-query-b546558bf-frn98 from istio-system started at 2021-03-01 07:45:14 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container jaeger-agent ready: true, restart count 1
Mar  5 08:32:19.192: INFO: 	Container jaeger-query ready: true, restart count 1
Mar  5 08:32:19.192: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2021-03-04 17:34:29 +0000 UTC (3 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container prometheus ready: true, restart count 1
Mar  5 08:32:19.192: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  5 08:32:19.192: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  5 08:32:19.192: INFO: machine-controller-webhook-747c599b5c-k7xw4 from kube-system started at 2021-02-23 18:23:36 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container machine-controller-webhook ready: true, restart count 1
Mar  5 08:32:19.192: INFO: notification-manager-deployment-7c8df68d94-5dv72 from kubesphere-monitoring-system started at 2021-02-24 13:05:51 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container notification-manager ready: true, restart count 1
Mar  5 08:32:19.192: INFO: redis-ha-server-0 from kubesphere-system started at 2021-02-24 13:03:18 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container redis ready: true, restart count 1
Mar  5 08:32:19.192: INFO: 	Container sentinel ready: true, restart count 1
Mar  5 08:32:19.192: INFO: fluent-bit-s284n from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:32:19.192: INFO: kube-auditing-operator-6ddc8db4b-vfxqc from kubesphere-logging-system started at 2021-02-24 13:34:03 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container kube-auditing-operator ready: true, restart count 1
Mar  5 08:32:19.192: INFO: notification-db-init-job-mqs2r from kubesphere-alerting-system started at 2021-03-03 17:34:05 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container notification-db-init ready: false, restart count 0
Mar  5 08:32:19.192: INFO: notification-db-ctrl-job-7kzd5 from kubesphere-alerting-system started at 2021-03-03 17:34:15 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container notification-db-ctrl ready: false, restart count 0
Mar  5 08:32:19.192: INFO: without-dockerfile-v1-7dc7678956-mh5vw from serv-mesh-ex started at 2021-03-04 08:15:54 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container container-2a87d6 ready: true, restart count 1
Mar  5 08:32:19.192: INFO: node-local-dns-twq6t from kube-system started at 2021-02-23 18:22:38 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:32:19.192: INFO: rook-ceph-crashcollector-devops-control-plane-3-587c5dc7b8mt27z from rook-ceph started at 2021-02-23 18:34:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:32:19.192: INFO: logsidecar-injector-deploy-74c66bfd85-62xbg from kubesphere-logging-system started at 2021-02-24 13:22:19 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:32:19.192: INFO: 	Container logsidecar-injector ready: true, restart count 1
Mar  5 08:32:19.192: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-wp8ts from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:32:19.192: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:32:19.192: INFO: kiali-operator-58b8765b9c-6qh7r from istio-system started at 2021-03-01 07:44:57 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container operator ready: true, restart count 1
Mar  5 08:32:19.192: INFO: etcd-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:36 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container etcd ready: true, restart count 1
Mar  5 08:32:19.192: INFO: redis-ha-haproxy-5c6559d588-qhqhx from kubesphere-system started at 2021-02-24 13:03:15 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container haproxy ready: true, restart count 1
Mar  5 08:32:19.192: INFO: node-exporter-6fsk8 from kubesphere-monitoring-system started at 2021-03-04 17:34:28 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:32:19.192: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:32:19.192: INFO: rook-ceph-osd-1-845f4b4486-5phnd from rook-ceph started at 2021-02-23 18:34:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container osd ready: true, restart count 1
Mar  5 08:32:19.192: INFO: openldap-0 from kubesphere-system started at 2021-02-24 13:03:27 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container openldap-ha ready: true, restart count 1
Mar  5 08:32:19.192: INFO: gogs-gogs-7467fdcf8f-rw58p from default started at 2021-03-05 08:23:09 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container gogs ready: true, restart count 0
Mar  5 08:32:19.192: INFO: kube-apiserver-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar  5 08:32:19.192: INFO: csi-rbdplugin-7zr2t from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:32:19.192: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:32:19.192: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:32:19.192: INFO: ks-events-ruler-698b7899c7-rjqqv from kubesphere-logging-system started at 2021-02-24 13:22:32 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:32:19.192: INFO: 	Container events-ruler ready: true, restart count 1
Mar  5 08:32:19.192: INFO: calico-kube-controllers-589975f454-whrcl from kube-system started at 2021-02-23 18:23:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.192: INFO: 	Container calico-kube-controllers ready: true, restart count 1
Mar  5 08:32:19.192: INFO: canal-r57rn from kube-system started at 2021-02-23 18:23:39 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.193: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:32:19.193: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:32:19.193: INFO: thanos-ruler-kubesphere-1 from kubesphere-monitoring-system started at 2021-03-04 07:58:11 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.193: INFO: 	Container rules-configmap-reloader ready: true, restart count 1
Mar  5 08:32:19.193: INFO: 	Container thanos-ruler ready: true, restart count 1
Mar  5 08:32:19.193: INFO: ks-apiserver-5894746f6b-8p8vg from kubesphere-system started at 2021-03-04 18:54:02 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.193: INFO: 	Container ks-apiserver ready: true, restart count 0
Mar  5 08:32:19.193: INFO: rook-ceph-osd-prepare-devops-control-plane-3-q6vvm from rook-ceph started at 2021-03-05 08:21:05 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.193: INFO: 	Container provision ready: false, restart count 0
Mar  5 08:32:19.193: INFO: machine-controller-74bbdbb8b8-7r4nj from kube-system started at 2021-02-23 18:23:35 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.193: INFO: 	Container machine-controller ready: true, restart count 27
Mar  5 08:32:19.193: INFO: minio-7bfdb5968b-cnd55 from kubesphere-system started at 2021-02-24 14:10:33 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.193: INFO: 	Container minio ready: true, restart count 1
Mar  5 08:32:19.193: INFO: ks-console-fb7f5895-5pmpn from kubesphere-system started at 2021-02-24 13:04:07 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.193: INFO: 	Container ks-console ready: true, restart count 1
Mar  5 08:32:19.193: INFO: csi-rbdplugin-provisioner-5bc97cdbb9-zjbws from rook-ceph started at 2021-03-04 08:15:54 +0000 UTC (6 container statuses recorded)
Mar  5 08:32:19.193: INFO: 	Container csi-attacher ready: true, restart count 1
Mar  5 08:32:19.193: INFO: 	Container csi-provisioner ready: true, restart count 1
Mar  5 08:32:19.193: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:32:19.193: INFO: 	Container csi-resizer ready: true, restart count 1
Mar  5 08:32:19.193: INFO: 	Container csi-snapshotter ready: true, restart count 1
Mar  5 08:32:19.193: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:32:19.193: INFO: gogs-gogs-test-ssh from proteus started at 2021-03-03 07:35:46 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.193: INFO: 	Container wget ready: false, restart count 0
Mar  5 08:32:19.193: INFO: kube-proxy-nrg4j from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.193: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:32:19.193: INFO: csi-cephfsplugin-287cb from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:32:19.193: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:32:19.193: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:32:19.193: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:32:19.193: INFO: fluentbit-operator-7c56d66dd-7b4df from kubesphere-logging-system started at 2021-02-24 13:20:01 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.193: INFO: 	Container fluentbit-operator ready: true, restart count 2
Mar  5 08:32:19.193: INFO: istio-ingressgateway-76df6567c6-tndjv from istio-system started at 2021-03-01 07:44:17 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.193: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:32:19.193: INFO: ks-controller-manager-8556448648-c4spd from kubesphere-system started at 2021-03-04 18:54:03 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.193: INFO: 	Container ks-controller-manager ready: true, restart count 0
Mar  5 08:32:19.193: INFO: kube-controller-manager-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.193: INFO: 	Container kube-controller-manager ready: true, restart count 11
Mar  5 08:32:19.193: INFO: kube-scheduler-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.193: INFO: 	Container kube-scheduler ready: true, restart count 13
Mar  5 08:32:19.193: INFO: 
Logging pods the kubelet thinks is on node devops-pool1-6c76d44df9-8tgs6 before test
Mar  5 08:32:19.212: INFO: kube-proxy-jxx7v from kube-system started at 2021-02-23 18:25:59 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.212: INFO: 	Container kube-proxy ready: true, restart count 8
Mar  5 08:32:19.212: INFO: fluent-bit-2vc49 from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.212: INFO: 	Container fluent-bit ready: true, restart count 8
Mar  5 08:32:19.212: INFO: csi-cephfsplugin-crjw8 from rook-ceph started at 2021-03-05 08:23:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:32:19.212: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar  5 08:32:19.213: INFO: 	Container driver-registrar ready: true, restart count 0
Mar  5 08:32:19.213: INFO: 	Container liveness-prometheus ready: true, restart count 0
Mar  5 08:32:19.213: INFO: elasticsearch-logging-data-2 from kubesphere-logging-system started at 2021-03-05 08:25:23 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.213: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  5 08:32:19.213: INFO: csi-rbdplugin-7hr88 from rook-ceph started at 2021-03-05 08:23:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:32:19.213: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar  5 08:32:19.213: INFO: 	Container driver-registrar ready: true, restart count 0
Mar  5 08:32:19.213: INFO: 	Container liveness-prometheus ready: true, restart count 0
Mar  5 08:32:19.213: INFO: gogs-postgresql-0 from default started at 2021-03-05 08:23:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.213: INFO: 	Container gogs-postgresql ready: true, restart count 0
Mar  5 08:32:19.213: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-9ffgc from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.213: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:32:19.213: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:32:19.213: INFO: canal-rm2l9 from kube-system started at 2021-02-23 18:25:59 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.213: INFO: 	Container calico-node ready: true, restart count 8
Mar  5 08:32:19.213: INFO: 	Container kube-flannel ready: true, restart count 8
Mar  5 08:32:19.213: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2021-03-05 08:23:53 +0000 UTC (3 container statuses recorded)
Mar  5 08:32:19.213: INFO: 	Container prometheus ready: true, restart count 1
Mar  5 08:32:19.213: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  5 08:32:19.213: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  5 08:32:19.213: INFO: elasticsearch-logging-discovery-1 from kubesphere-logging-system started at 2021-03-05 08:23:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.213: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  5 08:32:19.213: INFO: sonobuoy from sonobuoy started at 2021-03-05 07:46:37 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.213: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  5 08:32:19.213: INFO: node-local-dns-kh572 from kube-system started at 2021-02-23 18:25:59 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.213: INFO: 	Container node-cache ready: true, restart count 8
Mar  5 08:32:19.213: INFO: node-exporter-824lt from kubesphere-monitoring-system started at 2021-03-04 17:34:42 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.213: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:32:19.213: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:32:19.213: INFO: pod-projected-configmaps-2b0ff675-547e-482d-80c1-c0c3d9bd37bc from projected-9193 started at 2021-03-05 08:32:08 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.213: INFO: 	Container projected-configmap-volume-test ready: true, restart count 0
Mar  5 08:32:19.213: INFO: 
Logging pods the kubelet thinks is on node devops-pool1-6c76d44df9-dwtv7 before test
Mar  5 08:32:19.257: INFO: rook-ceph-mgr-a-55fb9d48b6-clq94 from rook-ceph started at 2021-02-23 18:34:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.258: INFO: 	Container mgr ready: true, restart count 2
Mar  5 08:32:19.258: INFO: ks-console-fb7f5895-vvxbg from kubesphere-system started at 2021-02-25 07:43:56 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.258: INFO: 	Container ks-console ready: true, restart count 1
Mar  5 08:32:19.258: INFO: tutorial-deployment-v1-5549b8b9d8-t8t9z from tutorial-s2i started at 2021-03-02 18:10:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.258: INFO: 	Container container-t1u32h ready: true, restart count 1
Mar  5 08:32:19.258: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-f7ttl from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.258: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:32:19.258: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:32:19.258: INFO: snapshot-controller-0 from kube-system started at 2021-03-05 08:23:14 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.258: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  5 08:32:19.258: INFO: kube-proxy-2czsl from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.258: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:32:19.258: INFO: csi-cephfsplugin-provisioner-c68f789b8-k89fp from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (6 container statuses recorded)
Mar  5 08:32:19.258: INFO: 	Container csi-attacher ready: true, restart count 14
Mar  5 08:32:19.258: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:32:19.258: INFO: 	Container csi-provisioner ready: true, restart count 14
Mar  5 08:32:19.258: INFO: 	Container csi-resizer ready: true, restart count 12
Mar  5 08:32:19.258: INFO: 	Container csi-snapshotter ready: true, restart count 13
Mar  5 08:32:19.258: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:32:19.258: INFO: ks-apiserver-5894746f6b-2dq28 from kubesphere-system started at 2021-03-04 18:53:54 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.258: INFO: 	Container ks-apiserver ready: true, restart count 0
Mar  5 08:32:19.258: INFO: thanos-ruler-k8s-1 from kubesphere-monitoring-system started at 2021-03-05 08:23:19 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.258: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  5 08:32:19.258: INFO: 	Container thanos-ruler ready: true, restart count 0
Mar  5 08:32:19.258: INFO: rook-ceph-mon-a-85cd5968bc-hq6sb from rook-ceph started at 2021-02-23 18:33:25 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.258: INFO: 	Container mon ready: true, restart count 1
Mar  5 08:32:19.258: INFO: alertmanager-main-1 from kubesphere-monitoring-system started at 2021-02-24 13:05:28 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.258: INFO: 	Container alertmanager ready: true, restart count 1
Mar  5 08:32:19.258: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:32:19.258: INFO: elasticsearch-logging-discovery-2 from kubesphere-logging-system started at 2021-02-24 13:21:07 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.258: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 08:32:19.258: INFO: canal-mfdhb from kube-system started at 2021-02-23 18:26:00 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.258: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:32:19.258: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:32:19.259: INFO: node-local-dns-8s8tk from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.259: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:32:19.260: INFO: rook-ceph-crashcollector-devops-pool1-6c76d44df9-dwtv7-7dfdvvgh from rook-ceph started at 2021-02-23 18:34:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.260: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:32:19.260: INFO: ks-sample-dev-7bb944b987-hnlc4 from kubesphere-sample-dev started at 2021-03-03 11:03:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.260: INFO: 	Container js-sample ready: true, restart count 1
Mar  5 08:32:19.260: INFO: notification-manager-operator-6958786cd6-26xxr from kubesphere-monitoring-system started at 2021-03-04 08:15:54 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.260: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Mar  5 08:32:19.260: INFO: 	Container notification-manager-operator ready: true, restart count 2
Mar  5 08:32:19.260: INFO: openpitrix-hyperpitrix-deployment-6d48d87c6c-z7vww from openpitrix-system started at 2021-03-05 08:23:09 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.260: INFO: 	Container hyperpitrix ready: true, restart count 0
Mar  5 08:32:19.260: INFO: rook-ceph-operator-6fb9f456fc-8g9z2 from rook-ceph started at 2021-02-23 18:29:52 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.260: INFO: 	Container rook-ceph-operator ready: true, restart count 1
Mar  5 08:32:19.260: INFO: csi-rbdplugin-nhvm9 from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:32:19.260: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:32:19.260: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:32:19.260: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:32:19.260: INFO: enginsgungor-hello-deployment-latest--0dq-14b4f67d0b7e-jobptskv from serv-mesh-ex started at 2021-03-01 19:28:59 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.260: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:32:19.260: INFO: harbor-da5q24-harbor-redis-0 from pipeline-tools started at 2021-03-04 10:01:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.260: INFO: 	Container redis ready: true, restart count 1
Mar  5 08:32:19.260: INFO: kube-auditing-webhook-deploy-b74bfb885-qf48j from kubesphere-logging-system started at 2021-02-24 13:34:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.260: INFO: 	Container kube-auditing-webhook ready: true, restart count 1
Mar  5 08:32:19.260: INFO: kiali-5fc8bfd66b-wjrqr from istio-system started at 2021-03-01 07:45:58 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.260: INFO: 	Container kiali ready: true, restart count 1
Mar  5 08:32:19.260: INFO: enginsgungor-hello-deployment-latest--6eg-3afd31aa2f90-jobgl2pn from serv-mesh-ex started at 2021-03-01 19:15:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.260: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:32:19.260: INFO: enginsgungor-tutorial-deployment-late-9pz-042c363e1509-job598vw from tutorial-s2i started at 2021-03-02 18:10:13 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.260: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:32:19.260: INFO: csi-cephfsplugin-nxcmz from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:32:19.260: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:32:19.260: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:32:19.260: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:32:19.260: INFO: fluent-bit-s42x9 from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.260: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:32:19.260: INFO: hyperpitrix-release-app-job-tqln9 from openpitrix-system started at 2021-03-04 18:51:28 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.261: INFO: 	Container hyperpitrix-release-app-job ready: false, restart count 0
Mar  5 08:32:19.261: INFO: elasticsearch-logging-curator-elasticsearch-curator-161481wvfqt from kubesphere-logging-system started at 2021-03-04 01:00:01 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.261: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Mar  5 08:32:19.261: INFO: node-exporter-f6kh5 from kubesphere-monitoring-system started at 2021-03-04 17:34:32 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.261: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:32:19.261: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:32:19.261: INFO: sonobuoy-e2e-job-41e333abf2c348c1 from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.261: INFO: 	Container e2e ready: true, restart count 0
Mar  5 08:32:19.261: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:32:19.261: INFO: ks-sample-dev-5c5d97c6cc-64hxc from kubesphere-offline-dev started at 2021-03-05 08:23:09 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.261: INFO: 	Container js-sample ready: true, restart count 0
Mar  5 08:32:19.261: INFO: elasticsearch-logging-data-1 from kubesphere-logging-system started at 2021-02-24 13:20:42 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.261: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 08:32:19.261: INFO: ks-controller-manager-8556448648-cbt26 from kubesphere-system started at 2021-03-04 18:53:55 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.261: INFO: 	Container ks-controller-manager ready: true, restart count 1
Mar  5 08:32:19.261: INFO: enginsgungor-hello-world-latest-ug1-ejy-f941edb3103d-job-pr5nr from serv-mesh-ex started at 2021-03-01 18:50:41 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.261: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:32:19.261: INFO: 
Logging pods the kubelet thinks is on node devops-pool1-6c76d44df9-njb8n before test
Mar  5 08:32:19.312: INFO: enginsgungor-hello-deployment-latest--fsk-bb68efbac665-jobm9zxh from serv-mesh-ex started at 2021-03-02 06:09:25 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:32:19.312: INFO: node-exporter-fhq4b from kubesphere-monitoring-system started at 2021-03-04 17:34:24 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:32:19.312: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:32:19.312: INFO: rook-ceph-mon-c-764d6d7649-wcbn5 from rook-ceph started at 2021-02-23 18:33:46 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container mon ready: true, restart count 1
Mar  5 08:32:19.312: INFO: elasticsearch-logging-discovery-0 from kubesphere-logging-system started at 2021-02-24 13:19:50 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 08:32:19.312: INFO: fluent-bit-cqbf6 from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:32:19.312: INFO: ks-jenkins-5bf5cbf449-tzqjz from kubesphere-devops-system started at 2021-02-28 09:58:29 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container ks-jenkins ready: true, restart count 1
Mar  5 08:32:19.312: INFO: enginsgungor-hello-deployment-latest--cnc-39caa4291750-jobgnx27 from serv-mesh-ex started at 2021-03-01 19:24:09 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:32:19.312: INFO: productpage-v1-795bd6db76-trctn from serv-mesh-ex started at 2021-03-04 08:15:54 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:32:19.312: INFO: 	Container productpage ready: true, restart count 1
Mar  5 08:32:19.312: INFO: alertmanager-main-2 from kubesphere-monitoring-system started at 2021-02-24 13:05:28 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container alertmanager ready: true, restart count 1
Mar  5 08:32:19.312: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:32:19.312: INFO: ks-events-exporter-5bc4d9f496-zm5tx from kubesphere-logging-system started at 2021-02-24 13:22:33 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:32:19.312: INFO: 	Container events-exporter ready: true, restart count 1
Mar  5 08:32:19.312: INFO: kubesphere-router-serv-mesh-ex-585558c74b-45mf8 from kubesphere-controls-system started at 2021-03-01 08:45:33 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:32:19.312: INFO: 	Container nginx-ingress-controller ready: true, restart count 1
Mar  5 08:32:19.312: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-xprqf from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:32:19.312: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:32:19.312: INFO: csi-rbdplugin-provisioner-5bc97cdbb9-nlb2z from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (6 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container csi-attacher ready: true, restart count 21
Mar  5 08:32:19.312: INFO: 	Container csi-provisioner ready: true, restart count 12
Mar  5 08:32:19.312: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:32:19.312: INFO: 	Container csi-resizer ready: true, restart count 11
Mar  5 08:32:19.312: INFO: 	Container csi-snapshotter ready: true, restart count 13
Mar  5 08:32:19.312: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:32:19.312: INFO: kube-proxy-bpngj from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:32:19.312: INFO: default-http-backend-857d7b6856-gt6hh from kubesphere-controls-system started at 2021-02-24 13:03:47 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container default-http-backend ready: true, restart count 1
Mar  5 08:32:19.312: INFO: node-local-dns-5qx5m from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:32:19.312: INFO: rook-ceph-crashcollector-devops-pool1-6c76d44df9-njb8n-cfbd6xbn from rook-ceph started at 2021-02-23 18:33:46 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:32:19.312: INFO: prometheus-operator-84d58bf775-q2drc from kubesphere-monitoring-system started at 2021-02-24 13:05:18 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Mar  5 08:32:19.312: INFO: 	Container prometheus-operator ready: true, restart count 1
Mar  5 08:32:19.312: INFO: notification-manager-deployment-7c8df68d94-7vd2s from kubesphere-monitoring-system started at 2021-02-24 13:05:51 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container notification-manager ready: true, restart count 1
Mar  5 08:32:19.312: INFO: mysql-7f64d9f584-49fnb from kubesphere-system started at 2021-02-24 13:19:43 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container mysql ready: true, restart count 1
Mar  5 08:32:19.312: INFO: rook-ceph-tools-64bd84c8b5-hd7mk from rook-ceph started at 2021-02-25 07:43:01 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container rook-ceph-tools ready: true, restart count 1
Mar  5 08:32:19.312: INFO: csi-cephfsplugin-t7l65 from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:32:19.312: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:32:19.312: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:32:19.312: INFO: elasticsearch-logging-data-0 from kubesphere-logging-system started at 2021-02-24 13:19:49 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 08:32:19.312: INFO: canal-tp2fn from kube-system started at 2021-02-23 18:26:00 +0000 UTC (2 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:32:19.312: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:32:19.312: INFO: kube-state-metrics-95c974544-drf59 from kubesphere-monitoring-system started at 2021-02-24 13:05:20 +0000 UTC (3 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 1
Mar  5 08:32:19.312: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 1
Mar  5 08:32:19.312: INFO: 	Container kube-state-metrics ready: true, restart count 1
Mar  5 08:32:19.312: INFO: hellojs-v1-5f4d98c7df-snl82 from serv-mesh-ex started at 2021-03-02 06:09:42 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container container-3nysic ready: true, restart count 1
Mar  5 08:32:19.312: INFO: csi-rbdplugin-5n26w from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:32:19.312: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:32:19.312: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:32:19.312: INFO: kubectl-admin-5d98567c78-m72x5 from kubesphere-controls-system started at 2021-02-24 13:06:05 +0000 UTC (1 container statuses recorded)
Mar  5 08:32:19.312: INFO: 	Container kubectl ready: true, restart count 1
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-46ad90a9-72e1-4beb-8ea1-ae70b1205670 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-46ad90a9-72e1-4beb-8ea1-ae70b1205670 off the node devops-pool1-6c76d44df9-8tgs6
STEP: verifying the node doesn't have the label kubernetes.io/e2e-46ad90a9-72e1-4beb-8ea1-ae70b1205670
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:32:27.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2831" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

â€¢ [SLOW TEST:8.812 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":277,"completed":180,"skipped":3226,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:32:27.585: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-5379
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  5 08:32:31.922: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5379 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:32:31.922: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:32:32.132: INFO: Exec stderr: ""
Mar  5 08:32:32.133: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5379 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:32:32.133: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:32:32.321: INFO: Exec stderr: ""
Mar  5 08:32:32.321: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5379 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:32:32.321: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:32:32.511: INFO: Exec stderr: ""
Mar  5 08:32:32.511: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5379 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:32:32.511: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:32:32.744: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  5 08:32:32.744: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5379 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:32:32.744: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:32:32.957: INFO: Exec stderr: ""
Mar  5 08:32:32.957: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5379 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:32:32.957: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:32:33.152: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  5 08:32:33.153: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5379 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:32:33.153: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:32:33.333: INFO: Exec stderr: ""
Mar  5 08:32:33.333: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5379 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:32:33.333: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:32:33.553: INFO: Exec stderr: ""
Mar  5 08:32:33.553: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5379 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:32:33.553: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:32:33.744: INFO: Exec stderr: ""
Mar  5 08:32:33.744: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5379 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:32:33.744: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:32:33.910: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:32:33.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5379" for this suite.

â€¢ [SLOW TEST:6.349 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":181,"skipped":3236,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:32:33.935: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2658
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  5 08:32:34.155: INFO: Waiting up to 5m0s for pod "pod-81b07e52-e054-4ca7-82f5-6276921e8101" in namespace "emptydir-2658" to be "Succeeded or Failed"
Mar  5 08:32:34.166: INFO: Pod "pod-81b07e52-e054-4ca7-82f5-6276921e8101": Phase="Pending", Reason="", readiness=false. Elapsed: 10.312733ms
Mar  5 08:32:36.172: INFO: Pod "pod-81b07e52-e054-4ca7-82f5-6276921e8101": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016718777s
STEP: Saw pod success
Mar  5 08:32:36.173: INFO: Pod "pod-81b07e52-e054-4ca7-82f5-6276921e8101" satisfied condition "Succeeded or Failed"
Mar  5 08:32:36.179: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-81b07e52-e054-4ca7-82f5-6276921e8101 container test-container: <nil>
STEP: delete the pod
Mar  5 08:32:36.229: INFO: Waiting for pod pod-81b07e52-e054-4ca7-82f5-6276921e8101 to disappear
Mar  5 08:32:36.235: INFO: Pod pod-81b07e52-e054-4ca7-82f5-6276921e8101 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:32:36.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2658" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":182,"skipped":3253,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:32:36.257: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2005
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-tgmm
STEP: Creating a pod to test atomic-volume-subpath
Mar  5 08:32:36.479: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tgmm" in namespace "subpath-2005" to be "Succeeded or Failed"
Mar  5 08:32:36.487: INFO: Pod "pod-subpath-test-configmap-tgmm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.558909ms
Mar  5 08:32:38.493: INFO: Pod "pod-subpath-test-configmap-tgmm": Phase="Running", Reason="", readiness=true. Elapsed: 2.012805603s
Mar  5 08:32:40.505: INFO: Pod "pod-subpath-test-configmap-tgmm": Phase="Running", Reason="", readiness=true. Elapsed: 4.024997232s
Mar  5 08:32:42.513: INFO: Pod "pod-subpath-test-configmap-tgmm": Phase="Running", Reason="", readiness=true. Elapsed: 6.032701007s
Mar  5 08:32:44.520: INFO: Pod "pod-subpath-test-configmap-tgmm": Phase="Running", Reason="", readiness=true. Elapsed: 8.040250456s
Mar  5 08:32:46.529: INFO: Pod "pod-subpath-test-configmap-tgmm": Phase="Running", Reason="", readiness=true. Elapsed: 10.04944102s
Mar  5 08:32:48.537: INFO: Pod "pod-subpath-test-configmap-tgmm": Phase="Running", Reason="", readiness=true. Elapsed: 12.057032235s
Mar  5 08:32:50.545: INFO: Pod "pod-subpath-test-configmap-tgmm": Phase="Running", Reason="", readiness=true. Elapsed: 14.065169579s
Mar  5 08:32:52.554: INFO: Pod "pod-subpath-test-configmap-tgmm": Phase="Running", Reason="", readiness=true. Elapsed: 16.074167784s
Mar  5 08:32:54.563: INFO: Pod "pod-subpath-test-configmap-tgmm": Phase="Running", Reason="", readiness=true. Elapsed: 18.082797393s
Mar  5 08:32:56.570: INFO: Pod "pod-subpath-test-configmap-tgmm": Phase="Running", Reason="", readiness=true. Elapsed: 20.090294155s
Mar  5 08:32:58.590: INFO: Pod "pod-subpath-test-configmap-tgmm": Phase="Running", Reason="", readiness=true. Elapsed: 22.109517675s
Mar  5 08:33:00.606: INFO: Pod "pod-subpath-test-configmap-tgmm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.125596658s
STEP: Saw pod success
Mar  5 08:33:00.606: INFO: Pod "pod-subpath-test-configmap-tgmm" satisfied condition "Succeeded or Failed"
Mar  5 08:33:00.620: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-subpath-test-configmap-tgmm container test-container-subpath-configmap-tgmm: <nil>
STEP: delete the pod
Mar  5 08:33:00.699: INFO: Waiting for pod pod-subpath-test-configmap-tgmm to disappear
Mar  5 08:33:00.702: INFO: Pod pod-subpath-test-configmap-tgmm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tgmm
Mar  5 08:33:00.702: INFO: Deleting pod "pod-subpath-test-configmap-tgmm" in namespace "subpath-2005"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:33:00.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2005" for this suite.

â€¢ [SLOW TEST:24.480 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":277,"completed":183,"skipped":3262,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:33:00.737: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3816
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  5 08:33:00.929: INFO: Waiting up to 5m0s for pod "pod-7fdaf5ad-703d-40c0-b6e1-fb930b3714ba" in namespace "emptydir-3816" to be "Succeeded or Failed"
Mar  5 08:33:00.934: INFO: Pod "pod-7fdaf5ad-703d-40c0-b6e1-fb930b3714ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.869209ms
Mar  5 08:33:02.943: INFO: Pod "pod-7fdaf5ad-703d-40c0-b6e1-fb930b3714ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013635751s
Mar  5 08:33:04.948: INFO: Pod "pod-7fdaf5ad-703d-40c0-b6e1-fb930b3714ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01880087s
STEP: Saw pod success
Mar  5 08:33:04.948: INFO: Pod "pod-7fdaf5ad-703d-40c0-b6e1-fb930b3714ba" satisfied condition "Succeeded or Failed"
Mar  5 08:33:04.952: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-7fdaf5ad-703d-40c0-b6e1-fb930b3714ba container test-container: <nil>
STEP: delete the pod
Mar  5 08:33:05.000: INFO: Waiting for pod pod-7fdaf5ad-703d-40c0-b6e1-fb930b3714ba to disappear
Mar  5 08:33:05.008: INFO: Pod pod-7fdaf5ad-703d-40c0-b6e1-fb930b3714ba no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:33:05.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3816" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":184,"skipped":3269,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:33:05.035: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl label
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1206
STEP: creating the pod
Mar  5 08:33:05.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 create -f - --namespace=kubectl-7433'
Mar  5 08:33:05.826: INFO: stderr: ""
Mar  5 08:33:05.826: INFO: stdout: "pod/pause created\n"
Mar  5 08:33:05.826: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  5 08:33:05.826: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7433" to be "running and ready"
Mar  5 08:33:05.833: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.717371ms
Mar  5 08:33:07.841: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.014408184s
Mar  5 08:33:07.841: INFO: Pod "pause" satisfied condition "running and ready"
Mar  5 08:33:07.841: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: adding the label testing-label with value testing-label-value to a pod
Mar  5 08:33:07.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 label pods pause testing-label=testing-label-value --namespace=kubectl-7433'
Mar  5 08:33:07.972: INFO: stderr: ""
Mar  5 08:33:07.972: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar  5 08:33:07.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pod pause -L testing-label --namespace=kubectl-7433'
Mar  5 08:33:08.123: INFO: stderr: ""
Mar  5 08:33:08.123: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar  5 08:33:08.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 label pods pause testing-label- --namespace=kubectl-7433'
Mar  5 08:33:08.269: INFO: stderr: ""
Mar  5 08:33:08.269: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar  5 08:33:08.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pod pause -L testing-label --namespace=kubectl-7433'
Mar  5 08:33:08.420: INFO: stderr: ""
Mar  5 08:33:08.420: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1213
STEP: using delete to clean up resources
Mar  5 08:33:08.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 delete --grace-period=0 --force -f - --namespace=kubectl-7433'
Mar  5 08:33:08.591: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 08:33:08.592: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  5 08:33:08.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get rc,svc -l name=pause --no-headers --namespace=kubectl-7433'
Mar  5 08:33:08.732: INFO: stderr: "No resources found in kubectl-7433 namespace.\n"
Mar  5 08:33:08.732: INFO: stdout: ""
Mar  5 08:33:08.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods -l name=pause --namespace=kubectl-7433 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  5 08:33:08.854: INFO: stderr: ""
Mar  5 08:33:08.854: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:33:08.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7433" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":277,"completed":185,"skipped":3272,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:33:08.876: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-313
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar  5 08:33:09.081: INFO: Waiting up to 5m0s for pod "pod-c19046e1-f78d-4e4b-91b3-57042051ba7e" in namespace "emptydir-313" to be "Succeeded or Failed"
Mar  5 08:33:09.086: INFO: Pod "pod-c19046e1-f78d-4e4b-91b3-57042051ba7e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.171485ms
Mar  5 08:33:11.094: INFO: Pod "pod-c19046e1-f78d-4e4b-91b3-57042051ba7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013094617s
STEP: Saw pod success
Mar  5 08:33:11.095: INFO: Pod "pod-c19046e1-f78d-4e4b-91b3-57042051ba7e" satisfied condition "Succeeded or Failed"
Mar  5 08:33:11.101: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-c19046e1-f78d-4e4b-91b3-57042051ba7e container test-container: <nil>
STEP: delete the pod
Mar  5 08:33:11.158: INFO: Waiting for pod pod-c19046e1-f78d-4e4b-91b3-57042051ba7e to disappear
Mar  5 08:33:11.167: INFO: Pod pod-c19046e1-f78d-4e4b-91b3-57042051ba7e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:33:11.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-313" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":186,"skipped":3278,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:33:11.192: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5300
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:33:11.406: INFO: Creating deployment "test-recreate-deployment"
Mar  5 08:33:11.415: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  5 08:33:11.427: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar  5 08:33:13.445: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  5 08:33:13.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529991, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529991, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529991, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750529991, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 08:33:15.463: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  5 08:33:15.481: INFO: Updating deployment test-recreate-deployment
Mar  5 08:33:15.481: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar  5 08:33:15.650: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5300 /apis/apps/v1/namespaces/deployment-5300/deployments/test-recreate-deployment 4391e83f-9dae-4573-a3e1-32350207e310 5636940 2 2021-03-05 08:33:11 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-05 08:33:15 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2021-03-05 08:33:15 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0031d00d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-03-05 08:33:15 +0000 UTC,LastTransitionTime:2021-03-05 08:33:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-d5667d9c7" is progressing.,LastUpdateTime:2021-03-05 08:33:15 +0000 UTC,LastTransitionTime:2021-03-05 08:33:11 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar  5 08:33:15.661: INFO: New ReplicaSet "test-recreate-deployment-d5667d9c7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-d5667d9c7  deployment-5300 /apis/apps/v1/namespaces/deployment-5300/replicasets/test-recreate-deployment-d5667d9c7 87ff377e-e0fb-4159-aecb-51b1ac01a5ea 5636937 1 2021-03-05 08:33:15 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4391e83f-9dae-4573-a3e1-32350207e310 0xc0031d0610 0xc0031d0611}] []  [{kube-controller-manager Update apps/v1 2021-03-05 08:33:15 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 51 57 49 101 56 51 102 45 57 100 97 101 45 52 53 55 51 45 97 51 101 49 45 51 50 51 53 48 50 48 55 101 51 49 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: d5667d9c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0031d0688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  5 08:33:15.661: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  5 08:33:15.661: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-74d98b5f7c  deployment-5300 /apis/apps/v1/namespaces/deployment-5300/replicasets/test-recreate-deployment-74d98b5f7c 25f67d98-f405-4ccd-85db-7d3436041038 5636928 2 2021-03-05 08:33:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4391e83f-9dae-4573-a3e1-32350207e310 0xc0031d0517 0xc0031d0518}] []  [{kube-controller-manager Update apps/v1 2021-03-05 08:33:15 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 51 57 49 101 56 51 102 45 57 100 97 101 45 52 53 55 51 45 97 51 101 49 45 51 50 51 53 48 50 48 55 101 51 49 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 74d98b5f7c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0031d05a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  5 08:33:15.667: INFO: Pod "test-recreate-deployment-d5667d9c7-md4r7" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-d5667d9c7-md4r7 test-recreate-deployment-d5667d9c7- deployment-5300 /api/v1/namespaces/deployment-5300/pods/test-recreate-deployment-d5667d9c7-md4r7 d1a7b06c-490d-42b9-8546-cfa550acf19e 5636939 0 2021-03-05 08:33:15 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [{apps/v1 ReplicaSet test-recreate-deployment-d5667d9c7 87ff377e-e0fb-4159-aecb-51b1ac01a5ea 0xc0031d0b70 0xc0031d0b71}] []  [{kube-controller-manager Update v1 2021-03-05 08:33:15 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 56 55 102 102 51 55 55 101 45 101 48 102 98 45 52 49 53 57 45 97 101 99 98 45 53 49 98 49 97 99 48 49 97 53 101 97 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2021-03-05 08:33:15 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-svtgl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-svtgl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-svtgl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-8tgs6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 08:33:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 08:33:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 08:33:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 08:33:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.8,PodIP:,StartTime:2021-03-05 08:33:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:33:15.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5300" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":277,"completed":187,"skipped":3288,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:33:15.698: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6580
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:33:17.963: INFO: Waiting up to 5m0s for pod "client-envvars-f9373551-a084-4aa3-8d22-86a2a6c8ee40" in namespace "pods-6580" to be "Succeeded or Failed"
Mar  5 08:33:17.974: INFO: Pod "client-envvars-f9373551-a084-4aa3-8d22-86a2a6c8ee40": Phase="Pending", Reason="", readiness=false. Elapsed: 10.422217ms
Mar  5 08:33:19.983: INFO: Pod "client-envvars-f9373551-a084-4aa3-8d22-86a2a6c8ee40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019316585s
STEP: Saw pod success
Mar  5 08:33:19.983: INFO: Pod "client-envvars-f9373551-a084-4aa3-8d22-86a2a6c8ee40" satisfied condition "Succeeded or Failed"
Mar  5 08:33:19.990: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod client-envvars-f9373551-a084-4aa3-8d22-86a2a6c8ee40 container env3cont: <nil>
STEP: delete the pod
Mar  5 08:33:20.054: INFO: Waiting for pod client-envvars-f9373551-a084-4aa3-8d22-86a2a6c8ee40 to disappear
Mar  5 08:33:20.062: INFO: Pod client-envvars-f9373551-a084-4aa3-8d22-86a2a6c8ee40 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:33:20.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6580" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":277,"completed":188,"skipped":3292,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:33:20.086: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7672
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-339c2038-aa46-497c-b81a-95f6219b0e71 in namespace container-probe-7672
Mar  5 08:33:22.511: INFO: Started pod liveness-339c2038-aa46-497c-b81a-95f6219b0e71 in namespace container-probe-7672
STEP: checking the pod's current state and verifying that restartCount is present
Mar  5 08:33:22.518: INFO: Initial restart count of pod liveness-339c2038-aa46-497c-b81a-95f6219b0e71 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:37:23.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7672" for this suite.

â€¢ [SLOW TEST:243.808 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":277,"completed":189,"skipped":3311,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:37:23.895: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3192
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 08:37:24.167: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de99e663-6930-41d3-a573-172d41bc08fb" in namespace "downward-api-3192" to be "Succeeded or Failed"
Mar  5 08:37:24.175: INFO: Pod "downwardapi-volume-de99e663-6930-41d3-a573-172d41bc08fb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.045616ms
Mar  5 08:37:26.182: INFO: Pod "downwardapi-volume-de99e663-6930-41d3-a573-172d41bc08fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014570931s
STEP: Saw pod success
Mar  5 08:37:26.182: INFO: Pod "downwardapi-volume-de99e663-6930-41d3-a573-172d41bc08fb" satisfied condition "Succeeded or Failed"
Mar  5 08:37:26.204: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-de99e663-6930-41d3-a573-172d41bc08fb container client-container: <nil>
STEP: delete the pod
Mar  5 08:37:26.285: INFO: Waiting for pod downwardapi-volume-de99e663-6930-41d3-a573-172d41bc08fb to disappear
Mar  5 08:37:26.296: INFO: Pod downwardapi-volume-de99e663-6930-41d3-a573-172d41bc08fb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:37:26.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3192" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":190,"skipped":3314,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:37:26.325: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6235
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:37:26.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 create -f - --namespace=kubectl-6235'
Mar  5 08:37:26.960: INFO: stderr: ""
Mar  5 08:37:26.961: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Mar  5 08:37:26.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 create -f - --namespace=kubectl-6235'
Mar  5 08:37:27.376: INFO: stderr: ""
Mar  5 08:37:27.376: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar  5 08:37:28.401: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  5 08:37:28.401: INFO: Found 0 / 1
Mar  5 08:37:29.384: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  5 08:37:29.384: INFO: Found 1 / 1
Mar  5 08:37:29.384: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  5 08:37:29.389: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  5 08:37:29.389: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  5 08:37:29.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 describe pod agnhost-master-hb69n --namespace=kubectl-6235'
Mar  5 08:37:29.596: INFO: stderr: ""
Mar  5 08:37:29.596: INFO: stdout: "Name:         agnhost-master-hb69n\nNamespace:    kubectl-6235\nPriority:     0\nNode:         devops-pool1-6c76d44df9-8tgs6/192.168.0.8\nStart Time:   Fri, 05 Mar 2021 08:37:27 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 10.244.3.9/32\n              cni.projectcalico.org/podIPs: 10.244.3.9/32\nStatus:       Running\nIP:           10.244.3.9\nIPs:\n  IP:           10.244.3.9\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://6e281c3901a03e236837bc30e940c3470bfde6bf2e54ab011135952221899e8f\n    Image:          us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Image ID:       docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 05 Mar 2021 08:37:28 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-g7qjs (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-g7qjs:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-g7qjs\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                    Message\n  ----    ------     ----       ----                                    -------\n  Normal  Scheduled  <unknown>  default-scheduler                       Successfully assigned kubectl-6235/agnhost-master-hb69n to devops-pool1-6c76d44df9-8tgs6\n  Normal  Pulled     2s         kubelet, devops-pool1-6c76d44df9-8tgs6  Container image \"us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\" already present on machine\n  Normal  Created    2s         kubelet, devops-pool1-6c76d44df9-8tgs6  Created container agnhost-master\n  Normal  Started    1s         kubelet, devops-pool1-6c76d44df9-8tgs6  Started container agnhost-master\n"
Mar  5 08:37:29.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 describe rc agnhost-master --namespace=kubectl-6235'
Mar  5 08:37:29.780: INFO: stderr: ""
Mar  5 08:37:29.780: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-6235\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-master-hb69n\n"
Mar  5 08:37:29.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 describe service agnhost-master --namespace=kubectl-6235'
Mar  5 08:37:29.931: INFO: stderr: ""
Mar  5 08:37:29.931: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-6235\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.96.114.82\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.3.9:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  5 08:37:29.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 describe node devops-control-plane-1'
Mar  5 08:37:30.323: INFO: stderr: ""
Mar  5 08:37:30.323: INFO: stdout: "Name:               devops-control-plane-1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=cx41\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=nbg1\n                    failure-domain.beta.kubernetes.io/zone=nbg1-dc3\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=devops-control-plane-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=cx41\n                    topology.kubernetes.io/region=nbg1\n                    topology.kubernetes.io/zone=nbg1-dc3\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.0.3\n                    csi.volume.kubernetes.io/nodeid: {\"rook-ceph.rbd.csi.ceph.com\":\"devops-control-plane-1\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"0a:f7:58:fc:d4:12\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.0.3\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.0.3/32\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 23 Feb 2021 18:20:21 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  devops-control-plane-1\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 05 Mar 2021 08:37:25 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 04 Mar 2021 11:35:40 +0000   Thu, 04 Mar 2021 11:35:40 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Fri, 05 Mar 2021 08:37:27 +0000   Thu, 25 Feb 2021 07:45:13 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 05 Mar 2021 08:37:27 +0000   Thu, 25 Feb 2021 07:45:13 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 05 Mar 2021 08:37:27 +0000   Thu, 25 Feb 2021 07:45:13 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 05 Mar 2021 08:37:27 +0000   Thu, 25 Feb 2021 07:45:13 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  Hostname:    devops-control-plane-1\n  ExternalIP:  116.203.78.99\n  InternalIP:  192.168.0.3\nCapacity:\n  cpu:                4\n  ephemeral-storage:  157365228Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16010964Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  145027793885\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15908564Ki\n  pods:               110\nSystem Info:\n  Machine ID:                   183718e4da5b46829ed15e70ae927972\n  System UUID:                  183718e4-da5b-4682-9ed1-5e70ae927972\n  Boot ID:                      582a449b-cdd9-47cc-8072-e1c9765c1581\n  Kernel Version:               5.4.0-54-generic\n  OS Image:                     Ubuntu 20.04.1 LTS\n  Operating System:             linux\n  Architecture:                 amd64\n  Container Runtime Version:    docker://19.3.14\n  Kubelet Version:              v1.18.6\n  Kube-Proxy Version:           v1.18.6\nPodCIDR:                        10.244.0.0/24\nPodCIDRs:                       10.244.0.0/24\nProviderID:                     hcloud://10226825\nNon-terminated Pods:            (27 in total)\n  Namespace                     Name                                                               CPU Requests  CPU Limits   Memory Requests  Memory Limits  AGE\n  ---------                     ----                                                               ------------  ----------   ---------------  -------------  ---\n  istio-system                  istiod-1-6-10-7d869d7f58-tg6zc                                     500m (12%)    0 (0%)       2Gi (13%)        0 (0%)         4d\n  kube-system                   canal-v67nk                                                        250m (6%)     0 (0%)       0 (0%)           0 (0%)         9d\n  kube-system                   etcd-devops-control-plane-1                                        0 (0%)        0 (0%)       0 (0%)           0 (0%)         9d\n  kube-system                   kube-apiserver-devops-control-plane-1                              250m (6%)     0 (0%)       0 (0%)           0 (0%)         9d\n  kube-system                   kube-controller-manager-devops-control-plane-1                     200m (5%)     0 (0%)       0 (0%)           0 (0%)         9d\n  kube-system                   kube-proxy-v54pf                                                   0 (0%)        0 (0%)       0 (0%)           0 (0%)         9d\n  kube-system                   kube-scheduler-devops-control-plane-1                              100m (2%)     0 (0%)       0 (0%)           0 (0%)         9d\n  kube-system                   node-local-dns-27gsv                                               25m (0%)      0 (0%)       5Mi (0%)         0 (0%)         9d\n  kubesphere-devops-system      s2ioperator-0                                                      100m (2%)     100m (2%)    20Mi (0%)        500Mi (3%)     14m\n  kubesphere-logging-system     fluent-bit-6z2kz                                                   0 (0%)        0 (0%)       0 (0%)           0 (0%)         8d\n  kubesphere-monitoring-system  alertmanager-main-0                                                20m (0%)      200m (5%)    55Mi (0%)        225Mi (1%)     14m\n  kubesphere-monitoring-system  node-exporter-grhwd                                                112m (2%)     1300m (32%)  200Mi (1%)       500Mi (3%)     15h\n  kubesphere-monitoring-system  thanos-ruler-k8s-0                                                 100m (2%)     1 (25%)      125Mi (0%)       1049Mi (6%)    8d\n  kubesphere-system             redis-ha-haproxy-5c6559d588-b2n68                                  0 (0%)        0 (0%)       0 (0%)           0 (0%)         24h\n  kubesphere-system             redis-ha-server-2                                                  0 (0%)        0 (0%)       0 (0%)           0 (0%)         8d\n  pipeline-tools                harbor-da5q24-harbor-database-0                                    0 (0%)        0 (0%)       0 (0%)           0 (0%)         22h\n  pipeline-tools                harbor-da5q24-harbor-jobservice-b55d6b698-8zwxd                    0 (0%)        0 (0%)       0 (0%)           0 (0%)         22h\n  pipeline-tools                harbor-da5q24-harbor-notary-signer-6785bf5b4-6w2dq                 0 (0%)        0 (0%)       0 (0%)           0 (0%)         22h\n  rook-ceph                     csi-cephfsplugin-8sbwl                                             0 (0%)        0 (0%)       0 (0%)           0 (0%)         9d\n  rook-ceph                     csi-cephfsplugin-provisioner-c68f789b8-2rdqm                       0 (0%)        0 (0%)       0 (0%)           0 (0%)         9d\n  rook-ceph                     csi-rbdplugin-l7fcg                                                0 (0%)        0 (0%)       0 (0%)           0 (0%)         9d\n  rook-ceph                     rook-ceph-crashcollector-devops-control-plane-1-66d7bd67c8zzdhg    0 (0%)        0 (0%)       0 (0%)           0 (0%)         9d\n  rook-ceph                     rook-ceph-mon-d-c99989cc-lrjlz                                     0 (0%)        0 (0%)       0 (0%)           0 (0%)         24h\n  rook-ceph                     rook-ceph-osd-2-7d8c68d866-pljjp                                   0 (0%)        0 (0%)       0 (0%)           0 (0%)         9d\n  serv-mesh-ex                  details-v1-dc74fc894-m9kzk                                         110m (2%)     3 (75%)      138Mi (0%)       2024Mi (13%)   3d23h\n  serv-mesh-ex                  ratings-v1-675894856b-tjjdl                                        110m (2%)     3 (75%)      138Mi (0%)       2024Mi (13%)   3d23h\n  sonobuoy                      sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-4n6zm            0 (0%)        0 (0%)       0 (0%)           0 (0%)         50m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                1877m (46%)   8600m (215%)\n  memory             2729Mi (17%)  6322Mi (40%)\n  ephemeral-storage  0 (0%)        0 (0%)\n  hugepages-1Gi      0 (0%)        0 (0%)\n  hugepages-2Mi      0 (0%)        0 (0%)\nEvents:              <none>\n"
Mar  5 08:37:30.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 describe namespace kubectl-6235'
Mar  5 08:37:30.477: INFO: stderr: ""
Mar  5 08:37:30.477: INFO: stdout: "Name:         kubectl-6235\nLabels:       e2e-framework=kubectl\n              e2e-run=60939dfd-5b1d-4aed-9f99-bbd14685f386\n              kubesphere.io/namespace=kubectl-6235\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:37:30.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6235" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":277,"completed":191,"skipped":3327,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:37:30.517: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6398
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6398 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6398;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6398 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6398;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6398.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6398.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6398.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6398.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6398.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6398.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6398.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6398.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6398.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6398.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6398.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6398.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6398.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 70.86.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.86.70_udp@PTR;check="$$(dig +tcp +noall +answer +search 70.86.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.86.70_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6398 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6398;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6398 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6398;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6398.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6398.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6398.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6398.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6398.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6398.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6398.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6398.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6398.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6398.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6398.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6398.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6398.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 70.86.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.86.70_udp@PTR;check="$$(dig +tcp +noall +answer +search 70.86.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.86.70_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  5 08:37:32.793: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:32.800: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:32.807: INFO: Unable to read wheezy_udp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:32.815: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:32.825: INFO: Unable to read wheezy_udp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:32.833: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:32.848: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:32.876: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:32.931: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:32.944: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:32.955: INFO: Unable to read jessie_udp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:32.965: INFO: Unable to read jessie_tcp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:32.972: INFO: Unable to read jessie_udp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:32.979: INFO: Unable to read jessie_tcp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:32.986: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:32.994: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:33.044: INFO: Lookups using dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6398 wheezy_tcp@dns-test-service.dns-6398 wheezy_udp@dns-test-service.dns-6398.svc wheezy_tcp@dns-test-service.dns-6398.svc wheezy_udp@_http._tcp.dns-test-service.dns-6398.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6398.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6398 jessie_tcp@dns-test-service.dns-6398 jessie_udp@dns-test-service.dns-6398.svc jessie_tcp@dns-test-service.dns-6398.svc jessie_udp@_http._tcp.dns-test-service.dns-6398.svc jessie_tcp@_http._tcp.dns-test-service.dns-6398.svc]

Mar  5 08:37:38.077: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:38.091: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:38.100: INFO: Unable to read wheezy_udp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:38.108: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:38.117: INFO: Unable to read wheezy_udp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:38.127: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:38.136: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:38.146: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:38.213: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:38.223: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:38.232: INFO: Unable to read jessie_udp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:38.240: INFO: Unable to read jessie_tcp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:38.248: INFO: Unable to read jessie_udp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:38.257: INFO: Unable to read jessie_tcp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:38.266: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:38.279: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:38.331: INFO: Lookups using dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6398 wheezy_tcp@dns-test-service.dns-6398 wheezy_udp@dns-test-service.dns-6398.svc wheezy_tcp@dns-test-service.dns-6398.svc wheezy_udp@_http._tcp.dns-test-service.dns-6398.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6398.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6398 jessie_tcp@dns-test-service.dns-6398 jessie_udp@dns-test-service.dns-6398.svc jessie_tcp@dns-test-service.dns-6398.svc jessie_udp@_http._tcp.dns-test-service.dns-6398.svc jessie_tcp@_http._tcp.dns-test-service.dns-6398.svc]

Mar  5 08:37:43.053: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:43.062: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:43.070: INFO: Unable to read wheezy_udp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:43.077: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:43.084: INFO: Unable to read wheezy_udp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:43.096: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:43.103: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:43.109: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:43.211: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:43.221: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:43.228: INFO: Unable to read jessie_udp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:43.235: INFO: Unable to read jessie_tcp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:43.243: INFO: Unable to read jessie_udp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:43.250: INFO: Unable to read jessie_tcp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:43.256: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:43.263: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:43.315: INFO: Lookups using dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6398 wheezy_tcp@dns-test-service.dns-6398 wheezy_udp@dns-test-service.dns-6398.svc wheezy_tcp@dns-test-service.dns-6398.svc wheezy_udp@_http._tcp.dns-test-service.dns-6398.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6398.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6398 jessie_tcp@dns-test-service.dns-6398 jessie_udp@dns-test-service.dns-6398.svc jessie_tcp@dns-test-service.dns-6398.svc jessie_udp@_http._tcp.dns-test-service.dns-6398.svc jessie_tcp@_http._tcp.dns-test-service.dns-6398.svc]

Mar  5 08:37:48.052: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:48.058: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:48.064: INFO: Unable to read wheezy_udp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:48.070: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:48.076: INFO: Unable to read wheezy_udp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:48.084: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:48.093: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:48.102: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:48.168: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:48.177: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:48.188: INFO: Unable to read jessie_udp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:48.196: INFO: Unable to read jessie_tcp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:48.203: INFO: Unable to read jessie_udp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:48.219: INFO: Unable to read jessie_tcp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:48.229: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:48.241: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:48.290: INFO: Lookups using dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6398 wheezy_tcp@dns-test-service.dns-6398 wheezy_udp@dns-test-service.dns-6398.svc wheezy_tcp@dns-test-service.dns-6398.svc wheezy_udp@_http._tcp.dns-test-service.dns-6398.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6398.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6398 jessie_tcp@dns-test-service.dns-6398 jessie_udp@dns-test-service.dns-6398.svc jessie_tcp@dns-test-service.dns-6398.svc jessie_udp@_http._tcp.dns-test-service.dns-6398.svc jessie_tcp@_http._tcp.dns-test-service.dns-6398.svc]

Mar  5 08:37:53.055: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:53.064: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:53.071: INFO: Unable to read wheezy_udp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:53.080: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:53.091: INFO: Unable to read wheezy_udp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:53.103: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:53.115: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:53.122: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:53.209: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:53.219: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:53.229: INFO: Unable to read jessie_udp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:53.248: INFO: Unable to read jessie_tcp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:53.256: INFO: Unable to read jessie_udp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:53.264: INFO: Unable to read jessie_tcp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:53.270: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:53.276: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:53.315: INFO: Lookups using dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6398 wheezy_tcp@dns-test-service.dns-6398 wheezy_udp@dns-test-service.dns-6398.svc wheezy_tcp@dns-test-service.dns-6398.svc wheezy_udp@_http._tcp.dns-test-service.dns-6398.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6398.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6398 jessie_tcp@dns-test-service.dns-6398 jessie_udp@dns-test-service.dns-6398.svc jessie_tcp@dns-test-service.dns-6398.svc jessie_udp@_http._tcp.dns-test-service.dns-6398.svc jessie_tcp@_http._tcp.dns-test-service.dns-6398.svc]

Mar  5 08:37:58.053: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:58.063: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:58.076: INFO: Unable to read wheezy_udp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:58.085: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:58.094: INFO: Unable to read wheezy_udp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:58.105: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:58.115: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:58.123: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:58.214: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:58.223: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:58.233: INFO: Unable to read jessie_udp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:58.249: INFO: Unable to read jessie_tcp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:58.256: INFO: Unable to read jessie_udp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:58.264: INFO: Unable to read jessie_tcp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:58.271: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:58.277: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:37:58.314: INFO: Lookups using dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6398 wheezy_tcp@dns-test-service.dns-6398 wheezy_udp@dns-test-service.dns-6398.svc wheezy_tcp@dns-test-service.dns-6398.svc wheezy_udp@_http._tcp.dns-test-service.dns-6398.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6398.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6398 jessie_tcp@dns-test-service.dns-6398 jessie_udp@dns-test-service.dns-6398.svc jessie_tcp@dns-test-service.dns-6398.svc jessie_udp@_http._tcp.dns-test-service.dns-6398.svc jessie_tcp@_http._tcp.dns-test-service.dns-6398.svc]

Mar  5 08:38:03.057: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:38:03.069: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:38:03.076: INFO: Unable to read wheezy_udp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:38:03.086: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:38:03.097: INFO: Unable to read wheezy_udp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:38:03.105: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:38:03.112: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:38:03.120: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:38:03.173: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:38:03.181: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:38:03.190: INFO: Unable to read jessie_udp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:38:03.199: INFO: Unable to read jessie_tcp@dns-test-service.dns-6398 from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:38:03.208: INFO: Unable to read jessie_udp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:38:03.216: INFO: Unable to read jessie_tcp@dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:38:03.225: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:38:03.232: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6398.svc from pod dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7: the server could not find the requested resource (get pods dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7)
Mar  5 08:38:03.278: INFO: Lookups using dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6398 wheezy_tcp@dns-test-service.dns-6398 wheezy_udp@dns-test-service.dns-6398.svc wheezy_tcp@dns-test-service.dns-6398.svc wheezy_udp@_http._tcp.dns-test-service.dns-6398.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6398.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6398 jessie_tcp@dns-test-service.dns-6398 jessie_udp@dns-test-service.dns-6398.svc jessie_tcp@dns-test-service.dns-6398.svc jessie_udp@_http._tcp.dns-test-service.dns-6398.svc jessie_tcp@_http._tcp.dns-test-service.dns-6398.svc]

Mar  5 08:38:08.298: INFO: DNS probes using dns-6398/dns-test-85699f6d-0191-4d3b-9939-a0ddf6ed82c7 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:38:08.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6398" for this suite.

â€¢ [SLOW TEST:37.952 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":277,"completed":192,"skipped":3387,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:38:08.471: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9176
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-downwardapi-wcmk
STEP: Creating a pod to test atomic-volume-subpath
Mar  5 08:38:08.710: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-wcmk" in namespace "subpath-9176" to be "Succeeded or Failed"
Mar  5 08:38:08.716: INFO: Pod "pod-subpath-test-downwardapi-wcmk": Phase="Pending", Reason="", readiness=false. Elapsed: 5.461961ms
Mar  5 08:38:10.727: INFO: Pod "pod-subpath-test-downwardapi-wcmk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016606009s
Mar  5 08:38:12.736: INFO: Pod "pod-subpath-test-downwardapi-wcmk": Phase="Running", Reason="", readiness=true. Elapsed: 4.025659578s
Mar  5 08:38:14.745: INFO: Pod "pod-subpath-test-downwardapi-wcmk": Phase="Running", Reason="", readiness=true. Elapsed: 6.034471129s
Mar  5 08:38:16.753: INFO: Pod "pod-subpath-test-downwardapi-wcmk": Phase="Running", Reason="", readiness=true. Elapsed: 8.042371523s
Mar  5 08:38:18.760: INFO: Pod "pod-subpath-test-downwardapi-wcmk": Phase="Running", Reason="", readiness=true. Elapsed: 10.049784804s
Mar  5 08:38:20.771: INFO: Pod "pod-subpath-test-downwardapi-wcmk": Phase="Running", Reason="", readiness=true. Elapsed: 12.060249513s
Mar  5 08:38:22.785: INFO: Pod "pod-subpath-test-downwardapi-wcmk": Phase="Running", Reason="", readiness=true. Elapsed: 14.074477598s
Mar  5 08:38:24.792: INFO: Pod "pod-subpath-test-downwardapi-wcmk": Phase="Running", Reason="", readiness=true. Elapsed: 16.081910023s
Mar  5 08:38:26.800: INFO: Pod "pod-subpath-test-downwardapi-wcmk": Phase="Running", Reason="", readiness=true. Elapsed: 18.090049315s
Mar  5 08:38:28.810: INFO: Pod "pod-subpath-test-downwardapi-wcmk": Phase="Running", Reason="", readiness=true. Elapsed: 20.099214591s
Mar  5 08:38:30.817: INFO: Pod "pod-subpath-test-downwardapi-wcmk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.106921975s
STEP: Saw pod success
Mar  5 08:38:30.817: INFO: Pod "pod-subpath-test-downwardapi-wcmk" satisfied condition "Succeeded or Failed"
Mar  5 08:38:30.822: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-subpath-test-downwardapi-wcmk container test-container-subpath-downwardapi-wcmk: <nil>
STEP: delete the pod
Mar  5 08:38:30.866: INFO: Waiting for pod pod-subpath-test-downwardapi-wcmk to disappear
Mar  5 08:38:30.873: INFO: Pod pod-subpath-test-downwardapi-wcmk no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-wcmk
Mar  5 08:38:30.873: INFO: Deleting pod "pod-subpath-test-downwardapi-wcmk" in namespace "subpath-9176"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:38:30.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9176" for this suite.

â€¢ [SLOW TEST:22.431 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":277,"completed":193,"skipped":3399,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:38:30.908: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-450
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:38:31.214: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar  5 08:38:31.228: INFO: Number of nodes with available pods: 0
Mar  5 08:38:31.228: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar  5 08:38:31.265: INFO: Number of nodes with available pods: 0
Mar  5 08:38:31.266: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 08:38:32.273: INFO: Number of nodes with available pods: 0
Mar  5 08:38:32.274: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 08:38:33.273: INFO: Number of nodes with available pods: 0
Mar  5 08:38:33.273: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 08:38:34.275: INFO: Number of nodes with available pods: 1
Mar  5 08:38:34.275: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar  5 08:38:34.315: INFO: Number of nodes with available pods: 1
Mar  5 08:38:34.315: INFO: Number of running nodes: 0, number of available pods: 1
Mar  5 08:38:35.323: INFO: Number of nodes with available pods: 0
Mar  5 08:38:35.323: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar  5 08:38:35.340: INFO: Number of nodes with available pods: 0
Mar  5 08:38:35.340: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 08:38:36.349: INFO: Number of nodes with available pods: 0
Mar  5 08:38:36.349: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 08:38:37.350: INFO: Number of nodes with available pods: 0
Mar  5 08:38:37.350: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 08:38:38.348: INFO: Number of nodes with available pods: 0
Mar  5 08:38:38.348: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 08:38:39.348: INFO: Number of nodes with available pods: 0
Mar  5 08:38:39.348: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 08:38:40.346: INFO: Number of nodes with available pods: 1
Mar  5 08:38:40.346: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-450, will wait for the garbage collector to delete the pods
Mar  5 08:38:40.431: INFO: Deleting DaemonSet.extensions daemon-set took: 16.426763ms
Mar  5 08:38:40.531: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.372045ms
Mar  5 08:38:44.041: INFO: Number of nodes with available pods: 0
Mar  5 08:38:44.041: INFO: Number of running nodes: 0, number of available pods: 0
Mar  5 08:38:44.046: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-450/daemonsets","resourceVersion":"5639860"},"items":null}

Mar  5 08:38:44.051: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-450/pods","resourceVersion":"5639860"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:38:44.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-450" for this suite.

â€¢ [SLOW TEST:13.251 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":277,"completed":194,"skipped":3423,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:38:44.160: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-8958
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:38:46.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8958" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":277,"completed":195,"skipped":3431,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:38:46.529: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  5 08:38:47.554: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 08:38:50.617: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar  5 08:38:50.664: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:38:50.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7797" for this suite.
STEP: Destroying namespace "webhook-7797-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":277,"completed":196,"skipped":3440,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:38:50.821: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1615
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-1a62441f-2027-4a53-a1e6-b3ceb268ca08
STEP: Creating a pod to test consume configMaps
Mar  5 08:38:51.027: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba3e02aa-a39c-4cc7-a2c4-c3d03b61a14c" in namespace "configmap-1615" to be "Succeeded or Failed"
Mar  5 08:38:51.033: INFO: Pod "pod-configmaps-ba3e02aa-a39c-4cc7-a2c4-c3d03b61a14c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.189124ms
Mar  5 08:38:53.039: INFO: Pod "pod-configmaps-ba3e02aa-a39c-4cc7-a2c4-c3d03b61a14c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012117731s
Mar  5 08:38:55.050: INFO: Pod "pod-configmaps-ba3e02aa-a39c-4cc7-a2c4-c3d03b61a14c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022284647s
STEP: Saw pod success
Mar  5 08:38:55.050: INFO: Pod "pod-configmaps-ba3e02aa-a39c-4cc7-a2c4-c3d03b61a14c" satisfied condition "Succeeded or Failed"
Mar  5 08:38:55.061: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-configmaps-ba3e02aa-a39c-4cc7-a2c4-c3d03b61a14c container configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 08:38:55.106: INFO: Waiting for pod pod-configmaps-ba3e02aa-a39c-4cc7-a2c4-c3d03b61a14c to disappear
Mar  5 08:38:55.112: INFO: Pod pod-configmaps-ba3e02aa-a39c-4cc7-a2c4-c3d03b61a14c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:38:55.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1615" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":197,"skipped":3450,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:38:55.141: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-178
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3595
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3460
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:39:02.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-178" for this suite.
STEP: Destroying namespace "nsdeletetest-3595" for this suite.
Mar  5 08:39:02.819: INFO: Namespace nsdeletetest-3595 was already deleted
STEP: Destroying namespace "nsdeletetest-3460" for this suite.

â€¢ [SLOW TEST:7.689 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":277,"completed":198,"skipped":3470,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:39:02.834: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3271
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-398b1554-0947-49d5-a5b9-017d3ef50b3a
STEP: Creating a pod to test consume configMaps
Mar  5 08:39:03.086: INFO: Waiting up to 5m0s for pod "pod-configmaps-8751176c-3842-44c9-9e4f-4451b61b5f8a" in namespace "configmap-3271" to be "Succeeded or Failed"
Mar  5 08:39:03.092: INFO: Pod "pod-configmaps-8751176c-3842-44c9-9e4f-4451b61b5f8a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.651747ms
Mar  5 08:39:05.102: INFO: Pod "pod-configmaps-8751176c-3842-44c9-9e4f-4451b61b5f8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015671082s
STEP: Saw pod success
Mar  5 08:39:05.102: INFO: Pod "pod-configmaps-8751176c-3842-44c9-9e4f-4451b61b5f8a" satisfied condition "Succeeded or Failed"
Mar  5 08:39:05.109: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-configmaps-8751176c-3842-44c9-9e4f-4451b61b5f8a container configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 08:39:05.172: INFO: Waiting for pod pod-configmaps-8751176c-3842-44c9-9e4f-4451b61b5f8a to disappear
Mar  5 08:39:05.177: INFO: Pod pod-configmaps-8751176c-3842-44c9-9e4f-4451b61b5f8a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:39:05.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3271" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":277,"completed":199,"skipped":3493,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:39:05.219: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9897
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 08:39:05.438: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e37d6e4d-557b-42ed-a534-d922c19f5b4f" in namespace "projected-9897" to be "Succeeded or Failed"
Mar  5 08:39:05.444: INFO: Pod "downwardapi-volume-e37d6e4d-557b-42ed-a534-d922c19f5b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.558559ms
Mar  5 08:39:07.453: INFO: Pod "downwardapi-volume-e37d6e4d-557b-42ed-a534-d922c19f5b4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014965837s
STEP: Saw pod success
Mar  5 08:39:07.453: INFO: Pod "downwardapi-volume-e37d6e4d-557b-42ed-a534-d922c19f5b4f" satisfied condition "Succeeded or Failed"
Mar  5 08:39:07.463: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-e37d6e4d-557b-42ed-a534-d922c19f5b4f container client-container: <nil>
STEP: delete the pod
Mar  5 08:39:07.506: INFO: Waiting for pod downwardapi-volume-e37d6e4d-557b-42ed-a534-d922c19f5b4f to disappear
Mar  5 08:39:07.515: INFO: Pod downwardapi-volume-e37d6e4d-557b-42ed-a534-d922c19f5b4f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:39:07.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9897" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":277,"completed":200,"skipped":3496,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:39:07.539: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5854
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl logs
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1288
STEP: creating an pod
Mar  5 08:39:07.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 run logs-generator --image=us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 --namespace=kubectl-5854 -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar  5 08:39:07.925: INFO: stderr: ""
Mar  5 08:39:07.925: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Waiting for log generator to start.
Mar  5 08:39:07.925: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar  5 08:39:07.925: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5854" to be "running and ready, or succeeded"
Mar  5 08:39:07.933: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.514239ms
Mar  5 08:39:09.941: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.015822162s
Mar  5 08:39:09.941: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar  5 08:39:09.941: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar  5 08:39:09.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 logs logs-generator logs-generator --namespace=kubectl-5854'
Mar  5 08:39:10.119: INFO: stderr: ""
Mar  5 08:39:10.119: INFO: stdout: "I0305 08:39:09.020127       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/lnt 487\nI0305 08:39:09.219860       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/gzq 541\nI0305 08:39:09.420120       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/btpl 231\nI0305 08:39:09.619963       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/dsj 226\nI0305 08:39:09.819926       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/7gv 271\nI0305 08:39:10.020011       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/xjrf 596\n"
STEP: limiting log lines
Mar  5 08:39:10.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 logs logs-generator logs-generator --namespace=kubectl-5854 --tail=1'
Mar  5 08:39:10.295: INFO: stderr: ""
Mar  5 08:39:10.295: INFO: stdout: "I0305 08:39:10.219985       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/zwc9 530\n"
Mar  5 08:39:10.295: INFO: got output "I0305 08:39:10.219985       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/zwc9 530\n"
STEP: limiting log bytes
Mar  5 08:39:10.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 logs logs-generator logs-generator --namespace=kubectl-5854 --limit-bytes=1'
Mar  5 08:39:10.436: INFO: stderr: ""
Mar  5 08:39:10.436: INFO: stdout: "I"
Mar  5 08:39:10.436: INFO: got output "I"
STEP: exposing timestamps
Mar  5 08:39:10.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 logs logs-generator logs-generator --namespace=kubectl-5854 --tail=1 --timestamps'
Mar  5 08:39:10.576: INFO: stderr: ""
Mar  5 08:39:10.576: INFO: stdout: "2021-03-05T08:39:10.420261591Z I0305 08:39:10.419980       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/s2z 329\n"
Mar  5 08:39:10.576: INFO: got output "2021-03-05T08:39:10.420261591Z I0305 08:39:10.419980       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/s2z 329\n"
STEP: restricting to a time range
Mar  5 08:39:13.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 logs logs-generator logs-generator --namespace=kubectl-5854 --since=1s'
Mar  5 08:39:13.276: INFO: stderr: ""
Mar  5 08:39:13.276: INFO: stdout: "I0305 08:39:12.420048       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/dcgj 311\nI0305 08:39:12.619943       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/9wxm 476\nI0305 08:39:12.820083       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/q6m 426\nI0305 08:39:13.019970       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/wzv 252\nI0305 08:39:13.219979       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/pqh9 507\n"
Mar  5 08:39:13.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 logs logs-generator logs-generator --namespace=kubectl-5854 --since=24h'
Mar  5 08:39:13.435: INFO: stderr: ""
Mar  5 08:39:13.435: INFO: stdout: "I0305 08:39:09.020127       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/lnt 487\nI0305 08:39:09.219860       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/gzq 541\nI0305 08:39:09.420120       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/btpl 231\nI0305 08:39:09.619963       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/dsj 226\nI0305 08:39:09.819926       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/7gv 271\nI0305 08:39:10.020011       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/xjrf 596\nI0305 08:39:10.219985       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/zwc9 530\nI0305 08:39:10.419980       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/s2z 329\nI0305 08:39:10.619918       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/xl9q 522\nI0305 08:39:10.820012       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/c8jq 265\nI0305 08:39:11.020127       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/45z2 581\nI0305 08:39:11.219998       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/pfw 573\nI0305 08:39:11.420083       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/gksw 515\nI0305 08:39:11.620068       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/7mfj 352\nI0305 08:39:11.820017       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/4dfh 396\nI0305 08:39:12.020126       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/wmf 359\nI0305 08:39:12.220093       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/nt6w 532\nI0305 08:39:12.420048       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/dcgj 311\nI0305 08:39:12.619943       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/9wxm 476\nI0305 08:39:12.820083       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/q6m 426\nI0305 08:39:13.019970       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/wzv 252\nI0305 08:39:13.219979       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/pqh9 507\nI0305 08:39:13.420011       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/jbs7 571\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1294
Mar  5 08:39:13.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 delete pod logs-generator --namespace=kubectl-5854'
Mar  5 08:39:23.674: INFO: stderr: ""
Mar  5 08:39:23.674: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:39:23.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5854" for this suite.

â€¢ [SLOW TEST:16.163 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1284
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":277,"completed":201,"skipped":3510,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:39:23.703: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2041
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  5 08:39:24.370: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 08:39:27.440: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:39:27.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2041" for this suite.
STEP: Destroying namespace "webhook-2041-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":277,"completed":202,"skipped":3537,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:39:27.685: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2099
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 08:39:27.973: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f27173c4-c581-4267-9335-834aa36768ae" in namespace "projected-2099" to be "Succeeded or Failed"
Mar  5 08:39:27.980: INFO: Pod "downwardapi-volume-f27173c4-c581-4267-9335-834aa36768ae": Phase="Pending", Reason="", readiness=false. Elapsed: 7.315909ms
Mar  5 08:39:29.990: INFO: Pod "downwardapi-volume-f27173c4-c581-4267-9335-834aa36768ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016611984s
Mar  5 08:39:31.999: INFO: Pod "downwardapi-volume-f27173c4-c581-4267-9335-834aa36768ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026290843s
STEP: Saw pod success
Mar  5 08:39:31.999: INFO: Pod "downwardapi-volume-f27173c4-c581-4267-9335-834aa36768ae" satisfied condition "Succeeded or Failed"
Mar  5 08:39:32.008: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-f27173c4-c581-4267-9335-834aa36768ae container client-container: <nil>
STEP: delete the pod
Mar  5 08:39:32.052: INFO: Waiting for pod downwardapi-volume-f27173c4-c581-4267-9335-834aa36768ae to disappear
Mar  5 08:39:32.057: INFO: Pod downwardapi-volume-f27173c4-c581-4267-9335-834aa36768ae no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:39:32.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2099" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":277,"completed":203,"skipped":3540,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:39:32.078: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2092
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar  5 08:39:32.261: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  5 08:39:32.287: INFO: Waiting for terminating namespaces to be deleted...
Mar  5 08:39:32.296: INFO: 
Logging pods the kubelet thinks is on node devops-control-plane-1 before test
Mar  5 08:39:32.357: INFO: kube-scheduler-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container kube-scheduler ready: true, restart count 10
Mar  5 08:39:32.357: INFO: canal-v67nk from kube-system started at 2021-02-23 18:23:29 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:39:32.357: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:39:32.357: INFO: kube-controller-manager-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container kube-controller-manager ready: true, restart count 14
Mar  5 08:39:32.357: INFO: redis-ha-server-2 from kubesphere-system started at 2021-02-24 13:03:31 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container redis ready: true, restart count 1
Mar  5 08:39:32.357: INFO: 	Container sentinel ready: true, restart count 1
Mar  5 08:39:32.357: INFO: fluent-bit-6z2kz from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:39:32.357: INFO: rook-ceph-osd-prepare-devops-control-plane-1-c2ck4 from rook-ceph started at 2021-03-05 08:21:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container provision ready: false, restart count 0
Mar  5 08:39:32.357: INFO: node-exporter-grhwd from kubesphere-monitoring-system started at 2021-03-04 17:34:37 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:39:32.357: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:39:32.357: INFO: etcd-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container etcd ready: true, restart count 1
Mar  5 08:39:32.357: INFO: rook-ceph-osd-2-7d8c68d866-pljjp from rook-ceph started at 2021-02-23 18:35:33 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container osd ready: true, restart count 1
Mar  5 08:39:32.357: INFO: thanos-ruler-k8s-0 from kubesphere-monitoring-system started at 2021-02-24 13:23:29 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container rules-configmap-reloader ready: true, restart count 1
Mar  5 08:39:32.357: INFO: 	Container thanos-ruler ready: true, restart count 1
Mar  5 08:39:32.357: INFO: s2ioperator-0 from kubesphere-devops-system started at 2021-03-05 08:23:16 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container manager ready: true, restart count 0
Mar  5 08:39:32.357: INFO: kube-apiserver-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar  5 08:39:32.357: INFO: csi-cephfsplugin-provisioner-c68f789b8-2rdqm from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (6 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container csi-attacher ready: true, restart count 10
Mar  5 08:39:32.357: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:39:32.357: INFO: 	Container csi-provisioner ready: true, restart count 13
Mar  5 08:39:32.357: INFO: 	Container csi-resizer ready: true, restart count 11
Mar  5 08:39:32.357: INFO: 	Container csi-snapshotter ready: true, restart count 19
Mar  5 08:39:32.357: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:39:32.357: INFO: istiod-1-6-10-7d869d7f58-tg6zc from istio-system started at 2021-03-01 07:43:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container discovery ready: true, restart count 1
Mar  5 08:39:32.357: INFO: rook-ceph-mon-d-c99989cc-lrjlz from rook-ceph started at 2021-03-04 08:21:35 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container mon ready: true, restart count 1
Mar  5 08:39:32.357: INFO: harbor-da5q24-harbor-jobservice-b55d6b698-8zwxd from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container jobservice ready: true, restart count 2
Mar  5 08:39:32.357: INFO: csi-cephfsplugin-8sbwl from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:39:32.357: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:39:32.357: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:39:32.357: INFO: redis-ha-haproxy-5c6559d588-b2n68 from kubesphere-system started at 2021-03-04 08:11:54 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container haproxy ready: true, restart count 2
Mar  5 08:39:32.357: INFO: csi-rbdplugin-l7fcg from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:39:32.357: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:39:32.357: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:39:32.357: INFO: details-v1-dc74fc894-m9kzk from serv-mesh-ex started at 2021-03-01 08:46:17 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container details ready: true, restart count 1
Mar  5 08:39:32.357: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:39:32.357: INFO: harbor-da5q24-harbor-notary-signer-6785bf5b4-6w2dq from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container notary-signer ready: true, restart count 2
Mar  5 08:39:32.357: INFO: harbor-da5q24-harbor-database-0 from pipeline-tools started at 2021-03-04 10:01:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container database ready: true, restart count 1
Mar  5 08:39:32.357: INFO: kube-proxy-v54pf from kube-system started at 2021-02-23 18:20:31 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:39:32.357: INFO: node-local-dns-27gsv from kube-system started at 2021-02-23 18:22:38 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:39:32.357: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-4n6zm from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:39:32.357: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:39:32.357: INFO: ratings-v1-675894856b-tjjdl from serv-mesh-ex started at 2021-03-01 08:46:17 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:39:32.357: INFO: 	Container ratings ready: true, restart count 1
Mar  5 08:39:32.357: INFO: enginsgungor-tutorial-deployment-late-e4m-3f096c9b3046-jobgsk6b from tutorial-s2i started at 2021-03-02 18:05:58 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:39:32.357: INFO: alertmanager-main-0 from kubesphere-monitoring-system started at 2021-03-05 08:23:17 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container alertmanager ready: true, restart count 0
Mar  5 08:39:32.357: INFO: 	Container config-reloader ready: true, restart count 0
Mar  5 08:39:32.357: INFO: rook-ceph-crashcollector-devops-control-plane-1-66d7bd67c8zzdhg from rook-ceph started at 2021-02-23 18:35:33 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.357: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:39:32.357: INFO: 
Logging pods the kubelet thinks is on node devops-control-plane-2 before test
Mar  5 08:39:32.410: INFO: rook-ceph-crashcollector-devops-control-plane-2-6686b8d74fqx7lt from rook-ceph started at 2021-02-23 18:34:51 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.410: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:39:32.411: INFO: rook-ceph-osd-0-855b585564-645vx from rook-ceph started at 2021-02-23 18:34:51 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container osd ready: true, restart count 1
Mar  5 08:39:32.411: INFO: redis-ha-haproxy-5c6559d588-l22bp from kubesphere-system started at 2021-02-24 13:03:15 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container haproxy ready: true, restart count 2
Mar  5 08:39:32.411: INFO: jaeger-es-index-cleaner-1614729300-p7kmh from istio-system started at 2021-03-02 23:55:03 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container jaeger-es-index-cleaner ready: false, restart count 0
Mar  5 08:39:32.411: INFO: logsidecar-injector-deploy-74c66bfd85-vpkvb from kubesphere-logging-system started at 2021-02-24 13:22:19 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container config-reloader ready: true, restart count 2
Mar  5 08:39:32.411: INFO: 	Container logsidecar-injector ready: true, restart count 2
Mar  5 08:39:32.411: INFO: minio-image-st-668b878c77-4rwrb from pipeline-tools started at 2021-03-03 17:33:04 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container minio ready: true, restart count 1
Mar  5 08:39:32.411: INFO: harbor-da5q24-harbor-trivy-0 from pipeline-tools started at 2021-03-04 10:01:13 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container trivy ready: true, restart count 1
Mar  5 08:39:32.411: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-5bzp2 from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:39:32.411: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:39:32.411: INFO: rook-ceph-osd-prepare-devops-control-plane-2-x2r69 from rook-ceph started at 2021-03-05 08:21:02 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container provision ready: false, restart count 0
Mar  5 08:39:32.411: INFO: kube-scheduler-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container kube-scheduler ready: true, restart count 8
Mar  5 08:39:32.411: INFO: etcd-65796969c7-mr9hs from kubesphere-system started at 2021-02-24 13:19:44 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container etcd ready: true, restart count 1
Mar  5 08:39:32.411: INFO: notification-deployment-65547747f7-zxr25 from kubesphere-alerting-system started at 2021-02-24 13:45:42 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container notification ready: true, restart count 1
Mar  5 08:39:32.411: INFO: harbor-da5q24-harbor-portal-84fb8bdb7f-nttwj from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container portal ready: true, restart count 1
Mar  5 08:39:32.411: INFO: node-exporter-cvhfl from kubesphere-monitoring-system started at 2021-03-04 17:34:19 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:39:32.411: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:39:32.411: INFO: etcd-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:23 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container etcd ready: true, restart count 1
Mar  5 08:39:32.411: INFO: csi-rbdplugin-4hzbs from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:39:32.411: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:39:32.411: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:39:32.411: INFO: openldap-1 from kubesphere-system started at 2021-02-24 13:04:13 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container openldap-ha ready: true, restart count 1
Mar  5 08:39:32.411: INFO: ks-installer-74b46bd68d-q6nrh from kubesphere-system started at 2021-03-04 08:15:54 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container installer ready: true, restart count 1
Mar  5 08:39:32.411: INFO: kube-controller-manager-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container kube-controller-manager ready: true, restart count 7
Mar  5 08:39:32.411: INFO: fluent-bit-grhml from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:39:32.411: INFO: ks-events-ruler-698b7899c7-9qnjp from kubesphere-logging-system started at 2021-02-24 13:22:32 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:39:32.411: INFO: 	Container events-ruler ready: true, restart count 1
Mar  5 08:39:32.411: INFO: jaeger-operator-c78679c9f-r9pbh from istio-system started at 2021-03-01 07:44:49 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container jaeger-operator ready: true, restart count 2
Mar  5 08:39:32.411: INFO: harbor-da5q24-harbor-core-67659b6b55-4mcwv from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container core ready: true, restart count 2
Mar  5 08:39:32.411: INFO: harbor-da5q24-harbor-notary-server-7fc788c84b-bdpf9 from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container notary-server ready: true, restart count 3
Mar  5 08:39:32.411: INFO: harbor-da5q24-harbor-chartmuseum-874964bb8-7dqmz from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container chartmuseum ready: true, restart count 1
Mar  5 08:39:32.411: INFO: csi-cephfsplugin-l8vgz from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:39:32.411: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:39:32.411: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:39:32.411: INFO: ks-console-fb7f5895-rxb4f from kubesphere-system started at 2021-02-24 13:04:07 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container ks-console ready: true, restart count 1
Mar  5 08:39:32.411: INFO: harbor-da5q24-harbor-registry-56df8d46bf-2s6ct from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container registry ready: true, restart count 1
Mar  5 08:39:32.411: INFO: 	Container registryctl ready: true, restart count 1
Mar  5 08:39:32.411: INFO: gogs-gogs-test-http from proteus started at 2021-03-03 07:35:46 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container wget ready: false, restart count 0
Mar  5 08:39:32.411: INFO: canal-hctnd from kube-system started at 2021-02-23 18:23:44 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:39:32.411: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:39:32.411: INFO: kube-auditing-webhook-deploy-b74bfb885-5cnp5 from kubesphere-logging-system started at 2021-02-24 13:34:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container kube-auditing-webhook ready: true, restart count 1
Mar  5 08:39:32.411: INFO: harbor-da5q24-harbor-clair-5dd889dd4f-4nclv from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container adapter ready: true, restart count 1
Mar  5 08:39:32.411: INFO: 	Container clair ready: true, restart count 5
Mar  5 08:39:32.411: INFO: jaeger-es-index-cleaner-1614815700-7s2kc from istio-system started at 2021-03-03 23:55:05 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container jaeger-es-index-cleaner ready: false, restart count 0
Mar  5 08:39:32.411: INFO: ks-apiserver-5894746f6b-gprr9 from kubesphere-system started at 2021-03-04 18:53:58 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container ks-apiserver ready: true, restart count 0
Mar  5 08:39:32.411: INFO: coredns-6cf46fccfd-vgfsb from kube-system started at 2021-02-23 18:23:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container coredns ready: true, restart count 1
Mar  5 08:39:32.411: INFO: kube-apiserver-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar  5 08:39:32.411: INFO: jaeger-collector-7887b45bf-n8cbz from istio-system started at 2021-03-01 07:45:14 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container jaeger-collector ready: true, restart count 3
Mar  5 08:39:32.411: INFO: elasticsearch-logging-curator-elasticsearch-curator-161473657d4 from kubesphere-logging-system started at 2021-03-03 01:00:02 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Mar  5 08:39:32.411: INFO: thanos-ruler-kubesphere-0 from kubesphere-monitoring-system started at 2021-03-04 07:58:11 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container rules-configmap-reloader ready: true, restart count 1
Mar  5 08:39:32.411: INFO: 	Container thanos-ruler ready: true, restart count 1
Mar  5 08:39:32.411: INFO: redis-ha-server-1 from kubesphere-system started at 2021-02-24 13:03:23 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container redis ready: true, restart count 1
Mar  5 08:39:32.411: INFO: 	Container sentinel ready: true, restart count 1
Mar  5 08:39:32.411: INFO: harbor-da5q24-harbor-nginx-687f459b8d-drvcb from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container nginx ready: true, restart count 2
Mar  5 08:39:32.411: INFO: kube-proxy-vkr26 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:39:32.411: INFO: coredns-6cf46fccfd-lbcwc from kube-system started at 2021-02-23 18:23:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container coredns ready: true, restart count 1
Mar  5 08:39:32.411: INFO: hcloud-cloud-controller-manager-7c5d46cc54-rhlq7 from kube-system started at 2021-02-23 18:23:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container hcloud-cloud-controller-manager ready: true, restart count 2
Mar  5 08:39:32.411: INFO: ks-events-operator-8dbf7fccc-v8zdm from kubesphere-logging-system started at 2021-02-24 13:22:25 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container events-operator ready: true, restart count 1
Mar  5 08:39:32.411: INFO: node-local-dns-4wxxm from kube-system started at 2021-02-23 18:22:39 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:39:32.411: INFO: reviews-v1-549f6b9d47-rttzb from serv-mesh-ex started at 2021-03-01 10:23:35 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:39:32.411: INFO: 	Container reviews ready: true, restart count 1
Mar  5 08:39:32.411: INFO: ks-controller-manager-8556448648-dk5xc from kubesphere-system started at 2021-03-04 18:53:58 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.411: INFO: 	Container ks-controller-manager ready: true, restart count 0
Mar  5 08:39:32.411: INFO: 
Logging pods the kubelet thinks is on node devops-control-plane-3 before test
Mar  5 08:39:32.473: INFO: calico-kube-controllers-589975f454-whrcl from kube-system started at 2021-02-23 18:23:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.473: INFO: 	Container calico-kube-controllers ready: true, restart count 1
Mar  5 08:39:32.473: INFO: canal-r57rn from kube-system started at 2021-02-23 18:23:39 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.473: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:39:32.473: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:39:32.473: INFO: ks-events-ruler-698b7899c7-rjqqv from kubesphere-logging-system started at 2021-02-24 13:22:32 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.473: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:39:32.473: INFO: 	Container events-ruler ready: true, restart count 1
Mar  5 08:39:32.473: INFO: machine-controller-74bbdbb8b8-7r4nj from kube-system started at 2021-02-23 18:23:35 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.473: INFO: 	Container machine-controller ready: true, restart count 27
Mar  5 08:39:32.473: INFO: minio-7bfdb5968b-cnd55 from kubesphere-system started at 2021-02-24 14:10:33 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.473: INFO: 	Container minio ready: true, restart count 1
Mar  5 08:39:32.473: INFO: thanos-ruler-kubesphere-1 from kubesphere-monitoring-system started at 2021-03-04 07:58:11 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.473: INFO: 	Container rules-configmap-reloader ready: true, restart count 1
Mar  5 08:39:32.473: INFO: 	Container thanos-ruler ready: true, restart count 1
Mar  5 08:39:32.473: INFO: ks-apiserver-5894746f6b-8p8vg from kubesphere-system started at 2021-03-04 18:54:02 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.473: INFO: 	Container ks-apiserver ready: true, restart count 0
Mar  5 08:39:32.473: INFO: rook-ceph-osd-prepare-devops-control-plane-3-q6vvm from rook-ceph started at 2021-03-05 08:21:05 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.473: INFO: 	Container provision ready: false, restart count 0
Mar  5 08:39:32.473: INFO: kube-proxy-nrg4j from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.473: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:39:32.473: INFO: csi-cephfsplugin-287cb from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:39:32.473: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:39:32.473: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:39:32.473: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:39:32.473: INFO: ks-console-fb7f5895-5pmpn from kubesphere-system started at 2021-02-24 13:04:07 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.473: INFO: 	Container ks-console ready: true, restart count 1
Mar  5 08:39:32.473: INFO: csi-rbdplugin-provisioner-5bc97cdbb9-zjbws from rook-ceph started at 2021-03-04 08:15:54 +0000 UTC (6 container statuses recorded)
Mar  5 08:39:32.473: INFO: 	Container csi-attacher ready: true, restart count 1
Mar  5 08:39:32.473: INFO: 	Container csi-provisioner ready: true, restart count 1
Mar  5 08:39:32.473: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:39:32.473: INFO: 	Container csi-resizer ready: true, restart count 1
Mar  5 08:39:32.473: INFO: 	Container csi-snapshotter ready: true, restart count 1
Mar  5 08:39:32.473: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:39:32.473: INFO: gogs-gogs-test-ssh from proteus started at 2021-03-03 07:35:46 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.473: INFO: 	Container wget ready: false, restart count 0
Mar  5 08:39:32.474: INFO: kube-controller-manager-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container kube-controller-manager ready: true, restart count 11
Mar  5 08:39:32.474: INFO: kube-scheduler-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container kube-scheduler ready: true, restart count 13
Mar  5 08:39:32.474: INFO: fluentbit-operator-7c56d66dd-7b4df from kubesphere-logging-system started at 2021-02-24 13:20:01 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container fluentbit-operator ready: true, restart count 2
Mar  5 08:39:32.474: INFO: istio-ingressgateway-76df6567c6-tndjv from istio-system started at 2021-03-01 07:44:17 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:39:32.474: INFO: ks-controller-manager-8556448648-c4spd from kubesphere-system started at 2021-03-04 18:54:03 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container ks-controller-manager ready: true, restart count 0
Mar  5 08:39:32.474: INFO: machine-controller-webhook-747c599b5c-k7xw4 from kube-system started at 2021-02-23 18:23:36 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container machine-controller-webhook ready: true, restart count 1
Mar  5 08:39:32.474: INFO: notification-manager-deployment-7c8df68d94-5dv72 from kubesphere-monitoring-system started at 2021-02-24 13:05:51 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container notification-manager ready: true, restart count 1
Mar  5 08:39:32.474: INFO: jaeger-query-b546558bf-frn98 from istio-system started at 2021-03-01 07:45:14 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container jaeger-agent ready: true, restart count 1
Mar  5 08:39:32.474: INFO: 	Container jaeger-query ready: true, restart count 1
Mar  5 08:39:32.474: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2021-03-04 17:34:29 +0000 UTC (3 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container prometheus ready: true, restart count 1
Mar  5 08:39:32.474: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  5 08:39:32.474: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  5 08:39:32.474: INFO: node-local-dns-twq6t from kube-system started at 2021-02-23 18:22:38 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:39:32.474: INFO: rook-ceph-crashcollector-devops-control-plane-3-587c5dc7b8mt27z from rook-ceph started at 2021-02-23 18:34:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:39:32.474: INFO: redis-ha-server-0 from kubesphere-system started at 2021-02-24 13:03:18 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container redis ready: true, restart count 1
Mar  5 08:39:32.474: INFO: 	Container sentinel ready: true, restart count 1
Mar  5 08:39:32.474: INFO: fluent-bit-s284n from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:39:32.474: INFO: kube-auditing-operator-6ddc8db4b-vfxqc from kubesphere-logging-system started at 2021-02-24 13:34:03 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container kube-auditing-operator ready: true, restart count 1
Mar  5 08:39:32.474: INFO: notification-db-init-job-mqs2r from kubesphere-alerting-system started at 2021-03-03 17:34:05 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container notification-db-init ready: false, restart count 0
Mar  5 08:39:32.474: INFO: notification-db-ctrl-job-7kzd5 from kubesphere-alerting-system started at 2021-03-03 17:34:15 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container notification-db-ctrl ready: false, restart count 0
Mar  5 08:39:32.474: INFO: without-dockerfile-v1-7dc7678956-mh5vw from serv-mesh-ex started at 2021-03-04 08:15:54 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container container-2a87d6 ready: true, restart count 1
Mar  5 08:39:32.474: INFO: etcd-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:36 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container etcd ready: true, restart count 1
Mar  5 08:39:32.474: INFO: redis-ha-haproxy-5c6559d588-qhqhx from kubesphere-system started at 2021-02-24 13:03:15 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container haproxy ready: true, restart count 1
Mar  5 08:39:32.474: INFO: logsidecar-injector-deploy-74c66bfd85-62xbg from kubesphere-logging-system started at 2021-02-24 13:22:19 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:39:32.474: INFO: 	Container logsidecar-injector ready: true, restart count 1
Mar  5 08:39:32.474: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-wp8ts from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:39:32.474: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:39:32.474: INFO: kiali-operator-58b8765b9c-6qh7r from istio-system started at 2021-03-01 07:44:57 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container operator ready: true, restart count 1
Mar  5 08:39:32.474: INFO: kube-apiserver-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar  5 08:39:32.474: INFO: csi-rbdplugin-7zr2t from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:39:32.474: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:39:32.474: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:39:32.474: INFO: node-exporter-6fsk8 from kubesphere-monitoring-system started at 2021-03-04 17:34:28 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:39:32.474: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:39:32.474: INFO: rook-ceph-osd-1-845f4b4486-5phnd from rook-ceph started at 2021-02-23 18:34:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container osd ready: true, restart count 1
Mar  5 08:39:32.474: INFO: openldap-0 from kubesphere-system started at 2021-02-24 13:03:27 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container openldap-ha ready: true, restart count 1
Mar  5 08:39:32.474: INFO: gogs-gogs-7467fdcf8f-rw58p from default started at 2021-03-05 08:23:09 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.474: INFO: 	Container gogs ready: true, restart count 0
Mar  5 08:39:32.474: INFO: 
Logging pods the kubelet thinks is on node devops-pool1-6c76d44df9-8tgs6 before test
Mar  5 08:39:32.497: INFO: canal-rm2l9 from kube-system started at 2021-02-23 18:25:59 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.497: INFO: 	Container calico-node ready: true, restart count 8
Mar  5 08:39:32.497: INFO: 	Container kube-flannel ready: true, restart count 8
Mar  5 08:39:32.497: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2021-03-05 08:23:53 +0000 UTC (3 container statuses recorded)
Mar  5 08:39:32.497: INFO: 	Container prometheus ready: true, restart count 1
Mar  5 08:39:32.497: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  5 08:39:32.497: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  5 08:39:32.497: INFO: elasticsearch-logging-discovery-1 from kubesphere-logging-system started at 2021-03-05 08:23:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.497: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  5 08:39:32.497: INFO: sonobuoy from sonobuoy started at 2021-03-05 07:46:37 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.497: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  5 08:39:32.497: INFO: node-local-dns-kh572 from kube-system started at 2021-02-23 18:25:59 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.497: INFO: 	Container node-cache ready: true, restart count 8
Mar  5 08:39:32.497: INFO: node-exporter-824lt from kubesphere-monitoring-system started at 2021-03-04 17:34:42 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.497: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:39:32.497: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:39:32.497: INFO: webhook-to-be-mutated from webhook-2041 started at 2021-03-05 08:39:27 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.497: INFO: 	Container example ready: false, restart count 0
Mar  5 08:39:32.497: INFO: kube-proxy-jxx7v from kube-system started at 2021-02-23 18:25:59 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.497: INFO: 	Container kube-proxy ready: true, restart count 8
Mar  5 08:39:32.497: INFO: fluent-bit-2vc49 from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.497: INFO: 	Container fluent-bit ready: true, restart count 8
Mar  5 08:39:32.497: INFO: csi-cephfsplugin-crjw8 from rook-ceph started at 2021-03-05 08:23:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:39:32.497: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar  5 08:39:32.497: INFO: 	Container driver-registrar ready: true, restart count 0
Mar  5 08:39:32.497: INFO: 	Container liveness-prometheus ready: true, restart count 0
Mar  5 08:39:32.497: INFO: elasticsearch-logging-data-2 from kubesphere-logging-system started at 2021-03-05 08:25:23 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.497: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  5 08:39:32.497: INFO: csi-rbdplugin-7hr88 from rook-ceph started at 2021-03-05 08:23:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:39:32.497: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar  5 08:39:32.497: INFO: 	Container driver-registrar ready: true, restart count 0
Mar  5 08:39:32.497: INFO: 	Container liveness-prometheus ready: true, restart count 0
Mar  5 08:39:32.497: INFO: gogs-postgresql-0 from default started at 2021-03-05 08:23:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.497: INFO: 	Container gogs-postgresql ready: true, restart count 0
Mar  5 08:39:32.497: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-9ffgc from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.497: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:39:32.497: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:39:32.497: INFO: 
Logging pods the kubelet thinks is on node devops-pool1-6c76d44df9-dwtv7 before test
Mar  5 08:39:32.560: INFO: rook-ceph-crashcollector-devops-pool1-6c76d44df9-dwtv7-7dfdvvgh from rook-ceph started at 2021-02-23 18:34:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:39:32.560: INFO: canal-mfdhb from kube-system started at 2021-02-23 18:26:00 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:39:32.560: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:39:32.560: INFO: node-local-dns-8s8tk from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:39:32.560: INFO: enginsgungor-hello-deployment-latest--0dq-14b4f67d0b7e-jobptskv from serv-mesh-ex started at 2021-03-01 19:28:59 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:39:32.560: INFO: ks-sample-dev-7bb944b987-hnlc4 from kubesphere-sample-dev started at 2021-03-03 11:03:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container js-sample ready: true, restart count 1
Mar  5 08:39:32.560: INFO: notification-manager-operator-6958786cd6-26xxr from kubesphere-monitoring-system started at 2021-03-04 08:15:54 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Mar  5 08:39:32.560: INFO: 	Container notification-manager-operator ready: true, restart count 2
Mar  5 08:39:32.560: INFO: openpitrix-hyperpitrix-deployment-6d48d87c6c-z7vww from openpitrix-system started at 2021-03-05 08:23:09 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container hyperpitrix ready: true, restart count 0
Mar  5 08:39:32.560: INFO: rook-ceph-operator-6fb9f456fc-8g9z2 from rook-ceph started at 2021-02-23 18:29:52 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container rook-ceph-operator ready: true, restart count 1
Mar  5 08:39:32.560: INFO: csi-rbdplugin-nhvm9 from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:39:32.560: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:39:32.560: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:39:32.560: INFO: harbor-da5q24-harbor-redis-0 from pipeline-tools started at 2021-03-04 10:01:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container redis ready: true, restart count 1
Mar  5 08:39:32.560: INFO: kube-auditing-webhook-deploy-b74bfb885-qf48j from kubesphere-logging-system started at 2021-02-24 13:34:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container kube-auditing-webhook ready: true, restart count 1
Mar  5 08:39:32.560: INFO: kiali-5fc8bfd66b-wjrqr from istio-system started at 2021-03-01 07:45:58 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container kiali ready: true, restart count 1
Mar  5 08:39:32.560: INFO: hyperpitrix-release-app-job-tqln9 from openpitrix-system started at 2021-03-04 18:51:28 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container hyperpitrix-release-app-job ready: false, restart count 0
Mar  5 08:39:32.560: INFO: enginsgungor-hello-deployment-latest--6eg-3afd31aa2f90-jobgl2pn from serv-mesh-ex started at 2021-03-01 19:15:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:39:32.560: INFO: enginsgungor-tutorial-deployment-late-9pz-042c363e1509-job598vw from tutorial-s2i started at 2021-03-02 18:10:13 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:39:32.560: INFO: csi-cephfsplugin-nxcmz from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:39:32.560: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:39:32.560: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:39:32.560: INFO: fluent-bit-s42x9 from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:39:32.560: INFO: enginsgungor-hello-world-latest-ug1-ejy-f941edb3103d-job-pr5nr from serv-mesh-ex started at 2021-03-01 18:50:41 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:39:32.560: INFO: elasticsearch-logging-curator-elasticsearch-curator-161481wvfqt from kubesphere-logging-system started at 2021-03-04 01:00:01 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Mar  5 08:39:32.560: INFO: node-exporter-f6kh5 from kubesphere-monitoring-system started at 2021-03-04 17:34:32 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:39:32.560: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:39:32.560: INFO: sonobuoy-e2e-job-41e333abf2c348c1 from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container e2e ready: true, restart count 0
Mar  5 08:39:32.560: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:39:32.560: INFO: ks-sample-dev-5c5d97c6cc-64hxc from kubesphere-offline-dev started at 2021-03-05 08:23:09 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container js-sample ready: true, restart count 0
Mar  5 08:39:32.560: INFO: elasticsearch-logging-data-1 from kubesphere-logging-system started at 2021-02-24 13:20:42 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 08:39:32.560: INFO: ks-controller-manager-8556448648-cbt26 from kubesphere-system started at 2021-03-04 18:53:55 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container ks-controller-manager ready: true, restart count 1
Mar  5 08:39:32.560: INFO: rook-ceph-mgr-a-55fb9d48b6-clq94 from rook-ceph started at 2021-02-23 18:34:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container mgr ready: true, restart count 2
Mar  5 08:39:32.560: INFO: ks-apiserver-5894746f6b-2dq28 from kubesphere-system started at 2021-03-04 18:53:54 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container ks-apiserver ready: true, restart count 0
Mar  5 08:39:32.560: INFO: ks-console-fb7f5895-vvxbg from kubesphere-system started at 2021-02-25 07:43:56 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container ks-console ready: true, restart count 1
Mar  5 08:39:32.560: INFO: tutorial-deployment-v1-5549b8b9d8-t8t9z from tutorial-s2i started at 2021-03-02 18:10:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container container-t1u32h ready: true, restart count 1
Mar  5 08:39:32.560: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-f7ttl from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:39:32.560: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:39:32.560: INFO: snapshot-controller-0 from kube-system started at 2021-03-05 08:23:14 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  5 08:39:32.560: INFO: kube-proxy-2czsl from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.560: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:39:32.560: INFO: csi-cephfsplugin-provisioner-c68f789b8-k89fp from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (6 container statuses recorded)
Mar  5 08:39:32.561: INFO: 	Container csi-attacher ready: true, restart count 14
Mar  5 08:39:32.561: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:39:32.561: INFO: 	Container csi-provisioner ready: true, restart count 14
Mar  5 08:39:32.561: INFO: 	Container csi-resizer ready: true, restart count 12
Mar  5 08:39:32.561: INFO: 	Container csi-snapshotter ready: true, restart count 13
Mar  5 08:39:32.561: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:39:32.561: INFO: elasticsearch-logging-discovery-2 from kubesphere-logging-system started at 2021-02-24 13:21:07 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.561: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 08:39:32.561: INFO: thanos-ruler-k8s-1 from kubesphere-monitoring-system started at 2021-03-05 08:23:19 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.561: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  5 08:39:32.561: INFO: 	Container thanos-ruler ready: true, restart count 0
Mar  5 08:39:32.561: INFO: rook-ceph-mon-a-85cd5968bc-hq6sb from rook-ceph started at 2021-02-23 18:33:25 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.561: INFO: 	Container mon ready: true, restart count 1
Mar  5 08:39:32.561: INFO: alertmanager-main-1 from kubesphere-monitoring-system started at 2021-02-24 13:05:28 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.561: INFO: 	Container alertmanager ready: true, restart count 1
Mar  5 08:39:32.561: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:39:32.561: INFO: 
Logging pods the kubelet thinks is on node devops-pool1-6c76d44df9-njb8n before test
Mar  5 08:39:32.621: INFO: csi-rbdplugin-provisioner-5bc97cdbb9-nlb2z from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (6 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container csi-attacher ready: true, restart count 21
Mar  5 08:39:32.621: INFO: 	Container csi-provisioner ready: true, restart count 12
Mar  5 08:39:32.621: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:39:32.621: INFO: 	Container csi-resizer ready: true, restart count 11
Mar  5 08:39:32.621: INFO: 	Container csi-snapshotter ready: true, restart count 13
Mar  5 08:39:32.621: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:39:32.621: INFO: ks-events-exporter-5bc4d9f496-zm5tx from kubesphere-logging-system started at 2021-02-24 13:22:33 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:39:32.621: INFO: 	Container events-exporter ready: true, restart count 1
Mar  5 08:39:32.621: INFO: kubesphere-router-serv-mesh-ex-585558c74b-45mf8 from kubesphere-controls-system started at 2021-03-01 08:45:33 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:39:32.621: INFO: 	Container nginx-ingress-controller ready: true, restart count 1
Mar  5 08:39:32.621: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-xprqf from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:39:32.621: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:39:32.621: INFO: node-local-dns-5qx5m from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:39:32.621: INFO: kube-proxy-bpngj from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:39:32.621: INFO: default-http-backend-857d7b6856-gt6hh from kubesphere-controls-system started at 2021-02-24 13:03:47 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container default-http-backend ready: true, restart count 1
Mar  5 08:39:32.621: INFO: mysql-7f64d9f584-49fnb from kubesphere-system started at 2021-02-24 13:19:43 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container mysql ready: true, restart count 1
Mar  5 08:39:32.621: INFO: rook-ceph-tools-64bd84c8b5-hd7mk from rook-ceph started at 2021-02-25 07:43:01 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container rook-ceph-tools ready: true, restart count 1
Mar  5 08:39:32.621: INFO: csi-cephfsplugin-t7l65 from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:39:32.621: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:39:32.621: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:39:32.621: INFO: rook-ceph-crashcollector-devops-pool1-6c76d44df9-njb8n-cfbd6xbn from rook-ceph started at 2021-02-23 18:33:46 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:39:32.621: INFO: prometheus-operator-84d58bf775-q2drc from kubesphere-monitoring-system started at 2021-02-24 13:05:18 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Mar  5 08:39:32.621: INFO: 	Container prometheus-operator ready: true, restart count 1
Mar  5 08:39:32.621: INFO: notification-manager-deployment-7c8df68d94-7vd2s from kubesphere-monitoring-system started at 2021-02-24 13:05:51 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container notification-manager ready: true, restart count 1
Mar  5 08:39:32.621: INFO: canal-tp2fn from kube-system started at 2021-02-23 18:26:00 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:39:32.621: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:39:32.621: INFO: elasticsearch-logging-data-0 from kubesphere-logging-system started at 2021-02-24 13:19:49 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 08:39:32.621: INFO: csi-rbdplugin-5n26w from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:39:32.621: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:39:32.621: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:39:32.621: INFO: kube-state-metrics-95c974544-drf59 from kubesphere-monitoring-system started at 2021-02-24 13:05:20 +0000 UTC (3 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 1
Mar  5 08:39:32.621: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 1
Mar  5 08:39:32.621: INFO: 	Container kube-state-metrics ready: true, restart count 1
Mar  5 08:39:32.621: INFO: hellojs-v1-5f4d98c7df-snl82 from serv-mesh-ex started at 2021-03-02 06:09:42 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container container-3nysic ready: true, restart count 1
Mar  5 08:39:32.621: INFO: kubectl-admin-5d98567c78-m72x5 from kubesphere-controls-system started at 2021-02-24 13:06:05 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container kubectl ready: true, restart count 1
Mar  5 08:39:32.621: INFO: rook-ceph-mon-c-764d6d7649-wcbn5 from rook-ceph started at 2021-02-23 18:33:46 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container mon ready: true, restart count 1
Mar  5 08:39:32.621: INFO: enginsgungor-hello-deployment-latest--fsk-bb68efbac665-jobm9zxh from serv-mesh-ex started at 2021-03-02 06:09:25 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:39:32.621: INFO: node-exporter-fhq4b from kubesphere-monitoring-system started at 2021-03-04 17:34:24 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:39:32.621: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:39:32.621: INFO: enginsgungor-hello-deployment-latest--cnc-39caa4291750-jobgnx27 from serv-mesh-ex started at 2021-03-01 19:24:09 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:39:32.621: INFO: productpage-v1-795bd6db76-trctn from serv-mesh-ex started at 2021-03-04 08:15:54 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:39:32.621: INFO: 	Container productpage ready: true, restart count 1
Mar  5 08:39:32.621: INFO: alertmanager-main-2 from kubesphere-monitoring-system started at 2021-02-24 13:05:28 +0000 UTC (2 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container alertmanager ready: true, restart count 1
Mar  5 08:39:32.621: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:39:32.621: INFO: elasticsearch-logging-discovery-0 from kubesphere-logging-system started at 2021-02-24 13:19:50 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 08:39:32.621: INFO: fluent-bit-cqbf6 from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:39:32.621: INFO: ks-jenkins-5bf5cbf449-tzqjz from kubesphere-devops-system started at 2021-02-28 09:58:29 +0000 UTC (1 container statuses recorded)
Mar  5 08:39:32.621: INFO: 	Container ks-jenkins ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-087b647b-7c2d-4bf5-943d-b6405117141a 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-087b647b-7c2d-4bf5-943d-b6405117141a off the node devops-pool1-6c76d44df9-8tgs6
STEP: verifying the node doesn't have the label kubernetes.io/e2e-087b647b-7c2d-4bf5-943d-b6405117141a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:39:36.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2092" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":277,"completed":204,"skipped":3554,"failed":0}
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:39:36.818: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4202
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Mar  5 08:39:37.010: INFO: PodSpec: initContainers in spec.initContainers
Mar  5 08:40:21.189: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8aec55a7-428e-4531-8335-04030bc78987", GenerateName:"", Namespace:"init-container-4202", SelfLink:"/api/v1/namespaces/init-container-4202/pods/pod-init-8aec55a7-428e-4531-8335-04030bc78987", UID:"5076f69b-c782-4fe7-a08c-a40fab58eb11", ResourceVersion:"5641112", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63750530377, loc:(*time.Location)(0x7b51220)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"10588936"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.244.3.23/32", "cni.projectcalico.org/podIPs":"10.244.3.23/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003e43e80), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e43f80)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003e43fe0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002f8a000)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002f8a020), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002f8a040)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-xm47d", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc006b92dc0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xm47d", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xm47d", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xm47d", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002e83c78), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"devops-pool1-6c76d44df9-8tgs6", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0006f4bd0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002e83cf0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002e83d10)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002e83d18), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002e83d1c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530377, loc:(*time.Location)(0x7b51220)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530377, loc:(*time.Location)(0x7b51220)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530377, loc:(*time.Location)(0x7b51220)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530377, loc:(*time.Location)(0x7b51220)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.0.8", PodIP:"10.244.3.23", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.3.23"}}, StartTime:(*v1.Time)(0xc002f8a060), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006f4cb0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006f4d90)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://9138685f405bd30ecd11880ba450a1ca50e51869532873e833ea78a53f7c9c10", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f8a0e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f8a0a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc002e83d9f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:40:21.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4202" for this suite.

â€¢ [SLOW TEST:44.396 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":277,"completed":205,"skipped":3555,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:40:21.215: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-5160
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:40:29.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5160" for this suite.

â€¢ [SLOW TEST:8.289 seconds]
[sig-apps] Job
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":277,"completed":206,"skipped":3587,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:40:29.512: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3612
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating all guestbook components
Mar  5 08:40:29.741: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Mar  5 08:40:29.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 create -f - --namespace=kubectl-3612'
Mar  5 08:40:30.205: INFO: stderr: ""
Mar  5 08:40:30.205: INFO: stdout: "service/agnhost-slave created\n"
Mar  5 08:40:30.206: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Mar  5 08:40:30.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 create -f - --namespace=kubectl-3612'
Mar  5 08:40:30.581: INFO: stderr: ""
Mar  5 08:40:30.581: INFO: stdout: "service/agnhost-master created\n"
Mar  5 08:40:30.581: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  5 08:40:30.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 create -f - --namespace=kubectl-3612'
Mar  5 08:40:31.085: INFO: stderr: ""
Mar  5 08:40:31.085: INFO: stdout: "service/frontend created\n"
Mar  5 08:40:31.086: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar  5 08:40:31.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 create -f - --namespace=kubectl-3612'
Mar  5 08:40:31.568: INFO: stderr: ""
Mar  5 08:40:31.568: INFO: stdout: "deployment.apps/frontend created\n"
Mar  5 08:40:31.569: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  5 08:40:31.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 create -f - --namespace=kubectl-3612'
Mar  5 08:40:31.964: INFO: stderr: ""
Mar  5 08:40:31.964: INFO: stdout: "deployment.apps/agnhost-master created\n"
Mar  5 08:40:31.965: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  5 08:40:31.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 create -f - --namespace=kubectl-3612'
Mar  5 08:40:32.326: INFO: stderr: ""
Mar  5 08:40:32.326: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Mar  5 08:40:32.326: INFO: Waiting for all frontend pods to be Running.
Mar  5 08:40:37.377: INFO: Waiting for frontend to serve content.
Mar  5 08:40:37.401: INFO: Trying to add a new entry to the guestbook.
Mar  5 08:40:37.420: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar  5 08:40:37.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 delete --grace-period=0 --force -f - --namespace=kubectl-3612'
Mar  5 08:40:37.589: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 08:40:37.589: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar  5 08:40:37.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 delete --grace-period=0 --force -f - --namespace=kubectl-3612'
Mar  5 08:40:37.740: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 08:40:37.740: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  5 08:40:37.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 delete --grace-period=0 --force -f - --namespace=kubectl-3612'
Mar  5 08:40:37.953: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 08:40:37.954: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  5 08:40:37.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 delete --grace-period=0 --force -f - --namespace=kubectl-3612'
Mar  5 08:40:38.105: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 08:40:38.105: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  5 08:40:38.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 delete --grace-period=0 --force -f - --namespace=kubectl-3612'
Mar  5 08:40:38.259: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 08:40:38.259: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Mar  5 08:40:38.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 delete --grace-period=0 --force -f - --namespace=kubectl-3612'
Mar  5 08:40:38.446: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 08:40:38.446: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:40:38.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3612" for this suite.

â€¢ [SLOW TEST:8.959 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:310
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":277,"completed":207,"skipped":3640,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:40:38.474: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7281
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar  5 08:40:38.684: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  5 08:40:38.711: INFO: Waiting for terminating namespaces to be deleted...
Mar  5 08:40:38.718: INFO: 
Logging pods the kubelet thinks is on node devops-control-plane-1 before test
Mar  5 08:40:38.754: INFO: node-local-dns-27gsv from kube-system started at 2021-02-23 18:22:38 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:40:38.754: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-4n6zm from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:40:38.754: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:40:38.754: INFO: ratings-v1-675894856b-tjjdl from serv-mesh-ex started at 2021-03-01 08:46:17 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:40:38.754: INFO: 	Container ratings ready: true, restart count 1
Mar  5 08:40:38.754: INFO: enginsgungor-tutorial-deployment-late-e4m-3f096c9b3046-jobgsk6b from tutorial-s2i started at 2021-03-02 18:05:58 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:40:38.754: INFO: alertmanager-main-0 from kubesphere-monitoring-system started at 2021-03-05 08:23:17 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container alertmanager ready: true, restart count 0
Mar  5 08:40:38.754: INFO: 	Container config-reloader ready: true, restart count 0
Mar  5 08:40:38.754: INFO: harbor-da5q24-harbor-database-0 from pipeline-tools started at 2021-03-04 10:01:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container database ready: true, restart count 1
Mar  5 08:40:38.754: INFO: kube-proxy-v54pf from kube-system started at 2021-02-23 18:20:31 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:40:38.754: INFO: rook-ceph-crashcollector-devops-control-plane-1-66d7bd67c8zzdhg from rook-ceph started at 2021-02-23 18:35:33 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:40:38.754: INFO: canal-v67nk from kube-system started at 2021-02-23 18:23:29 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:40:38.754: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:40:38.754: INFO: kube-scheduler-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container kube-scheduler ready: true, restart count 10
Mar  5 08:40:38.754: INFO: redis-ha-server-2 from kubesphere-system started at 2021-02-24 13:03:31 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container redis ready: true, restart count 1
Mar  5 08:40:38.754: INFO: 	Container sentinel ready: true, restart count 1
Mar  5 08:40:38.754: INFO: fluent-bit-6z2kz from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:40:38.754: INFO: rook-ceph-osd-prepare-devops-control-plane-1-c2ck4 from rook-ceph started at 2021-03-05 08:21:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container provision ready: false, restart count 0
Mar  5 08:40:38.754: INFO: node-exporter-grhwd from kubesphere-monitoring-system started at 2021-03-04 17:34:37 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:40:38.754: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:40:38.754: INFO: kube-controller-manager-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container kube-controller-manager ready: true, restart count 14
Mar  5 08:40:38.754: INFO: rook-ceph-osd-2-7d8c68d866-pljjp from rook-ceph started at 2021-02-23 18:35:33 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container osd ready: true, restart count 1
Mar  5 08:40:38.754: INFO: thanos-ruler-k8s-0 from kubesphere-monitoring-system started at 2021-02-24 13:23:29 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container rules-configmap-reloader ready: true, restart count 1
Mar  5 08:40:38.754: INFO: 	Container thanos-ruler ready: true, restart count 1
Mar  5 08:40:38.754: INFO: s2ioperator-0 from kubesphere-devops-system started at 2021-03-05 08:23:16 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container manager ready: true, restart count 0
Mar  5 08:40:38.754: INFO: etcd-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container etcd ready: true, restart count 1
Mar  5 08:40:38.754: INFO: csi-cephfsplugin-provisioner-c68f789b8-2rdqm from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (6 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container csi-attacher ready: true, restart count 10
Mar  5 08:40:38.754: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:40:38.754: INFO: 	Container csi-provisioner ready: true, restart count 13
Mar  5 08:40:38.754: INFO: 	Container csi-resizer ready: true, restart count 11
Mar  5 08:40:38.754: INFO: 	Container csi-snapshotter ready: true, restart count 19
Mar  5 08:40:38.754: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:40:38.754: INFO: istiod-1-6-10-7d869d7f58-tg6zc from istio-system started at 2021-03-01 07:43:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container discovery ready: true, restart count 1
Mar  5 08:40:38.754: INFO: rook-ceph-mon-d-c99989cc-lrjlz from rook-ceph started at 2021-03-04 08:21:35 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container mon ready: true, restart count 1
Mar  5 08:40:38.754: INFO: harbor-da5q24-harbor-jobservice-b55d6b698-8zwxd from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container jobservice ready: true, restart count 2
Mar  5 08:40:38.754: INFO: kube-apiserver-devops-control-plane-1 from kube-system started at 2021-02-23 18:20:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar  5 08:40:38.754: INFO: redis-ha-haproxy-5c6559d588-b2n68 from kubesphere-system started at 2021-03-04 08:11:54 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container haproxy ready: true, restart count 2
Mar  5 08:40:38.754: INFO: csi-cephfsplugin-8sbwl from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:40:38.754: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:40:38.754: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:40:38.754: INFO: details-v1-dc74fc894-m9kzk from serv-mesh-ex started at 2021-03-01 08:46:17 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container details ready: true, restart count 1
Mar  5 08:40:38.754: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:40:38.754: INFO: harbor-da5q24-harbor-notary-signer-6785bf5b4-6w2dq from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container notary-signer ready: true, restart count 2
Mar  5 08:40:38.754: INFO: csi-rbdplugin-l7fcg from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:40:38.754: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:40:38.754: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:40:38.754: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:40:38.754: INFO: 
Logging pods the kubelet thinks is on node devops-control-plane-2 before test
Mar  5 08:40:38.797: INFO: rook-ceph-crashcollector-devops-control-plane-2-6686b8d74fqx7lt from rook-ceph started at 2021-02-23 18:34:51 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.797: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:40:38.797: INFO: redis-ha-haproxy-5c6559d588-l22bp from kubesphere-system started at 2021-02-24 13:03:15 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.797: INFO: 	Container haproxy ready: true, restart count 2
Mar  5 08:40:38.797: INFO: jaeger-es-index-cleaner-1614729300-p7kmh from istio-system started at 2021-03-02 23:55:03 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.797: INFO: 	Container jaeger-es-index-cleaner ready: false, restart count 0
Mar  5 08:40:38.797: INFO: rook-ceph-osd-0-855b585564-645vx from rook-ceph started at 2021-02-23 18:34:51 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.797: INFO: 	Container osd ready: true, restart count 1
Mar  5 08:40:38.797: INFO: minio-image-st-668b878c77-4rwrb from pipeline-tools started at 2021-03-03 17:33:04 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.797: INFO: 	Container minio ready: true, restart count 1
Mar  5 08:40:38.797: INFO: harbor-da5q24-harbor-trivy-0 from pipeline-tools started at 2021-03-04 10:01:13 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.797: INFO: 	Container trivy ready: true, restart count 1
Mar  5 08:40:38.797: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-5bzp2 from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.797: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:40:38.797: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:40:38.797: INFO: rook-ceph-osd-prepare-devops-control-plane-2-x2r69 from rook-ceph started at 2021-03-05 08:21:02 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.797: INFO: 	Container provision ready: false, restart count 0
Mar  5 08:40:38.797: INFO: logsidecar-injector-deploy-74c66bfd85-vpkvb from kubesphere-logging-system started at 2021-02-24 13:22:19 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container config-reloader ready: true, restart count 2
Mar  5 08:40:38.798: INFO: 	Container logsidecar-injector ready: true, restart count 2
Mar  5 08:40:38.798: INFO: etcd-65796969c7-mr9hs from kubesphere-system started at 2021-02-24 13:19:44 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container etcd ready: true, restart count 1
Mar  5 08:40:38.798: INFO: notification-deployment-65547747f7-zxr25 from kubesphere-alerting-system started at 2021-02-24 13:45:42 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container notification ready: true, restart count 1
Mar  5 08:40:38.798: INFO: harbor-da5q24-harbor-portal-84fb8bdb7f-nttwj from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container portal ready: true, restart count 1
Mar  5 08:40:38.798: INFO: node-exporter-cvhfl from kubesphere-monitoring-system started at 2021-03-04 17:34:19 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:40:38.798: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:40:38.798: INFO: kube-scheduler-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container kube-scheduler ready: true, restart count 8
Mar  5 08:40:38.798: INFO: csi-rbdplugin-4hzbs from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:40:38.798: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:40:38.798: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:40:38.798: INFO: openldap-1 from kubesphere-system started at 2021-02-24 13:04:13 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container openldap-ha ready: true, restart count 1
Mar  5 08:40:38.798: INFO: ks-installer-74b46bd68d-q6nrh from kubesphere-system started at 2021-03-04 08:15:54 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container installer ready: true, restart count 1
Mar  5 08:40:38.798: INFO: etcd-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:23 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container etcd ready: true, restart count 1
Mar  5 08:40:38.798: INFO: fluent-bit-grhml from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:40:38.798: INFO: ks-events-ruler-698b7899c7-9qnjp from kubesphere-logging-system started at 2021-02-24 13:22:32 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:40:38.798: INFO: 	Container events-ruler ready: true, restart count 1
Mar  5 08:40:38.798: INFO: jaeger-operator-c78679c9f-r9pbh from istio-system started at 2021-03-01 07:44:49 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container jaeger-operator ready: true, restart count 2
Mar  5 08:40:38.798: INFO: harbor-da5q24-harbor-core-67659b6b55-4mcwv from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container core ready: true, restart count 2
Mar  5 08:40:38.798: INFO: harbor-da5q24-harbor-notary-server-7fc788c84b-bdpf9 from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container notary-server ready: true, restart count 3
Mar  5 08:40:38.798: INFO: kube-controller-manager-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container kube-controller-manager ready: true, restart count 7
Mar  5 08:40:38.798: INFO: harbor-da5q24-harbor-chartmuseum-874964bb8-7dqmz from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container chartmuseum ready: true, restart count 1
Mar  5 08:40:38.798: INFO: ks-console-fb7f5895-rxb4f from kubesphere-system started at 2021-02-24 13:04:07 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container ks-console ready: true, restart count 1
Mar  5 08:40:38.798: INFO: harbor-da5q24-harbor-registry-56df8d46bf-2s6ct from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container registry ready: true, restart count 1
Mar  5 08:40:38.798: INFO: 	Container registryctl ready: true, restart count 1
Mar  5 08:40:38.798: INFO: gogs-gogs-test-http from proteus started at 2021-03-03 07:35:46 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container wget ready: false, restart count 0
Mar  5 08:40:38.798: INFO: csi-cephfsplugin-l8vgz from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:40:38.798: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:40:38.798: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:40:38.798: INFO: kube-auditing-webhook-deploy-b74bfb885-5cnp5 from kubesphere-logging-system started at 2021-02-24 13:34:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container kube-auditing-webhook ready: true, restart count 1
Mar  5 08:40:38.798: INFO: harbor-da5q24-harbor-clair-5dd889dd4f-4nclv from pipeline-tools started at 2021-03-04 10:01:11 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container adapter ready: true, restart count 1
Mar  5 08:40:38.798: INFO: 	Container clair ready: true, restart count 5
Mar  5 08:40:38.798: INFO: jaeger-es-index-cleaner-1614815700-7s2kc from istio-system started at 2021-03-03 23:55:05 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container jaeger-es-index-cleaner ready: false, restart count 0
Mar  5 08:40:38.798: INFO: ks-apiserver-5894746f6b-gprr9 from kubesphere-system started at 2021-03-04 18:53:58 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container ks-apiserver ready: true, restart count 0
Mar  5 08:40:38.798: INFO: canal-hctnd from kube-system started at 2021-02-23 18:23:44 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:40:38.798: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:40:38.798: INFO: coredns-6cf46fccfd-vgfsb from kube-system started at 2021-02-23 18:23:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container coredns ready: true, restart count 1
Mar  5 08:40:38.798: INFO: jaeger-collector-7887b45bf-n8cbz from istio-system started at 2021-03-01 07:45:14 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container jaeger-collector ready: true, restart count 3
Mar  5 08:40:38.798: INFO: elasticsearch-logging-curator-elasticsearch-curator-161473657d4 from kubesphere-logging-system started at 2021-03-03 01:00:02 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.798: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Mar  5 08:40:38.799: INFO: thanos-ruler-kubesphere-0 from kubesphere-monitoring-system started at 2021-03-04 07:58:11 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.799: INFO: 	Container rules-configmap-reloader ready: true, restart count 1
Mar  5 08:40:38.799: INFO: 	Container thanos-ruler ready: true, restart count 1
Mar  5 08:40:38.799: INFO: kube-apiserver-devops-control-plane-2 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.799: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar  5 08:40:38.799: INFO: harbor-da5q24-harbor-nginx-687f459b8d-drvcb from pipeline-tools started at 2021-03-04 10:01:10 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.799: INFO: 	Container nginx ready: true, restart count 2
Mar  5 08:40:38.799: INFO: redis-ha-server-1 from kubesphere-system started at 2021-02-24 13:03:23 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.799: INFO: 	Container redis ready: true, restart count 1
Mar  5 08:40:38.799: INFO: 	Container sentinel ready: true, restart count 1
Mar  5 08:40:38.799: INFO: coredns-6cf46fccfd-lbcwc from kube-system started at 2021-02-23 18:23:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.799: INFO: 	Container coredns ready: true, restart count 1
Mar  5 08:40:38.799: INFO: hcloud-cloud-controller-manager-7c5d46cc54-rhlq7 from kube-system started at 2021-02-23 18:23:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.799: INFO: 	Container hcloud-cloud-controller-manager ready: true, restart count 2
Mar  5 08:40:38.799: INFO: ks-events-operator-8dbf7fccc-v8zdm from kubesphere-logging-system started at 2021-02-24 13:22:25 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.799: INFO: 	Container events-operator ready: true, restart count 1
Mar  5 08:40:38.799: INFO: kube-proxy-vkr26 from kube-system started at 2021-02-23 18:21:19 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.799: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:40:38.799: INFO: reviews-v1-549f6b9d47-rttzb from serv-mesh-ex started at 2021-03-01 10:23:35 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.799: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:40:38.799: INFO: 	Container reviews ready: true, restart count 1
Mar  5 08:40:38.799: INFO: ks-controller-manager-8556448648-dk5xc from kubesphere-system started at 2021-03-04 18:53:58 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.799: INFO: 	Container ks-controller-manager ready: true, restart count 0
Mar  5 08:40:38.799: INFO: node-local-dns-4wxxm from kube-system started at 2021-02-23 18:22:39 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.799: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:40:38.799: INFO: 
Logging pods the kubelet thinks is on node devops-control-plane-3 before test
Mar  5 08:40:38.836: INFO: machine-controller-74bbdbb8b8-7r4nj from kube-system started at 2021-02-23 18:23:35 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container machine-controller ready: true, restart count 27
Mar  5 08:40:38.836: INFO: minio-7bfdb5968b-cnd55 from kubesphere-system started at 2021-02-24 14:10:33 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container minio ready: true, restart count 1
Mar  5 08:40:38.836: INFO: thanos-ruler-kubesphere-1 from kubesphere-monitoring-system started at 2021-03-04 07:58:11 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container rules-configmap-reloader ready: true, restart count 1
Mar  5 08:40:38.836: INFO: 	Container thanos-ruler ready: true, restart count 1
Mar  5 08:40:38.836: INFO: ks-apiserver-5894746f6b-8p8vg from kubesphere-system started at 2021-03-04 18:54:02 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container ks-apiserver ready: true, restart count 0
Mar  5 08:40:38.836: INFO: rook-ceph-osd-prepare-devops-control-plane-3-q6vvm from rook-ceph started at 2021-03-05 08:21:05 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container provision ready: false, restart count 0
Mar  5 08:40:38.836: INFO: gogs-gogs-test-ssh from proteus started at 2021-03-03 07:35:46 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container wget ready: false, restart count 0
Mar  5 08:40:38.836: INFO: kube-proxy-nrg4j from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:40:38.836: INFO: csi-cephfsplugin-287cb from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:40:38.836: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:40:38.836: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:40:38.836: INFO: ks-console-fb7f5895-5pmpn from kubesphere-system started at 2021-02-24 13:04:07 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container ks-console ready: true, restart count 1
Mar  5 08:40:38.836: INFO: csi-rbdplugin-provisioner-5bc97cdbb9-zjbws from rook-ceph started at 2021-03-04 08:15:54 +0000 UTC (6 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container csi-attacher ready: true, restart count 1
Mar  5 08:40:38.836: INFO: 	Container csi-provisioner ready: true, restart count 1
Mar  5 08:40:38.836: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:40:38.836: INFO: 	Container csi-resizer ready: true, restart count 1
Mar  5 08:40:38.836: INFO: 	Container csi-snapshotter ready: true, restart count 1
Mar  5 08:40:38.836: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:40:38.836: INFO: kube-controller-manager-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container kube-controller-manager ready: true, restart count 11
Mar  5 08:40:38.836: INFO: kube-scheduler-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container kube-scheduler ready: true, restart count 13
Mar  5 08:40:38.836: INFO: fluentbit-operator-7c56d66dd-7b4df from kubesphere-logging-system started at 2021-02-24 13:20:01 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container fluentbit-operator ready: true, restart count 2
Mar  5 08:40:38.836: INFO: istio-ingressgateway-76df6567c6-tndjv from istio-system started at 2021-03-01 07:44:17 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:40:38.836: INFO: ks-controller-manager-8556448648-c4spd from kubesphere-system started at 2021-03-04 18:54:03 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container ks-controller-manager ready: true, restart count 0
Mar  5 08:40:38.836: INFO: machine-controller-webhook-747c599b5c-k7xw4 from kube-system started at 2021-02-23 18:23:36 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container machine-controller-webhook ready: true, restart count 1
Mar  5 08:40:38.836: INFO: notification-manager-deployment-7c8df68d94-5dv72 from kubesphere-monitoring-system started at 2021-02-24 13:05:51 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container notification-manager ready: true, restart count 1
Mar  5 08:40:38.836: INFO: jaeger-query-b546558bf-frn98 from istio-system started at 2021-03-01 07:45:14 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container jaeger-agent ready: true, restart count 1
Mar  5 08:40:38.836: INFO: 	Container jaeger-query ready: true, restart count 1
Mar  5 08:40:38.836: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2021-03-04 17:34:29 +0000 UTC (3 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container prometheus ready: true, restart count 1
Mar  5 08:40:38.836: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  5 08:40:38.836: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  5 08:40:38.836: INFO: notification-db-ctrl-job-7kzd5 from kubesphere-alerting-system started at 2021-03-03 17:34:15 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container notification-db-ctrl ready: false, restart count 0
Mar  5 08:40:38.836: INFO: without-dockerfile-v1-7dc7678956-mh5vw from serv-mesh-ex started at 2021-03-04 08:15:54 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container container-2a87d6 ready: true, restart count 1
Mar  5 08:40:38.836: INFO: node-local-dns-twq6t from kube-system started at 2021-02-23 18:22:38 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:40:38.836: INFO: rook-ceph-crashcollector-devops-control-plane-3-587c5dc7b8mt27z from rook-ceph started at 2021-02-23 18:34:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.836: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:40:38.837: INFO: redis-ha-server-0 from kubesphere-system started at 2021-02-24 13:03:18 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container redis ready: true, restart count 1
Mar  5 08:40:38.837: INFO: 	Container sentinel ready: true, restart count 1
Mar  5 08:40:38.837: INFO: fluent-bit-s284n from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:40:38.837: INFO: kube-auditing-operator-6ddc8db4b-vfxqc from kubesphere-logging-system started at 2021-02-24 13:34:03 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container kube-auditing-operator ready: true, restart count 1
Mar  5 08:40:38.837: INFO: notification-db-init-job-mqs2r from kubesphere-alerting-system started at 2021-03-03 17:34:05 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container notification-db-init ready: false, restart count 0
Mar  5 08:40:38.837: INFO: etcd-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:36 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container etcd ready: true, restart count 1
Mar  5 08:40:38.837: INFO: redis-ha-haproxy-5c6559d588-qhqhx from kubesphere-system started at 2021-02-24 13:03:15 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container haproxy ready: true, restart count 1
Mar  5 08:40:38.837: INFO: logsidecar-injector-deploy-74c66bfd85-62xbg from kubesphere-logging-system started at 2021-02-24 13:22:19 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:40:38.837: INFO: 	Container logsidecar-injector ready: true, restart count 1
Mar  5 08:40:38.837: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-wp8ts from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:40:38.837: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:40:38.837: INFO: kiali-operator-58b8765b9c-6qh7r from istio-system started at 2021-03-01 07:44:57 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container operator ready: true, restart count 1
Mar  5 08:40:38.837: INFO: kube-apiserver-devops-control-plane-3 from kube-system started at 2021-02-23 18:22:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container kube-apiserver ready: true, restart count 1
Mar  5 08:40:38.837: INFO: csi-rbdplugin-7zr2t from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:40:38.837: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:40:38.837: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:40:38.837: INFO: node-exporter-6fsk8 from kubesphere-monitoring-system started at 2021-03-04 17:34:28 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:40:38.837: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:40:38.837: INFO: rook-ceph-osd-1-845f4b4486-5phnd from rook-ceph started at 2021-02-23 18:34:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container osd ready: true, restart count 1
Mar  5 08:40:38.837: INFO: openldap-0 from kubesphere-system started at 2021-02-24 13:03:27 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container openldap-ha ready: true, restart count 1
Mar  5 08:40:38.837: INFO: gogs-gogs-7467fdcf8f-rw58p from default started at 2021-03-05 08:23:09 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container gogs ready: true, restart count 0
Mar  5 08:40:38.837: INFO: calico-kube-controllers-589975f454-whrcl from kube-system started at 2021-02-23 18:23:24 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container calico-kube-controllers ready: true, restart count 1
Mar  5 08:40:38.837: INFO: canal-r57rn from kube-system started at 2021-02-23 18:23:39 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:40:38.837: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:40:38.837: INFO: ks-events-ruler-698b7899c7-rjqqv from kubesphere-logging-system started at 2021-02-24 13:22:32 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.837: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:40:38.837: INFO: 	Container events-ruler ready: true, restart count 1
Mar  5 08:40:38.837: INFO: 
Logging pods the kubelet thinks is on node devops-pool1-6c76d44df9-8tgs6 before test
Mar  5 08:40:38.867: INFO: frontend-79b4847d65-rbq88 from kubectl-3612 started at 2021-03-05 08:40:31 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container guestbook-frontend ready: true, restart count 0
Mar  5 08:40:38.868: INFO: gogs-postgresql-0 from default started at 2021-03-05 08:23:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container gogs-postgresql ready: true, restart count 0
Mar  5 08:40:38.868: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-9ffgc from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:40:38.868: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:40:38.868: INFO: canal-rm2l9 from kube-system started at 2021-02-23 18:25:59 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container calico-node ready: true, restart count 8
Mar  5 08:40:38.868: INFO: 	Container kube-flannel ready: true, restart count 8
Mar  5 08:40:38.868: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2021-03-05 08:23:53 +0000 UTC (3 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container prometheus ready: true, restart count 1
Mar  5 08:40:38.868: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar  5 08:40:38.868: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  5 08:40:38.868: INFO: elasticsearch-logging-discovery-1 from kubesphere-logging-system started at 2021-03-05 08:23:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  5 08:40:38.868: INFO: sonobuoy from sonobuoy started at 2021-03-05 07:46:37 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  5 08:40:38.868: INFO: frontend-79b4847d65-lq965 from kubectl-3612 started at 2021-03-05 08:40:31 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container guestbook-frontend ready: true, restart count 0
Mar  5 08:40:38.868: INFO: agnhost-slave-85bc7468c4-9wt2f from kubectl-3612 started at 2021-03-05 08:40:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container slave ready: true, restart count 0
Mar  5 08:40:38.868: INFO: node-local-dns-kh572 from kube-system started at 2021-02-23 18:25:59 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container node-cache ready: true, restart count 8
Mar  5 08:40:38.868: INFO: node-exporter-824lt from kubesphere-monitoring-system started at 2021-03-04 17:34:42 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:40:38.868: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:40:38.868: INFO: kube-proxy-jxx7v from kube-system started at 2021-02-23 18:25:59 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container kube-proxy ready: true, restart count 8
Mar  5 08:40:38.868: INFO: fluent-bit-2vc49 from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container fluent-bit ready: true, restart count 8
Mar  5 08:40:38.868: INFO: frontend-79b4847d65-cp5wz from kubectl-3612 started at 2021-03-05 08:40:31 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container guestbook-frontend ready: true, restart count 0
Mar  5 08:40:38.868: INFO: agnhost-master-b88576bc-d2jhp from kubectl-3612 started at 2021-03-05 08:40:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container master ready: true, restart count 0
Mar  5 08:40:38.868: INFO: agnhost-slave-85bc7468c4-fwdgt from kubectl-3612 started at 2021-03-05 08:40:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container slave ready: true, restart count 0
Mar  5 08:40:38.868: INFO: csi-cephfsplugin-crjw8 from rook-ceph started at 2021-03-05 08:23:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Mar  5 08:40:38.868: INFO: 	Container driver-registrar ready: true, restart count 0
Mar  5 08:40:38.868: INFO: 	Container liveness-prometheus ready: true, restart count 0
Mar  5 08:40:38.868: INFO: elasticsearch-logging-data-2 from kubesphere-logging-system started at 2021-03-05 08:25:23 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container elasticsearch ready: true, restart count 0
Mar  5 08:40:38.868: INFO: csi-rbdplugin-7hr88 from rook-ceph started at 2021-03-05 08:23:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:40:38.868: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Mar  5 08:40:38.868: INFO: 	Container driver-registrar ready: true, restart count 0
Mar  5 08:40:38.868: INFO: 	Container liveness-prometheus ready: true, restart count 0
Mar  5 08:40:38.868: INFO: 
Logging pods the kubelet thinks is on node devops-pool1-6c76d44df9-dwtv7 before test
Mar  5 08:40:38.900: INFO: elasticsearch-logging-data-1 from kubesphere-logging-system started at 2021-02-24 13:20:42 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 08:40:38.900: INFO: ks-controller-manager-8556448648-cbt26 from kubesphere-system started at 2021-03-04 18:53:55 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container ks-controller-manager ready: true, restart count 1
Mar  5 08:40:38.900: INFO: enginsgungor-hello-world-latest-ug1-ejy-f941edb3103d-job-pr5nr from serv-mesh-ex started at 2021-03-01 18:50:41 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:40:38.900: INFO: elasticsearch-logging-curator-elasticsearch-curator-161481wvfqt from kubesphere-logging-system started at 2021-03-04 01:00:01 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Mar  5 08:40:38.900: INFO: node-exporter-f6kh5 from kubesphere-monitoring-system started at 2021-03-04 17:34:32 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:40:38.900: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:40:38.900: INFO: sonobuoy-e2e-job-41e333abf2c348c1 from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container e2e ready: true, restart count 0
Mar  5 08:40:38.900: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:40:38.900: INFO: ks-sample-dev-5c5d97c6cc-64hxc from kubesphere-offline-dev started at 2021-03-05 08:23:09 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container js-sample ready: true, restart count 0
Mar  5 08:40:38.900: INFO: rook-ceph-mgr-a-55fb9d48b6-clq94 from rook-ceph started at 2021-02-23 18:34:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container mgr ready: true, restart count 2
Mar  5 08:40:38.900: INFO: kube-proxy-2czsl from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:40:38.900: INFO: csi-cephfsplugin-provisioner-c68f789b8-k89fp from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (6 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container csi-attacher ready: true, restart count 14
Mar  5 08:40:38.900: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:40:38.900: INFO: 	Container csi-provisioner ready: true, restart count 14
Mar  5 08:40:38.900: INFO: 	Container csi-resizer ready: true, restart count 12
Mar  5 08:40:38.900: INFO: 	Container csi-snapshotter ready: true, restart count 13
Mar  5 08:40:38.900: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:40:38.900: INFO: ks-apiserver-5894746f6b-2dq28 from kubesphere-system started at 2021-03-04 18:53:54 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container ks-apiserver ready: true, restart count 0
Mar  5 08:40:38.900: INFO: ks-console-fb7f5895-vvxbg from kubesphere-system started at 2021-02-25 07:43:56 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container ks-console ready: true, restart count 1
Mar  5 08:40:38.900: INFO: tutorial-deployment-v1-5549b8b9d8-t8t9z from tutorial-s2i started at 2021-03-02 18:10:32 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container container-t1u32h ready: true, restart count 1
Mar  5 08:40:38.900: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-f7ttl from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:40:38.900: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:40:38.900: INFO: snapshot-controller-0 from kube-system started at 2021-03-05 08:23:14 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  5 08:40:38.900: INFO: rook-ceph-mon-a-85cd5968bc-hq6sb from rook-ceph started at 2021-02-23 18:33:25 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container mon ready: true, restart count 1
Mar  5 08:40:38.900: INFO: alertmanager-main-1 from kubesphere-monitoring-system started at 2021-02-24 13:05:28 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container alertmanager ready: true, restart count 1
Mar  5 08:40:38.900: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:40:38.900: INFO: elasticsearch-logging-discovery-2 from kubesphere-logging-system started at 2021-02-24 13:21:07 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 08:40:38.900: INFO: thanos-ruler-k8s-1 from kubesphere-monitoring-system started at 2021-03-05 08:23:19 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar  5 08:40:38.900: INFO: 	Container thanos-ruler ready: true, restart count 0
Mar  5 08:40:38.900: INFO: canal-mfdhb from kube-system started at 2021-02-23 18:26:00 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:40:38.900: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:40:38.900: INFO: node-local-dns-8s8tk from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:40:38.900: INFO: rook-ceph-crashcollector-devops-pool1-6c76d44df9-dwtv7-7dfdvvgh from rook-ceph started at 2021-02-23 18:34:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:40:38.900: INFO: rook-ceph-operator-6fb9f456fc-8g9z2 from rook-ceph started at 2021-02-23 18:29:52 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container rook-ceph-operator ready: true, restart count 1
Mar  5 08:40:38.900: INFO: csi-rbdplugin-nhvm9 from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:40:38.900: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:40:38.900: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:40:38.900: INFO: enginsgungor-hello-deployment-latest--0dq-14b4f67d0b7e-jobptskv from serv-mesh-ex started at 2021-03-01 19:28:59 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:40:38.900: INFO: ks-sample-dev-7bb944b987-hnlc4 from kubesphere-sample-dev started at 2021-03-03 11:03:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container js-sample ready: true, restart count 1
Mar  5 08:40:38.900: INFO: notification-manager-operator-6958786cd6-26xxr from kubesphere-monitoring-system started at 2021-03-04 08:15:54 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Mar  5 08:40:38.900: INFO: 	Container notification-manager-operator ready: true, restart count 2
Mar  5 08:40:38.900: INFO: openpitrix-hyperpitrix-deployment-6d48d87c6c-z7vww from openpitrix-system started at 2021-03-05 08:23:09 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container hyperpitrix ready: true, restart count 0
Mar  5 08:40:38.900: INFO: kube-auditing-webhook-deploy-b74bfb885-qf48j from kubesphere-logging-system started at 2021-02-24 13:34:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container kube-auditing-webhook ready: true, restart count 1
Mar  5 08:40:38.900: INFO: kiali-5fc8bfd66b-wjrqr from istio-system started at 2021-03-01 07:45:58 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container kiali ready: true, restart count 1
Mar  5 08:40:38.900: INFO: harbor-da5q24-harbor-redis-0 from pipeline-tools started at 2021-03-04 10:01:12 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container redis ready: true, restart count 1
Mar  5 08:40:38.900: INFO: csi-cephfsplugin-nxcmz from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:40:38.900: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:40:38.900: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:40:38.900: INFO: fluent-bit-s42x9 from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:40:38.900: INFO: hyperpitrix-release-app-job-tqln9 from openpitrix-system started at 2021-03-04 18:51:28 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container hyperpitrix-release-app-job ready: false, restart count 0
Mar  5 08:40:38.900: INFO: enginsgungor-hello-deployment-latest--6eg-3afd31aa2f90-jobgl2pn from serv-mesh-ex started at 2021-03-01 19:15:53 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:40:38.900: INFO: enginsgungor-tutorial-deployment-late-9pz-042c363e1509-job598vw from tutorial-s2i started at 2021-03-02 18:10:13 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.900: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:40:38.900: INFO: 
Logging pods the kubelet thinks is on node devops-pool1-6c76d44df9-njb8n before test
Mar  5 08:40:38.942: INFO: canal-tp2fn from kube-system started at 2021-02-23 18:26:00 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container calico-node ready: true, restart count 1
Mar  5 08:40:38.942: INFO: 	Container kube-flannel ready: true, restart count 1
Mar  5 08:40:38.942: INFO: elasticsearch-logging-data-0 from kubesphere-logging-system started at 2021-02-24 13:19:49 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 08:40:38.942: INFO: csi-rbdplugin-5n26w from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (3 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:40:38.942: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:40:38.942: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:40:38.942: INFO: kube-state-metrics-95c974544-drf59 from kubesphere-monitoring-system started at 2021-02-24 13:05:20 +0000 UTC (3 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 1
Mar  5 08:40:38.942: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 1
Mar  5 08:40:38.942: INFO: 	Container kube-state-metrics ready: true, restart count 1
Mar  5 08:40:38.942: INFO: hellojs-v1-5f4d98c7df-snl82 from serv-mesh-ex started at 2021-03-02 06:09:42 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container container-3nysic ready: true, restart count 1
Mar  5 08:40:38.942: INFO: kubectl-admin-5d98567c78-m72x5 from kubesphere-controls-system started at 2021-02-24 13:06:05 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container kubectl ready: true, restart count 1
Mar  5 08:40:38.942: INFO: rook-ceph-mon-c-764d6d7649-wcbn5 from rook-ceph started at 2021-02-23 18:33:46 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container mon ready: true, restart count 1
Mar  5 08:40:38.942: INFO: enginsgungor-hello-deployment-latest--fsk-bb68efbac665-jobm9zxh from serv-mesh-ex started at 2021-03-02 06:09:25 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:40:38.942: INFO: node-exporter-fhq4b from kubesphere-monitoring-system started at 2021-03-04 17:34:24 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  5 08:40:38.942: INFO: 	Container node-exporter ready: true, restart count 0
Mar  5 08:40:38.942: INFO: productpage-v1-795bd6db76-trctn from serv-mesh-ex started at 2021-03-04 08:15:54 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:40:38.942: INFO: 	Container productpage ready: true, restart count 1
Mar  5 08:40:38.942: INFO: alertmanager-main-2 from kubesphere-monitoring-system started at 2021-02-24 13:05:28 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container alertmanager ready: true, restart count 1
Mar  5 08:40:38.942: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:40:38.942: INFO: elasticsearch-logging-discovery-0 from kubesphere-logging-system started at 2021-02-24 13:19:50 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container elasticsearch ready: true, restart count 1
Mar  5 08:40:38.942: INFO: fluent-bit-cqbf6 from kubesphere-logging-system started at 2021-02-24 13:20:20 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container fluent-bit ready: true, restart count 1
Mar  5 08:40:38.942: INFO: ks-jenkins-5bf5cbf449-tzqjz from kubesphere-devops-system started at 2021-02-28 09:58:29 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container ks-jenkins ready: true, restart count 1
Mar  5 08:40:38.942: INFO: enginsgungor-hello-deployment-latest--cnc-39caa4291750-jobgnx27 from serv-mesh-ex started at 2021-03-01 19:24:09 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container s2irun ready: false, restart count 0
Mar  5 08:40:38.942: INFO: csi-rbdplugin-provisioner-5bc97cdbb9-nlb2z from rook-ceph started at 2021-02-23 18:32:35 +0000 UTC (6 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container csi-attacher ready: true, restart count 21
Mar  5 08:40:38.942: INFO: 	Container csi-provisioner ready: true, restart count 12
Mar  5 08:40:38.942: INFO: 	Container csi-rbdplugin ready: true, restart count 1
Mar  5 08:40:38.942: INFO: 	Container csi-resizer ready: true, restart count 11
Mar  5 08:40:38.942: INFO: 	Container csi-snapshotter ready: true, restart count 13
Mar  5 08:40:38.942: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:40:38.942: INFO: ks-events-exporter-5bc4d9f496-zm5tx from kubesphere-logging-system started at 2021-02-24 13:22:33 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container config-reloader ready: true, restart count 1
Mar  5 08:40:38.942: INFO: 	Container events-exporter ready: true, restart count 1
Mar  5 08:40:38.942: INFO: kubesphere-router-serv-mesh-ex-585558c74b-45mf8 from kubesphere-controls-system started at 2021-03-01 08:45:33 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container istio-proxy ready: true, restart count 1
Mar  5 08:40:38.942: INFO: 	Container nginx-ingress-controller ready: true, restart count 1
Mar  5 08:40:38.942: INFO: sonobuoy-systemd-logs-daemon-set-1d3c60ac6e4f409d-xprqf from sonobuoy started at 2021-03-05 07:46:46 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  5 08:40:38.942: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  5 08:40:38.942: INFO: node-local-dns-5qx5m from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container node-cache ready: true, restart count 1
Mar  5 08:40:38.942: INFO: kube-proxy-bpngj from kube-system started at 2021-02-23 18:26:00 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container kube-proxy ready: true, restart count 1
Mar  5 08:40:38.942: INFO: default-http-backend-857d7b6856-gt6hh from kubesphere-controls-system started at 2021-02-24 13:03:47 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container default-http-backend ready: true, restart count 1
Mar  5 08:40:38.942: INFO: rook-ceph-tools-64bd84c8b5-hd7mk from rook-ceph started at 2021-02-25 07:43:01 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container rook-ceph-tools ready: true, restart count 1
Mar  5 08:40:38.942: INFO: csi-cephfsplugin-t7l65 from rook-ceph started at 2021-02-23 18:32:36 +0000 UTC (3 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container csi-cephfsplugin ready: true, restart count 1
Mar  5 08:40:38.942: INFO: 	Container driver-registrar ready: true, restart count 1
Mar  5 08:40:38.942: INFO: 	Container liveness-prometheus ready: true, restart count 1
Mar  5 08:40:38.942: INFO: rook-ceph-crashcollector-devops-pool1-6c76d44df9-njb8n-cfbd6xbn from rook-ceph started at 2021-02-23 18:33:46 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container ceph-crash ready: true, restart count 1
Mar  5 08:40:38.942: INFO: prometheus-operator-84d58bf775-q2drc from kubesphere-monitoring-system started at 2021-02-24 13:05:18 +0000 UTC (2 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Mar  5 08:40:38.942: INFO: 	Container prometheus-operator ready: true, restart count 1
Mar  5 08:40:38.942: INFO: notification-manager-deployment-7c8df68d94-7vd2s from kubesphere-monitoring-system started at 2021-02-24 13:05:51 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container notification-manager ready: true, restart count 1
Mar  5 08:40:38.942: INFO: mysql-7f64d9f584-49fnb from kubesphere-system started at 2021-02-24 13:19:43 +0000 UTC (1 container statuses recorded)
Mar  5 08:40:38.942: INFO: 	Container mysql ready: true, restart count 1
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16696596a5ede1e5], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16696596a75ee044], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:40:40.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7281" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":277,"completed":208,"skipped":3646,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:40:40.114: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1027
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:40:40.331: INFO: (0) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 12.812809ms)
Mar  5 08:40:40.340: INFO: (1) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.590103ms)
Mar  5 08:40:40.350: INFO: (2) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 10.13338ms)
Mar  5 08:40:40.359: INFO: (3) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 9.574037ms)
Mar  5 08:40:40.369: INFO: (4) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 9.182592ms)
Mar  5 08:40:40.380: INFO: (5) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 11.137571ms)
Mar  5 08:40:40.388: INFO: (6) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.894409ms)
Mar  5 08:40:40.397: INFO: (7) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.837892ms)
Mar  5 08:40:40.405: INFO: (8) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.106446ms)
Mar  5 08:40:40.414: INFO: (9) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.931539ms)
Mar  5 08:40:40.422: INFO: (10) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.28786ms)
Mar  5 08:40:40.431: INFO: (11) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.198599ms)
Mar  5 08:40:40.439: INFO: (12) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.155938ms)
Mar  5 08:40:40.448: INFO: (13) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.882214ms)
Mar  5 08:40:40.467: INFO: (14) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 18.764048ms)
Mar  5 08:40:40.477: INFO: (15) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 10.119034ms)
Mar  5 08:40:40.486: INFO: (16) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.828712ms)
Mar  5 08:40:40.493: INFO: (17) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.221451ms)
Mar  5 08:40:40.501: INFO: (18) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.501939ms)
Mar  5 08:40:40.509: INFO: (19) /api/v1/nodes/devops-pool1-6c76d44df9-njb8n:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.853755ms)
[AfterEach] version v1
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:40:40.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1027" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":277,"completed":209,"skipped":3668,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:40:40.533: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8579
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-8579/configmap-test-4bd2904a-540a-4b20-8f13-ee2da4ec6d3f
STEP: Creating a pod to test consume configMaps
Mar  5 08:40:40.745: INFO: Waiting up to 5m0s for pod "pod-configmaps-94401c23-87b8-48f5-8905-c9ecd1a20e67" in namespace "configmap-8579" to be "Succeeded or Failed"
Mar  5 08:40:40.753: INFO: Pod "pod-configmaps-94401c23-87b8-48f5-8905-c9ecd1a20e67": Phase="Pending", Reason="", readiness=false. Elapsed: 8.029971ms
Mar  5 08:40:42.760: INFO: Pod "pod-configmaps-94401c23-87b8-48f5-8905-c9ecd1a20e67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014806609s
Mar  5 08:40:44.766: INFO: Pod "pod-configmaps-94401c23-87b8-48f5-8905-c9ecd1a20e67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021587846s
Mar  5 08:40:46.772: INFO: Pod "pod-configmaps-94401c23-87b8-48f5-8905-c9ecd1a20e67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027519465s
STEP: Saw pod success
Mar  5 08:40:46.772: INFO: Pod "pod-configmaps-94401c23-87b8-48f5-8905-c9ecd1a20e67" satisfied condition "Succeeded or Failed"
Mar  5 08:40:46.778: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-configmaps-94401c23-87b8-48f5-8905-c9ecd1a20e67 container env-test: <nil>
STEP: delete the pod
Mar  5 08:40:46.846: INFO: Waiting for pod pod-configmaps-94401c23-87b8-48f5-8905-c9ecd1a20e67 to disappear
Mar  5 08:40:46.857: INFO: Pod pod-configmaps-94401c23-87b8-48f5-8905-c9ecd1a20e67 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:40:46.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8579" for this suite.

â€¢ [SLOW TEST:6.345 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":277,"completed":210,"skipped":3710,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:40:46.881: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7931
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-4a707461-6874-45a1-9915-85cfdd1191ab
STEP: Creating a pod to test consume configMaps
Mar  5 08:40:47.101: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f39f652d-1e2c-4f54-9f93-66dfbac9fa78" in namespace "projected-7931" to be "Succeeded or Failed"
Mar  5 08:40:47.108: INFO: Pod "pod-projected-configmaps-f39f652d-1e2c-4f54-9f93-66dfbac9fa78": Phase="Pending", Reason="", readiness=false. Elapsed: 7.273407ms
Mar  5 08:40:49.116: INFO: Pod "pod-projected-configmaps-f39f652d-1e2c-4f54-9f93-66dfbac9fa78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01467816s
STEP: Saw pod success
Mar  5 08:40:49.116: INFO: Pod "pod-projected-configmaps-f39f652d-1e2c-4f54-9f93-66dfbac9fa78" satisfied condition "Succeeded or Failed"
Mar  5 08:40:49.122: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-projected-configmaps-f39f652d-1e2c-4f54-9f93-66dfbac9fa78 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 08:40:49.178: INFO: Waiting for pod pod-projected-configmaps-f39f652d-1e2c-4f54-9f93-66dfbac9fa78 to disappear
Mar  5 08:40:49.184: INFO: Pod pod-projected-configmaps-f39f652d-1e2c-4f54-9f93-66dfbac9fa78 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:40:49.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7931" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":277,"completed":211,"skipped":3710,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:40:49.209: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2687
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:40:49.494: INFO: Create a RollingUpdate DaemonSet
Mar  5 08:40:49.506: INFO: Check that daemon pods launch on every node of the cluster
Mar  5 08:40:49.525: INFO: Number of nodes with available pods: 0
Mar  5 08:40:49.525: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 08:40:50.557: INFO: Number of nodes with available pods: 0
Mar  5 08:40:50.557: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 08:40:51.545: INFO: Number of nodes with available pods: 2
Mar  5 08:40:51.545: INFO: Node devops-control-plane-2 is running more than one daemon pod
Mar  5 08:40:52.547: INFO: Number of nodes with available pods: 6
Mar  5 08:40:52.547: INFO: Number of running nodes: 6, number of available pods: 6
Mar  5 08:40:52.547: INFO: Update the DaemonSet to trigger a rollout
Mar  5 08:40:52.562: INFO: Updating DaemonSet daemon-set
Mar  5 08:40:56.604: INFO: Roll back the DaemonSet before rollout is complete
Mar  5 08:40:56.622: INFO: Updating DaemonSet daemon-set
Mar  5 08:40:56.622: INFO: Make sure DaemonSet rollback is complete
Mar  5 08:40:56.632: INFO: Wrong image for pod: daemon-set-hh4td. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  5 08:40:56.632: INFO: Pod daemon-set-hh4td is not available
Mar  5 08:40:57.652: INFO: Wrong image for pod: daemon-set-hh4td. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  5 08:40:57.652: INFO: Pod daemon-set-hh4td is not available
Mar  5 08:40:58.652: INFO: Pod daemon-set-74w5h is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2687, will wait for the garbage collector to delete the pods
Mar  5 08:40:58.775: INFO: Deleting DaemonSet.extensions daemon-set took: 16.644818ms
Mar  5 08:41:01.176: INFO: Terminating DaemonSet.extensions daemon-set pods took: 2.401436479s
Mar  5 08:41:15.184: INFO: Number of nodes with available pods: 0
Mar  5 08:41:15.184: INFO: Number of running nodes: 0, number of available pods: 0
Mar  5 08:41:15.190: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2687/daemonsets","resourceVersion":"5642135"},"items":null}

Mar  5 08:41:15.196: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2687/pods","resourceVersion":"5642135"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:41:15.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2687" for this suite.

â€¢ [SLOW TEST:26.060 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":277,"completed":212,"skipped":3719,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:41:15.273: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8244
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:41:15.499: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  5 08:41:15.517: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  5 08:41:20.525: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  5 08:41:20.525: INFO: Creating deployment "test-rolling-update-deployment"
Mar  5 08:41:20.535: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  5 08:41:20.549: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  5 08:41:22.561: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  5 08:41:22.568: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530480, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530480, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530480, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530480, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-59d5cb45c7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  5 08:41:24.583: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar  5 08:41:24.605: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8244 /apis/apps/v1/namespaces/deployment-8244/deployments/test-rolling-update-deployment 498ecb62-8a9a-4645-8bf1-04ffcd5cf60f 5642303 1 2021-03-05 08:41:20 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-03-05 08:41:20 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2021-03-05 08:41:22 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0064aa898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-05 08:41:20 +0000 UTC,LastTransitionTime:2021-03-05 08:41:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-59d5cb45c7" has successfully progressed.,LastUpdateTime:2021-03-05 08:41:22 +0000 UTC,LastTransitionTime:2021-03-05 08:41:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  5 08:41:24.614: INFO: New ReplicaSet "test-rolling-update-deployment-59d5cb45c7" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7  deployment-8244 /apis/apps/v1/namespaces/deployment-8244/replicasets/test-rolling-update-deployment-59d5cb45c7 b1fbc291-ce4f-411b-9cb5-588463f18f4b 5642291 1 2021-03-05 08:41:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 498ecb62-8a9a-4645-8bf1-04ffcd5cf60f 0xc004251747 0xc004251748}] []  [{kube-controller-manager Update apps/v1 2021-03-05 08:41:22 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 57 56 101 99 98 54 50 45 56 97 57 97 45 52 54 52 53 45 56 98 102 49 45 48 52 102 102 99 100 53 99 102 54 48 102 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 59d5cb45c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0042517d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  5 08:41:24.614: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  5 08:41:24.615: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8244 /apis/apps/v1/namespaces/deployment-8244/replicasets/test-rolling-update-controller 8224ad7e-d103-4a90-a17b-b4065089c30d 5642302 2 2021-03-05 08:41:15 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 498ecb62-8a9a-4645-8bf1-04ffcd5cf60f 0xc00425162f 0xc004251640}] []  [{e2e.test Update apps/v1 2021-03-05 08:41:15 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2021-03-05 08:41:22 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 57 56 101 99 98 54 50 45 56 97 57 97 45 52 54 52 53 45 56 98 102 49 45 48 52 102 102 99 100 53 99 102 54 48 102 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0042516d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  5 08:41:24.621: INFO: Pod "test-rolling-update-deployment-59d5cb45c7-9k49z" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7-9k49z test-rolling-update-deployment-59d5cb45c7- deployment-8244 /api/v1/namespaces/deployment-8244/pods/test-rolling-update-deployment-59d5cb45c7-9k49z b1efcfaf-88a2-4bbf-a087-10766c1a236e 5642290 0 2021-03-05 08:41:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[cni.projectcalico.org/podIP:10.244.3.39/32 cni.projectcalico.org/podIPs:10.244.3.39/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-59d5cb45c7 b1fbc291-ce4f-411b-9cb5-588463f18f4b 0xc004602987 0xc004602988}] []  [{kube-controller-manager Update v1 2021-03-05 08:41:20 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 49 102 98 99 50 57 49 45 99 101 52 102 45 52 49 49 98 45 57 99 98 53 45 53 56 56 52 54 51 102 49 56 102 52 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2021-03-05 08:41:21 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2021-03-05 08:41:22 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 50 52 52 46 51 46 51 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-w9tm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-w9tm6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-w9tm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:devops-pool1-6c76d44df9-8tgs6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 08:41:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 08:41:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 08:41:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-05 08:41:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.8,PodIP:10.244.3.39,StartTime:2021-03-05 08:41:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-05 08:41:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://f5447030676d29b603ec283e41b09f5ba3cb3a496f433f5ba3c5494d95184675,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.39,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:41:24.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8244" for this suite.

â€¢ [SLOW TEST:9.381 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":277,"completed":213,"skipped":3736,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:41:24.656: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2431
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-3d5627a7-e607-4fe9-b1c5-256ebbf62ec5
STEP: Creating a pod to test consume configMaps
Mar  5 08:41:24.900: INFO: Waiting up to 5m0s for pod "pod-configmaps-d7b5302d-b83f-4c84-90c4-884af9bdd9ea" in namespace "configmap-2431" to be "Succeeded or Failed"
Mar  5 08:41:24.905: INFO: Pod "pod-configmaps-d7b5302d-b83f-4c84-90c4-884af9bdd9ea": Phase="Pending", Reason="", readiness=false. Elapsed: 5.112804ms
Mar  5 08:41:26.913: INFO: Pod "pod-configmaps-d7b5302d-b83f-4c84-90c4-884af9bdd9ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012794537s
Mar  5 08:41:28.921: INFO: Pod "pod-configmaps-d7b5302d-b83f-4c84-90c4-884af9bdd9ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021573088s
STEP: Saw pod success
Mar  5 08:41:28.922: INFO: Pod "pod-configmaps-d7b5302d-b83f-4c84-90c4-884af9bdd9ea" satisfied condition "Succeeded or Failed"
Mar  5 08:41:28.927: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-configmaps-d7b5302d-b83f-4c84-90c4-884af9bdd9ea container configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 08:41:28.986: INFO: Waiting for pod pod-configmaps-d7b5302d-b83f-4c84-90c4-884af9bdd9ea to disappear
Mar  5 08:41:28.993: INFO: Pod pod-configmaps-d7b5302d-b83f-4c84-90c4-884af9bdd9ea no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:41:28.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2431" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":214,"skipped":3738,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:41:29.014: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3675
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-741053db-126c-424c-972f-963ceceea9a6
STEP: Creating a pod to test consume secrets
Mar  5 08:41:29.234: INFO: Waiting up to 5m0s for pod "pod-secrets-8cae9cad-c428-4819-b70d-825497a06525" in namespace "secrets-3675" to be "Succeeded or Failed"
Mar  5 08:41:29.238: INFO: Pod "pod-secrets-8cae9cad-c428-4819-b70d-825497a06525": Phase="Pending", Reason="", readiness=false. Elapsed: 4.610253ms
Mar  5 08:41:31.257: INFO: Pod "pod-secrets-8cae9cad-c428-4819-b70d-825497a06525": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022917441s
STEP: Saw pod success
Mar  5 08:41:31.257: INFO: Pod "pod-secrets-8cae9cad-c428-4819-b70d-825497a06525" satisfied condition "Succeeded or Failed"
Mar  5 08:41:31.267: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-secrets-8cae9cad-c428-4819-b70d-825497a06525 container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 08:41:31.350: INFO: Waiting for pod pod-secrets-8cae9cad-c428-4819-b70d-825497a06525 to disappear
Mar  5 08:41:31.356: INFO: Pod pod-secrets-8cae9cad-c428-4819-b70d-825497a06525 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:41:31.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3675" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":277,"completed":215,"skipped":3740,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:41:31.384: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7954
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar  5 08:41:31.615: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7954 /api/v1/namespaces/watch-7954/configmaps/e2e-watch-test-resource-version e28e5c30-0a07-4259-9c8e-ee36cced6279 5642443 0 2021-03-05 08:41:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-03-05 08:41:31 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  5 08:41:31.617: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7954 /api/v1/namespaces/watch-7954/configmaps/e2e-watch-test-resource-version e28e5c30-0a07-4259-9c8e-ee36cced6279 5642444 0 2021-03-05 08:41:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-03-05 08:41:31 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:41:31.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7954" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":277,"completed":216,"skipped":3744,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:41:31.640: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6222
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Mar  5 08:41:31.877: INFO: namespace kubectl-6222
Mar  5 08:41:31.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 create -f - --namespace=kubectl-6222'
Mar  5 08:41:32.468: INFO: stderr: ""
Mar  5 08:41:32.468: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar  5 08:41:33.491: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  5 08:41:33.491: INFO: Found 0 / 1
Mar  5 08:41:34.477: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  5 08:41:34.477: INFO: Found 1 / 1
Mar  5 08:41:34.477: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  5 08:41:34.485: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  5 08:41:34.485: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  5 08:41:34.485: INFO: wait on agnhost-master startup in kubectl-6222 
Mar  5 08:41:34.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 logs agnhost-master-mbnpl agnhost-master --namespace=kubectl-6222'
Mar  5 08:41:34.646: INFO: stderr: ""
Mar  5 08:41:34.646: INFO: stdout: "Paused\n"
STEP: exposing RC
Mar  5 08:41:34.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-6222'
Mar  5 08:41:34.850: INFO: stderr: ""
Mar  5 08:41:34.850: INFO: stdout: "service/rm2 exposed\n"
Mar  5 08:41:34.856: INFO: Service rm2 in namespace kubectl-6222 found.
STEP: exposing service
Mar  5 08:41:36.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-6222'
Mar  5 08:41:37.054: INFO: stderr: ""
Mar  5 08:41:37.054: INFO: stdout: "service/rm3 exposed\n"
Mar  5 08:41:37.059: INFO: Service rm3 in namespace kubectl-6222 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:41:39.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6222" for this suite.

â€¢ [SLOW TEST:7.468 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1119
    should create services for rc  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":277,"completed":217,"skipped":3744,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:41:39.121: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9426
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar  5 08:41:39.337: INFO: Waiting up to 5m0s for pod "downward-api-fb4e0bb6-e0e3-4427-90f1-18f805101b39" in namespace "downward-api-9426" to be "Succeeded or Failed"
Mar  5 08:41:39.354: INFO: Pod "downward-api-fb4e0bb6-e0e3-4427-90f1-18f805101b39": Phase="Pending", Reason="", readiness=false. Elapsed: 16.238104ms
Mar  5 08:41:41.363: INFO: Pod "downward-api-fb4e0bb6-e0e3-4427-90f1-18f805101b39": Phase="Running", Reason="", readiness=true. Elapsed: 2.025288224s
Mar  5 08:41:43.369: INFO: Pod "downward-api-fb4e0bb6-e0e3-4427-90f1-18f805101b39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032033449s
STEP: Saw pod success
Mar  5 08:41:43.370: INFO: Pod "downward-api-fb4e0bb6-e0e3-4427-90f1-18f805101b39" satisfied condition "Succeeded or Failed"
Mar  5 08:41:43.377: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downward-api-fb4e0bb6-e0e3-4427-90f1-18f805101b39 container dapi-container: <nil>
STEP: delete the pod
Mar  5 08:41:43.422: INFO: Waiting for pod downward-api-fb4e0bb6-e0e3-4427-90f1-18f805101b39 to disappear
Mar  5 08:41:43.426: INFO: Pod downward-api-fb4e0bb6-e0e3-4427-90f1-18f805101b39 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:41:43.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9426" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":277,"completed":218,"skipped":3773,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:41:43.446: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar  5 08:41:43.677: INFO: Waiting up to 5m0s for pod "downward-api-dc86c12a-5116-4290-9e29-96150a661bbc" in namespace "downward-api-3534" to be "Succeeded or Failed"
Mar  5 08:41:43.684: INFO: Pod "downward-api-dc86c12a-5116-4290-9e29-96150a661bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.607767ms
Mar  5 08:41:45.697: INFO: Pod "downward-api-dc86c12a-5116-4290-9e29-96150a661bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020237538s
Mar  5 08:41:47.707: INFO: Pod "downward-api-dc86c12a-5116-4290-9e29-96150a661bbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030315735s
STEP: Saw pod success
Mar  5 08:41:47.707: INFO: Pod "downward-api-dc86c12a-5116-4290-9e29-96150a661bbc" satisfied condition "Succeeded or Failed"
Mar  5 08:41:47.717: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downward-api-dc86c12a-5116-4290-9e29-96150a661bbc container dapi-container: <nil>
STEP: delete the pod
Mar  5 08:41:47.774: INFO: Waiting for pod downward-api-dc86c12a-5116-4290-9e29-96150a661bbc to disappear
Mar  5 08:41:47.781: INFO: Pod downward-api-dc86c12a-5116-4290-9e29-96150a661bbc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:41:47.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3534" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":277,"completed":219,"skipped":3779,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:41:47.825: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9997
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-1524
STEP: Creating secret with name secret-test-3755c09d-5165-4005-8a8c-dad8a52902ce
STEP: Creating a pod to test consume secrets
Mar  5 08:41:48.269: INFO: Waiting up to 5m0s for pod "pod-secrets-73c0cb5e-3e85-4965-b8c9-63a9f76212e4" in namespace "secrets-9997" to be "Succeeded or Failed"
Mar  5 08:41:48.275: INFO: Pod "pod-secrets-73c0cb5e-3e85-4965-b8c9-63a9f76212e4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.974179ms
Mar  5 08:41:50.282: INFO: Pod "pod-secrets-73c0cb5e-3e85-4965-b8c9-63a9f76212e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013200103s
STEP: Saw pod success
Mar  5 08:41:50.282: INFO: Pod "pod-secrets-73c0cb5e-3e85-4965-b8c9-63a9f76212e4" satisfied condition "Succeeded or Failed"
Mar  5 08:41:50.287: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-secrets-73c0cb5e-3e85-4965-b8c9-63a9f76212e4 container secret-volume-test: <nil>
STEP: delete the pod
Mar  5 08:41:50.325: INFO: Waiting for pod pod-secrets-73c0cb5e-3e85-4965-b8c9-63a9f76212e4 to disappear
Mar  5 08:41:50.331: INFO: Pod pod-secrets-73c0cb5e-3e85-4965-b8c9-63a9f76212e4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:41:50.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9997" for this suite.
STEP: Destroying namespace "secret-namespace-1524" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":277,"completed":220,"skipped":3809,"failed":0}

------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:41:50.394: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6941
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-85eb3b99-dd83-4d5d-9a28-7817f191cf2d
STEP: Creating a pod to test consume secrets
Mar  5 08:41:50.627: INFO: Waiting up to 5m0s for pod "pod-secrets-58d27a2e-a76f-43ce-8523-25aefe8c6537" in namespace "secrets-6941" to be "Succeeded or Failed"
Mar  5 08:41:50.634: INFO: Pod "pod-secrets-58d27a2e-a76f-43ce-8523-25aefe8c6537": Phase="Pending", Reason="", readiness=false. Elapsed: 7.108094ms
Mar  5 08:41:52.649: INFO: Pod "pod-secrets-58d27a2e-a76f-43ce-8523-25aefe8c6537": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022001671s
STEP: Saw pod success
Mar  5 08:41:52.649: INFO: Pod "pod-secrets-58d27a2e-a76f-43ce-8523-25aefe8c6537" satisfied condition "Succeeded or Failed"
Mar  5 08:41:52.654: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-secrets-58d27a2e-a76f-43ce-8523-25aefe8c6537 container secret-env-test: <nil>
STEP: delete the pod
Mar  5 08:41:52.712: INFO: Waiting for pod pod-secrets-58d27a2e-a76f-43ce-8523-25aefe8c6537 to disappear
Mar  5 08:41:52.718: INFO: Pod pod-secrets-58d27a2e-a76f-43ce-8523-25aefe8c6537 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:41:52.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6941" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":277,"completed":221,"skipped":3809,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:41:52.757: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-8616
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Mar  5 08:41:52.968: INFO: Found 0 stateful pods, waiting for 3
Mar  5 08:42:02.977: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 08:42:02.977: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 08:42:02.977: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 08:42:02.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-8616 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  5 08:42:03.677: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  5 08:42:03.677: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  5 08:42:03.677: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar  5 08:42:13.736: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar  5 08:42:23.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-8616 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  5 08:42:24.310: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  5 08:42:24.310: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  5 08:42:24.310: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  5 08:42:54.363: INFO: Waiting for StatefulSet statefulset-8616/ss2 to complete update
STEP: Rolling back to a previous revision
Mar  5 08:43:04.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-8616 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  5 08:43:04.755: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  5 08:43:04.755: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  5 08:43:04.755: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  5 08:43:14.833: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar  5 08:43:24.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-8616 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  5 08:43:25.333: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  5 08:43:25.333: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  5 08:43:25.333: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  5 08:43:55.390: INFO: Waiting for StatefulSet statefulset-8616/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar  5 08:44:05.407: INFO: Deleting all statefulset in ns statefulset-8616
Mar  5 08:44:05.416: INFO: Scaling statefulset ss2 to 0
Mar  5 08:44:15.450: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 08:44:15.468: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:44:15.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8616" for this suite.

â€¢ [SLOW TEST:142.771 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":277,"completed":222,"skipped":3834,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:44:15.534: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-3071
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar  5 08:44:16.487: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar  5 08:44:18.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530656, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530656, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530656, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530656, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 08:44:21.552: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:44:21.559: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:44:23.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3071" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:8.991 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":277,"completed":223,"skipped":3853,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:44:24.525: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9375
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:44:24.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 version'
Mar  5 08:44:24.879: INFO: stderr: ""
Mar  5 08:44:24.879: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.6\", GitCommit:\"dff82dc0de47299ab66c83c626e08b245ab19037\", GitTreeState:\"clean\", BuildDate:\"2020-07-15T16:58:53Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.6\", GitCommit:\"dff82dc0de47299ab66c83c626e08b245ab19037\", GitTreeState:\"clean\", BuildDate:\"2020-07-15T16:51:04Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:44:24.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9375" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":277,"completed":224,"skipped":3863,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:44:24.916: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2408
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2408.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2408.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2408.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2408.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2408.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2408.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2408.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 2.21.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.21.2_udp@PTR;check="$$(dig +tcp +noall +answer +search 2.21.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.21.2_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2408.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2408.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2408.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2408.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2408.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2408.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2408.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2408.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2408.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 2.21.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.21.2_udp@PTR;check="$$(dig +tcp +noall +answer +search 2.21.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.21.2_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  5 08:44:29.313: INFO: Unable to read wheezy_udp@dns-test-service.dns-2408.svc.cluster.local from pod dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8: the server could not find the requested resource (get pods dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8)
Mar  5 08:44:29.320: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2408.svc.cluster.local from pod dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8: the server could not find the requested resource (get pods dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8)
Mar  5 08:44:29.326: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local from pod dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8: the server could not find the requested resource (get pods dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8)
Mar  5 08:44:29.332: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local from pod dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8: the server could not find the requested resource (get pods dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8)
Mar  5 08:44:29.388: INFO: Unable to read jessie_udp@dns-test-service.dns-2408.svc.cluster.local from pod dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8: the server could not find the requested resource (get pods dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8)
Mar  5 08:44:29.395: INFO: Unable to read jessie_tcp@dns-test-service.dns-2408.svc.cluster.local from pod dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8: the server could not find the requested resource (get pods dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8)
Mar  5 08:44:29.403: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local from pod dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8: the server could not find the requested resource (get pods dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8)
Mar  5 08:44:29.410: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local from pod dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8: the server could not find the requested resource (get pods dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8)
Mar  5 08:44:29.474: INFO: Lookups using dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8 failed for: [wheezy_udp@dns-test-service.dns-2408.svc.cluster.local wheezy_tcp@dns-test-service.dns-2408.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local jessie_udp@dns-test-service.dns-2408.svc.cluster.local jessie_tcp@dns-test-service.dns-2408.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local]

Mar  5 08:44:34.484: INFO: Unable to read wheezy_udp@dns-test-service.dns-2408.svc.cluster.local from pod dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8: the server could not find the requested resource (get pods dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8)
Mar  5 08:44:34.494: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2408.svc.cluster.local from pod dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8: the server could not find the requested resource (get pods dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8)
Mar  5 08:44:34.501: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local from pod dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8: the server could not find the requested resource (get pods dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8)
Mar  5 08:44:34.508: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local from pod dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8: the server could not find the requested resource (get pods dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8)
Mar  5 08:44:34.573: INFO: Unable to read jessie_udp@dns-test-service.dns-2408.svc.cluster.local from pod dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8: the server could not find the requested resource (get pods dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8)
Mar  5 08:44:34.580: INFO: Unable to read jessie_tcp@dns-test-service.dns-2408.svc.cluster.local from pod dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8: the server could not find the requested resource (get pods dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8)
Mar  5 08:44:34.590: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local from pod dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8: the server could not find the requested resource (get pods dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8)
Mar  5 08:44:34.599: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local from pod dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8: the server could not find the requested resource (get pods dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8)
Mar  5 08:44:34.644: INFO: Lookups using dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8 failed for: [wheezy_udp@dns-test-service.dns-2408.svc.cluster.local wheezy_tcp@dns-test-service.dns-2408.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local jessie_udp@dns-test-service.dns-2408.svc.cluster.local jessie_tcp@dns-test-service.dns-2408.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2408.svc.cluster.local]

Mar  5 08:44:39.637: INFO: DNS probes using dns-2408/dns-test-2bb43842-2b2d-4293-99a0-7630ca6c90b8 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:44:39.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2408" for this suite.

â€¢ [SLOW TEST:14.868 seconds]
[sig-network] DNS
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":277,"completed":225,"skipped":3881,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:44:39.785: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2044
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2044
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2044
I0305 08:44:40.079134      26 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2044, replica count: 2
Mar  5 08:44:43.130: INFO: Creating new exec pod
I0305 08:44:43.130162      26 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  5 08:44:46.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=services-2044 execpodncdq8 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar  5 08:44:46.502: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  5 08:44:46.502: INFO: stdout: ""
Mar  5 08:44:46.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=services-2044 execpodncdq8 -- /bin/sh -x -c nc -zv -t -w 2 10.105.189.195 80'
Mar  5 08:44:46.808: INFO: stderr: "+ nc -zv -t -w 2 10.105.189.195 80\nConnection to 10.105.189.195 80 port [tcp/http] succeeded!\n"
Mar  5 08:44:46.808: INFO: stdout: ""
Mar  5 08:44:46.808: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:44:46.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2044" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:7.093 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":277,"completed":226,"skipped":3886,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:44:46.879: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Mar  5 08:44:47.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 create -f - --namespace=kubectl-9391'
Mar  5 08:44:47.708: INFO: stderr: ""
Mar  5 08:44:47.708: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  5 08:44:47.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9391'
Mar  5 08:44:47.867: INFO: stderr: ""
Mar  5 08:44:47.867: INFO: stdout: "update-demo-nautilus-rt9dk update-demo-nautilus-zlgtp "
Mar  5 08:44:47.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods update-demo-nautilus-rt9dk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9391'
Mar  5 08:44:48.029: INFO: stderr: ""
Mar  5 08:44:48.029: INFO: stdout: ""
Mar  5 08:44:48.029: INFO: update-demo-nautilus-rt9dk is created but not running
Mar  5 08:44:53.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9391'
Mar  5 08:44:53.151: INFO: stderr: ""
Mar  5 08:44:53.151: INFO: stdout: "update-demo-nautilus-rt9dk update-demo-nautilus-zlgtp "
Mar  5 08:44:53.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods update-demo-nautilus-rt9dk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9391'
Mar  5 08:44:53.270: INFO: stderr: ""
Mar  5 08:44:53.270: INFO: stdout: "true"
Mar  5 08:44:53.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods update-demo-nautilus-rt9dk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9391'
Mar  5 08:44:53.396: INFO: stderr: ""
Mar  5 08:44:53.396: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 08:44:53.396: INFO: validating pod update-demo-nautilus-rt9dk
Mar  5 08:44:53.405: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 08:44:53.405: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 08:44:53.405: INFO: update-demo-nautilus-rt9dk is verified up and running
Mar  5 08:44:53.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods update-demo-nautilus-zlgtp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9391'
Mar  5 08:44:53.515: INFO: stderr: ""
Mar  5 08:44:53.515: INFO: stdout: "true"
Mar  5 08:44:53.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods update-demo-nautilus-zlgtp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9391'
Mar  5 08:44:53.655: INFO: stderr: ""
Mar  5 08:44:53.656: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 08:44:53.656: INFO: validating pod update-demo-nautilus-zlgtp
Mar  5 08:44:53.665: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 08:44:53.665: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 08:44:53.665: INFO: update-demo-nautilus-zlgtp is verified up and running
STEP: scaling down the replication controller
Mar  5 08:44:53.677: INFO: scanned /root for discovery docs: <nil>
Mar  5 08:44:53.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9391'
Mar  5 08:44:54.894: INFO: stderr: ""
Mar  5 08:44:54.894: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  5 08:44:54.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9391'
Mar  5 08:44:55.026: INFO: stderr: ""
Mar  5 08:44:55.026: INFO: stdout: "update-demo-nautilus-rt9dk update-demo-nautilus-zlgtp "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  5 08:45:00.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9391'
Mar  5 08:45:00.185: INFO: stderr: ""
Mar  5 08:45:00.185: INFO: stdout: "update-demo-nautilus-rt9dk update-demo-nautilus-zlgtp "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  5 08:45:05.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9391'
Mar  5 08:45:05.317: INFO: stderr: ""
Mar  5 08:45:05.317: INFO: stdout: "update-demo-nautilus-rt9dk "
Mar  5 08:45:05.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods update-demo-nautilus-rt9dk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9391'
Mar  5 08:45:05.441: INFO: stderr: ""
Mar  5 08:45:05.441: INFO: stdout: "true"
Mar  5 08:45:05.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods update-demo-nautilus-rt9dk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9391'
Mar  5 08:45:05.568: INFO: stderr: ""
Mar  5 08:45:05.568: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 08:45:05.568: INFO: validating pod update-demo-nautilus-rt9dk
Mar  5 08:45:05.577: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 08:45:05.578: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 08:45:05.578: INFO: update-demo-nautilus-rt9dk is verified up and running
STEP: scaling up the replication controller
Mar  5 08:45:05.583: INFO: scanned /root for discovery docs: <nil>
Mar  5 08:45:05.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9391'
Mar  5 08:45:06.758: INFO: stderr: ""
Mar  5 08:45:06.758: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  5 08:45:06.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9391'
Mar  5 08:45:06.907: INFO: stderr: ""
Mar  5 08:45:06.907: INFO: stdout: "update-demo-nautilus-rt9dk update-demo-nautilus-zdlkm "
Mar  5 08:45:06.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods update-demo-nautilus-rt9dk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9391'
Mar  5 08:45:07.057: INFO: stderr: ""
Mar  5 08:45:07.057: INFO: stdout: "true"
Mar  5 08:45:07.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods update-demo-nautilus-rt9dk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9391'
Mar  5 08:45:07.195: INFO: stderr: ""
Mar  5 08:45:07.195: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 08:45:07.195: INFO: validating pod update-demo-nautilus-rt9dk
Mar  5 08:45:07.215: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 08:45:07.215: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 08:45:07.215: INFO: update-demo-nautilus-rt9dk is verified up and running
Mar  5 08:45:07.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods update-demo-nautilus-zdlkm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9391'
Mar  5 08:45:07.390: INFO: stderr: ""
Mar  5 08:45:07.390: INFO: stdout: "true"
Mar  5 08:45:07.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods update-demo-nautilus-zdlkm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9391'
Mar  5 08:45:07.534: INFO: stderr: ""
Mar  5 08:45:07.534: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  5 08:45:07.534: INFO: validating pod update-demo-nautilus-zdlkm
Mar  5 08:45:07.545: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  5 08:45:07.545: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  5 08:45:07.545: INFO: update-demo-nautilus-zdlkm is verified up and running
STEP: using delete to clean up resources
Mar  5 08:45:07.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 delete --grace-period=0 --force -f - --namespace=kubectl-9391'
Mar  5 08:45:07.684: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  5 08:45:07.684: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  5 08:45:07.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9391'
Mar  5 08:45:07.835: INFO: stderr: "No resources found in kubectl-9391 namespace.\n"
Mar  5 08:45:07.835: INFO: stdout: ""
Mar  5 08:45:07.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 get pods -l name=update-demo --namespace=kubectl-9391 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  5 08:45:07.970: INFO: stderr: ""
Mar  5 08:45:07.971: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:45:07.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9391" for this suite.

â€¢ [SLOW TEST:21.129 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":277,"completed":227,"skipped":3890,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:45:08.009: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7483
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7483.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7483.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  5 08:45:12.335: INFO: DNS probes using dns-7483/dns-test-19b76596-ce48-4d90-a7bf-62a7d73d0ccf succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:45:12.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7483" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":277,"completed":228,"skipped":3896,"failed":0}

------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:45:12.394: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7400
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar  5 08:45:12.630: INFO: Waiting up to 5m0s for pod "downward-api-dad0d416-5483-42fe-a25d-9c03c21225a2" in namespace "downward-api-7400" to be "Succeeded or Failed"
Mar  5 08:45:12.637: INFO: Pod "downward-api-dad0d416-5483-42fe-a25d-9c03c21225a2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.774209ms
Mar  5 08:45:14.644: INFO: Pod "downward-api-dad0d416-5483-42fe-a25d-9c03c21225a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013392581s
STEP: Saw pod success
Mar  5 08:45:14.644: INFO: Pod "downward-api-dad0d416-5483-42fe-a25d-9c03c21225a2" satisfied condition "Succeeded or Failed"
Mar  5 08:45:14.650: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downward-api-dad0d416-5483-42fe-a25d-9c03c21225a2 container dapi-container: <nil>
STEP: delete the pod
Mar  5 08:45:14.725: INFO: Waiting for pod downward-api-dad0d416-5483-42fe-a25d-9c03c21225a2 to disappear
Mar  5 08:45:14.731: INFO: Pod downward-api-dad0d416-5483-42fe-a25d-9c03c21225a2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:45:14.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7400" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":277,"completed":229,"skipped":3896,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:45:14.767: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8031
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  5 08:45:16.075: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  5 08:45:18.112: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530716, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530716, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530716, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530716, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 08:45:21.155: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar  5 08:45:23.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 attach --namespace=webhook-8031 to-be-attached-pod -i -c=container1'
Mar  5 08:45:23.502: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:45:23.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8031" for this suite.
STEP: Destroying namespace "webhook-8031-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:8.968 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":277,"completed":230,"skipped":3927,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:45:23.736: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-178
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 08:45:24.115: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b57818a0-bbd5-4de4-a0a6-c738a473c28e" in namespace "projected-178" to be "Succeeded or Failed"
Mar  5 08:45:24.128: INFO: Pod "downwardapi-volume-b57818a0-bbd5-4de4-a0a6-c738a473c28e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.323035ms
Mar  5 08:45:26.136: INFO: Pod "downwardapi-volume-b57818a0-bbd5-4de4-a0a6-c738a473c28e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020364499s
Mar  5 08:45:28.143: INFO: Pod "downwardapi-volume-b57818a0-bbd5-4de4-a0a6-c738a473c28e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02777867s
STEP: Saw pod success
Mar  5 08:45:28.144: INFO: Pod "downwardapi-volume-b57818a0-bbd5-4de4-a0a6-c738a473c28e" satisfied condition "Succeeded or Failed"
Mar  5 08:45:28.150: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-b57818a0-bbd5-4de4-a0a6-c738a473c28e container client-container: <nil>
STEP: delete the pod
Mar  5 08:45:28.201: INFO: Waiting for pod downwardapi-volume-b57818a0-bbd5-4de4-a0a6-c738a473c28e to disappear
Mar  5 08:45:28.206: INFO: Pod downwardapi-volume-b57818a0-bbd5-4de4-a0a6-c738a473c28e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:45:28.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-178" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":231,"skipped":3941,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:45:28.230: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2243
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-7ad20643-b1c4-47ca-a95e-18cb16be1c87
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:45:32.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2243" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":232,"skipped":3970,"failed":0}
SSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:45:32.548: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:45:32.798: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-e09f448a-9154-4e20-8d2d-ef23205ff5d7" in namespace "security-context-test-2988" to be "Succeeded or Failed"
Mar  5 08:45:32.803: INFO: Pod "busybox-privileged-false-e09f448a-9154-4e20-8d2d-ef23205ff5d7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.695171ms
Mar  5 08:45:34.810: INFO: Pod "busybox-privileged-false-e09f448a-9154-4e20-8d2d-ef23205ff5d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012529008s
Mar  5 08:45:36.817: INFO: Pod "busybox-privileged-false-e09f448a-9154-4e20-8d2d-ef23205ff5d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018816156s
Mar  5 08:45:36.817: INFO: Pod "busybox-privileged-false-e09f448a-9154-4e20-8d2d-ef23205ff5d7" satisfied condition "Succeeded or Failed"
Mar  5 08:45:36.833: INFO: Got logs for pod "busybox-privileged-false-e09f448a-9154-4e20-8d2d-ef23205ff5d7": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:45:36.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2988" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":233,"skipped":3976,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:45:36.857: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-199
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  5 08:45:37.479: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  5 08:45:39.504: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530737, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530737, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530737, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750530737, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 08:45:42.548: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:45:42.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-199" for this suite.
STEP: Destroying namespace "webhook-199-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.939 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":277,"completed":234,"skipped":3992,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:45:42.797: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4777
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod test-webserver-fec0d4d9-a06c-4cd7-bd08-831c8e8bf6a5 in namespace container-probe-4777
Mar  5 08:45:45.219: INFO: Started pod test-webserver-fec0d4d9-a06c-4cd7-bd08-831c8e8bf6a5 in namespace container-probe-4777
STEP: checking the pod's current state and verifying that restartCount is present
Mar  5 08:45:45.229: INFO: Initial restart count of pod test-webserver-fec0d4d9-a06c-4cd7-bd08-831c8e8bf6a5 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:49:46.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4777" for this suite.

â€¢ [SLOW TEST:243.824 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":277,"completed":235,"skipped":4018,"failed":0}
SS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:49:46.622: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3933
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:49:46.864: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-9bcfc4a4-00cb-410c-af0b-f795ab3a0346" in namespace "security-context-test-3933" to be "Succeeded or Failed"
Mar  5 08:49:46.870: INFO: Pod "busybox-readonly-false-9bcfc4a4-00cb-410c-af0b-f795ab3a0346": Phase="Pending", Reason="", readiness=false. Elapsed: 5.785234ms
Mar  5 08:49:48.883: INFO: Pod "busybox-readonly-false-9bcfc4a4-00cb-410c-af0b-f795ab3a0346": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019191988s
Mar  5 08:49:48.884: INFO: Pod "busybox-readonly-false-9bcfc4a4-00cb-410c-af0b-f795ab3a0346" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:49:48.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3933" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":277,"completed":236,"skipped":4020,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:49:48.905: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4700
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-cedb314b-9603-4b50-b503-43660ff7d21a
STEP: Creating a pod to test consume configMaps
Mar  5 08:49:49.154: INFO: Waiting up to 5m0s for pod "pod-configmaps-f36ac479-4882-408d-a5b5-4c33ad75e210" in namespace "configmap-4700" to be "Succeeded or Failed"
Mar  5 08:49:49.161: INFO: Pod "pod-configmaps-f36ac479-4882-408d-a5b5-4c33ad75e210": Phase="Pending", Reason="", readiness=false. Elapsed: 7.043316ms
Mar  5 08:49:51.168: INFO: Pod "pod-configmaps-f36ac479-4882-408d-a5b5-4c33ad75e210": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013738078s
STEP: Saw pod success
Mar  5 08:49:51.168: INFO: Pod "pod-configmaps-f36ac479-4882-408d-a5b5-4c33ad75e210" satisfied condition "Succeeded or Failed"
Mar  5 08:49:51.175: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-configmaps-f36ac479-4882-408d-a5b5-4c33ad75e210 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 08:49:51.226: INFO: Waiting for pod pod-configmaps-f36ac479-4882-408d-a5b5-4c33ad75e210 to disappear
Mar  5 08:49:51.232: INFO: Pod pod-configmaps-f36ac479-4882-408d-a5b5-4c33ad75e210 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:49:51.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4700" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":237,"skipped":4020,"failed":0}
SSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:49:51.262: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4978
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:157
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:49:51.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4978" for this suite.
â€¢{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":277,"completed":238,"skipped":4025,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:49:51.533: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9038
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-9038
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating stateful set ss in namespace statefulset-9038
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9038
Mar  5 08:49:51.771: INFO: Found 0 stateful pods, waiting for 1
Mar  5 08:50:01.784: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  5 08:50:01.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-9038 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  5 08:50:02.275: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  5 08:50:02.275: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  5 08:50:02.275: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  5 08:50:02.290: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 08:50:02.290: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 08:50:02.323: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Mar  5 08:50:02.323: INFO: ss-0  devops-pool1-6c76d44df9-8tgs6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  }]
Mar  5 08:50:02.323: INFO: 
Mar  5 08:50:02.323: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  5 08:50:03.334: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993655917s
Mar  5 08:50:04.341: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983545597s
Mar  5 08:50:05.420: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975531798s
Mar  5 08:50:06.428: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.896672517s
Mar  5 08:50:07.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.888941428s
Mar  5 08:50:08.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.878965136s
Mar  5 08:50:09.458: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.869480986s
Mar  5 08:50:10.465: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.858966248s
Mar  5 08:50:11.482: INFO: Verifying statefulset ss doesn't scale past 3 for another 851.481507ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9038
Mar  5 08:50:12.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-9038 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  5 08:50:12.841: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  5 08:50:12.841: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  5 08:50:12.841: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  5 08:50:12.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-9038 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  5 08:50:13.188: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  5 08:50:13.188: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  5 08:50:13.188: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  5 08:50:13.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-9038 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  5 08:50:13.522: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  5 08:50:13.522: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  5 08:50:13.522: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  5 08:50:13.533: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 08:50:13.533: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  5 08:50:13.533: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar  5 08:50:13.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-9038 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  5 08:50:13.871: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  5 08:50:13.871: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  5 08:50:13.871: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  5 08:50:13.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-9038 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  5 08:50:14.214: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  5 08:50:14.214: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  5 08:50:14.214: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  5 08:50:14.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=statefulset-9038 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  5 08:50:14.574: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  5 08:50:14.574: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  5 08:50:14.574: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  5 08:50:14.574: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 08:50:14.582: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  5 08:50:24.598: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 08:50:24.598: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 08:50:24.598: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  5 08:50:24.627: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Mar  5 08:50:24.627: INFO: ss-0  devops-pool1-6c76d44df9-8tgs6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  }]
Mar  5 08:50:24.628: INFO: ss-1  devops-pool1-6c76d44df9-8tgs6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:24.628: INFO: ss-2  devops-pool1-6c76d44df9-8tgs6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:24.628: INFO: 
Mar  5 08:50:24.628: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  5 08:50:25.641: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Mar  5 08:50:25.641: INFO: ss-0  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  }]
Mar  5 08:50:25.641: INFO: ss-1  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:25.641: INFO: ss-2  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:25.641: INFO: 
Mar  5 08:50:25.641: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  5 08:50:26.652: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Mar  5 08:50:26.652: INFO: ss-0  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  }]
Mar  5 08:50:26.652: INFO: ss-1  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:26.652: INFO: ss-2  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:26.652: INFO: 
Mar  5 08:50:26.652: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  5 08:50:27.661: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Mar  5 08:50:27.661: INFO: ss-0  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  }]
Mar  5 08:50:27.661: INFO: ss-1  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:27.661: INFO: ss-2  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:27.662: INFO: 
Mar  5 08:50:27.662: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  5 08:50:28.674: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Mar  5 08:50:28.674: INFO: ss-0  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  }]
Mar  5 08:50:28.674: INFO: ss-1  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:28.674: INFO: ss-2  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:28.674: INFO: 
Mar  5 08:50:28.674: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  5 08:50:29.683: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Mar  5 08:50:29.683: INFO: ss-0  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  }]
Mar  5 08:50:29.683: INFO: ss-1  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:29.683: INFO: ss-2  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:29.683: INFO: 
Mar  5 08:50:29.683: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  5 08:50:30.694: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Mar  5 08:50:30.694: INFO: ss-0  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  }]
Mar  5 08:50:30.694: INFO: ss-1  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:30.694: INFO: ss-2  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:30.694: INFO: 
Mar  5 08:50:30.694: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  5 08:50:31.700: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Mar  5 08:50:31.700: INFO: ss-0  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  }]
Mar  5 08:50:31.700: INFO: ss-1  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:31.700: INFO: ss-2  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:31.700: INFO: 
Mar  5 08:50:31.700: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  5 08:50:32.712: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Mar  5 08:50:32.712: INFO: ss-0  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:49:51 +0000 UTC  }]
Mar  5 08:50:32.712: INFO: ss-1  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:32.712: INFO: ss-2  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:32.712: INFO: 
Mar  5 08:50:32.712: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  5 08:50:33.726: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Mar  5 08:50:33.726: INFO: ss-2  devops-pool1-6c76d44df9-8tgs6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-05 08:50:02 +0000 UTC  }]
Mar  5 08:50:33.726: INFO: 
Mar  5 08:50:33.726: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9038
Mar  5 08:50:34.735: INFO: Scaling statefulset ss to 0
Mar  5 08:50:34.756: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar  5 08:50:34.764: INFO: Deleting all statefulset in ns statefulset-9038
Mar  5 08:50:34.772: INFO: Scaling statefulset ss to 0
Mar  5 08:50:34.794: INFO: Waiting for statefulset status.replicas updated to 0
Mar  5 08:50:34.801: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:50:34.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9038" for this suite.

â€¢ [SLOW TEST:43.343 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":277,"completed":239,"skipped":4078,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:50:34.882: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5040
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  5 08:50:38.126: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:50:38.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5040" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":240,"skipped":4113,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:50:38.191: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5935
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap that has name configmap-test-emptyKey-a78ee882-1688-477a-9c7a-eba833c5e2ac
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:50:38.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5935" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":277,"completed":241,"skipped":4123,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:50:38.416: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3551
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:50:40.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3551" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":277,"completed":242,"skipped":4133,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:50:40.715: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-2998
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:50:40.903: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Creating first CR 
Mar  5 08:50:41.592: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-05T08:50:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-05T08:50:41Z]] name:name1 resourceVersion:5648454 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:09367768-3ab4-4885-97a1-65a16b425d9c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar  5 08:50:51.605: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-05T08:50:51Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-05T08:50:51Z]] name:name2 resourceVersion:5648557 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:4614c4e9-3cd5-400c-a7cc-ae22ac71b951] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar  5 08:51:01.629: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-05T08:50:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-05T08:51:01Z]] name:name1 resourceVersion:5648631 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:09367768-3ab4-4885-97a1-65a16b425d9c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar  5 08:51:11.638: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-05T08:50:51Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-05T08:51:11Z]] name:name2 resourceVersion:5648698 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:4614c4e9-3cd5-400c-a7cc-ae22ac71b951] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar  5 08:51:21.661: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-05T08:50:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-05T08:51:01Z]] name:name1 resourceVersion:5648766 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:09367768-3ab4-4885-97a1-65a16b425d9c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar  5 08:51:31.682: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-05T08:50:51Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-05T08:51:11Z]] name:name2 resourceVersion:5648836 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:4614c4e9-3cd5-400c-a7cc-ae22ac71b951] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:51:42.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2998" for this suite.

â€¢ [SLOW TEST:61.522 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":277,"completed":243,"skipped":4134,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:51:42.239: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:51:46.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7867" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":244,"skipped":4136,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:51:46.770: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-193
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:51:46.977: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:51:47.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-193" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":277,"completed":245,"skipped":4173,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:51:47.600: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8853
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  5 08:51:49.053: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  5 08:51:51.258: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750531109, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750531109, loc:(*time.Location)(0x7b51220)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750531109, loc:(*time.Location)(0x7b51220)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750531109, loc:(*time.Location)(0x7b51220)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  5 08:51:54.310: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:52:06.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8853" for this suite.
STEP: Destroying namespace "webhook-8853-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:19.129 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":277,"completed":246,"skipped":4174,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:52:06.730: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1746
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Mar  5 08:52:06.939: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:52:54.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1746" for this suite.

â€¢ [SLOW TEST:47.459 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":277,"completed":247,"skipped":4186,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:52:54.192: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1056
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Mar  5 08:52:56.955: INFO: Successfully updated pod "annotationupdate936ae142-a3c3-4600-8035-712e9feacdb4"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:53:01.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1056" for this suite.

â€¢ [SLOW TEST:6.866 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":277,"completed":248,"skipped":4191,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:53:01.058: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2464
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:53:01.252: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  5 08:53:09.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-2464 create -f -'
Mar  5 08:53:10.264: INFO: stderr: ""
Mar  5 08:53:10.264: INFO: stdout: "e2e-test-crd-publish-openapi-1754-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  5 08:53:10.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-2464 delete e2e-test-crd-publish-openapi-1754-crds test-cr'
Mar  5 08:53:10.402: INFO: stderr: ""
Mar  5 08:53:10.402: INFO: stdout: "e2e-test-crd-publish-openapi-1754-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar  5 08:53:10.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-2464 apply -f -'
Mar  5 08:53:10.909: INFO: stderr: ""
Mar  5 08:53:10.909: INFO: stdout: "e2e-test-crd-publish-openapi-1754-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  5 08:53:10.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-2464 delete e2e-test-crd-publish-openapi-1754-crds test-cr'
Mar  5 08:53:11.056: INFO: stderr: ""
Mar  5 08:53:11.056: INFO: stdout: "e2e-test-crd-publish-openapi-1754-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar  5 08:53:11.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 explain e2e-test-crd-publish-openapi-1754-crds'
Mar  5 08:53:11.574: INFO: stderr: ""
Mar  5 08:53:11.574: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1754-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:53:19.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2464" for this suite.

â€¢ [SLOW TEST:18.506 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":277,"completed":249,"skipped":4225,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:53:19.564: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5992
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-70416d07-59fb-4c7b-9e44-6d908507a33d
STEP: Creating secret with name s-test-opt-upd-dce21796-ca14-4b08-a629-9708461da13f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-70416d07-59fb-4c7b-9e44-6d908507a33d
STEP: Updating secret s-test-opt-upd-dce21796-ca14-4b08-a629-9708461da13f
STEP: Creating secret with name s-test-opt-create-cacd711e-92c4-45a0-94a7-1430edda484f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:53:24.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5992" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":250,"skipped":4247,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:53:24.077: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6043
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6043
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6043
STEP: creating replication controller externalsvc in namespace services-6043
I0305 08:53:24.329820      26 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6043, replica count: 2
I0305 08:53:27.380740      26 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar  5 08:53:27.445: INFO: Creating new exec pod
Mar  5 08:53:29.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=services-6043 execpodlg5q8 -- /bin/sh -x -c nslookup clusterip-service'
Mar  5 08:53:29.860: INFO: stderr: "+ nslookup clusterip-service\n"
Mar  5 08:53:29.860: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-6043.svc.cluster.local\tcanonical name = externalsvc.services-6043.svc.cluster.local.\nName:\texternalsvc.services-6043.svc.cluster.local\nAddress: 10.101.106.140\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6043, will wait for the garbage collector to delete the pods
Mar  5 08:53:29.948: INFO: Deleting ReplicationController externalsvc took: 31.375761ms
Mar  5 08:53:30.049: INFO: Terminating ReplicationController externalsvc pods took: 100.304247ms
Mar  5 08:53:43.812: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:53:43.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6043" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:19.776 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":277,"completed":251,"skipped":4259,"failed":0}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:53:43.854: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3069
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  5 08:53:50.151: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 08:53:50.157: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  5 08:53:52.157: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 08:53:52.165: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  5 08:53:54.157: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 08:53:54.166: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  5 08:53:56.157: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 08:53:56.163: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  5 08:53:58.157: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 08:53:58.165: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  5 08:54:00.157: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 08:54:00.163: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  5 08:54:02.157: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 08:54:02.166: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  5 08:54:04.157: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  5 08:54:04.165: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:54:04.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3069" for this suite.

â€¢ [SLOW TEST:20.359 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":277,"completed":252,"skipped":4262,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:54:04.214: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-841
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Mar  5 08:54:06.478: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-841 PodName:pod-sharedvolume-217653e9-c693-479a-a0aa-b90a8cc7e1b2 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar  5 08:54:06.478: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
Mar  5 08:54:06.658: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:54:06.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-841" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":277,"completed":253,"skipped":4268,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:54:06.704: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1407
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:54:24.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1407" for this suite.

â€¢ [SLOW TEST:17.371 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":277,"completed":254,"skipped":4275,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:54:24.077: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4028
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:54:28.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4028" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":277,"completed":255,"skipped":4291,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:54:28.374: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9261
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-secret-878d
STEP: Creating a pod to test atomic-volume-subpath
Mar  5 08:54:28.580: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-878d" in namespace "subpath-9261" to be "Succeeded or Failed"
Mar  5 08:54:28.586: INFO: Pod "pod-subpath-test-secret-878d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.377409ms
Mar  5 08:54:30.595: INFO: Pod "pod-subpath-test-secret-878d": Phase="Running", Reason="", readiness=true. Elapsed: 2.014265494s
Mar  5 08:54:32.611: INFO: Pod "pod-subpath-test-secret-878d": Phase="Running", Reason="", readiness=true. Elapsed: 4.030120418s
Mar  5 08:54:34.619: INFO: Pod "pod-subpath-test-secret-878d": Phase="Running", Reason="", readiness=true. Elapsed: 6.038428641s
Mar  5 08:54:36.633: INFO: Pod "pod-subpath-test-secret-878d": Phase="Running", Reason="", readiness=true. Elapsed: 8.052102239s
Mar  5 08:54:38.640: INFO: Pod "pod-subpath-test-secret-878d": Phase="Running", Reason="", readiness=true. Elapsed: 10.059872646s
Mar  5 08:54:40.652: INFO: Pod "pod-subpath-test-secret-878d": Phase="Running", Reason="", readiness=true. Elapsed: 12.071943673s
Mar  5 08:54:42.661: INFO: Pod "pod-subpath-test-secret-878d": Phase="Running", Reason="", readiness=true. Elapsed: 14.080077339s
Mar  5 08:54:44.667: INFO: Pod "pod-subpath-test-secret-878d": Phase="Running", Reason="", readiness=true. Elapsed: 16.086499323s
Mar  5 08:54:46.675: INFO: Pod "pod-subpath-test-secret-878d": Phase="Running", Reason="", readiness=true. Elapsed: 18.094345357s
Mar  5 08:54:48.690: INFO: Pod "pod-subpath-test-secret-878d": Phase="Running", Reason="", readiness=true. Elapsed: 20.109598712s
Mar  5 08:54:50.704: INFO: Pod "pod-subpath-test-secret-878d": Phase="Running", Reason="", readiness=true. Elapsed: 22.123054964s
Mar  5 08:54:52.712: INFO: Pod "pod-subpath-test-secret-878d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.131520861s
STEP: Saw pod success
Mar  5 08:54:52.712: INFO: Pod "pod-subpath-test-secret-878d" satisfied condition "Succeeded or Failed"
Mar  5 08:54:52.731: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-subpath-test-secret-878d container test-container-subpath-secret-878d: <nil>
STEP: delete the pod
Mar  5 08:54:52.787: INFO: Waiting for pod pod-subpath-test-secret-878d to disappear
Mar  5 08:54:52.794: INFO: Pod pod-subpath-test-secret-878d no longer exists
STEP: Deleting pod pod-subpath-test-secret-878d
Mar  5 08:54:52.795: INFO: Deleting pod "pod-subpath-test-secret-878d" in namespace "subpath-9261"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:54:52.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9261" for this suite.

â€¢ [SLOW TEST:24.450 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":277,"completed":256,"skipped":4297,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:54:52.825: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3804
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 08:54:53.029: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b2b2ff8d-e254-407e-b084-96acdd942a15" in namespace "projected-3804" to be "Succeeded or Failed"
Mar  5 08:54:53.034: INFO: Pod "downwardapi-volume-b2b2ff8d-e254-407e-b084-96acdd942a15": Phase="Pending", Reason="", readiness=false. Elapsed: 5.309337ms
Mar  5 08:54:55.043: INFO: Pod "downwardapi-volume-b2b2ff8d-e254-407e-b084-96acdd942a15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014500726s
STEP: Saw pod success
Mar  5 08:54:55.043: INFO: Pod "downwardapi-volume-b2b2ff8d-e254-407e-b084-96acdd942a15" satisfied condition "Succeeded or Failed"
Mar  5 08:54:55.062: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-b2b2ff8d-e254-407e-b084-96acdd942a15 container client-container: <nil>
STEP: delete the pod
Mar  5 08:54:55.103: INFO: Waiting for pod downwardapi-volume-b2b2ff8d-e254-407e-b084-96acdd942a15 to disappear
Mar  5 08:54:55.109: INFO: Pod downwardapi-volume-b2b2ff8d-e254-407e-b084-96acdd942a15 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:54:55.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3804" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":257,"skipped":4302,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:54:55.143: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9963
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating pod
Mar  5 08:54:57.429: INFO: Pod pod-hostip-93ec3ebc-12be-46cd-9458-6f8ae1f62fe3 has hostIP: 192.168.0.8
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:54:57.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9963" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":277,"completed":258,"skipped":4372,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:54:57.461: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4378
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 08:54:57.748: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8a18848-f603-4eaa-b22d-35ec4e03642a" in namespace "projected-4378" to be "Succeeded or Failed"
Mar  5 08:54:57.758: INFO: Pod "downwardapi-volume-f8a18848-f603-4eaa-b22d-35ec4e03642a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.351658ms
Mar  5 08:54:59.765: INFO: Pod "downwardapi-volume-f8a18848-f603-4eaa-b22d-35ec4e03642a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016877846s
STEP: Saw pod success
Mar  5 08:54:59.765: INFO: Pod "downwardapi-volume-f8a18848-f603-4eaa-b22d-35ec4e03642a" satisfied condition "Succeeded or Failed"
Mar  5 08:54:59.770: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-f8a18848-f603-4eaa-b22d-35ec4e03642a container client-container: <nil>
STEP: delete the pod
Mar  5 08:54:59.812: INFO: Waiting for pod downwardapi-volume-f8a18848-f603-4eaa-b22d-35ec4e03642a to disappear
Mar  5 08:54:59.820: INFO: Pod downwardapi-volume-f8a18848-f603-4eaa-b22d-35ec4e03642a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:54:59.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4378" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":259,"skipped":4394,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:54:59.839: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3904
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-7a21d51d-38bc-4140-ab15-e5c262e33a8b
STEP: Creating a pod to test consume configMaps
Mar  5 08:55:00.066: INFO: Waiting up to 5m0s for pod "pod-configmaps-c90d6bac-ba7d-4ea5-82ad-bb895f0d27ae" in namespace "configmap-3904" to be "Succeeded or Failed"
Mar  5 08:55:00.072: INFO: Pod "pod-configmaps-c90d6bac-ba7d-4ea5-82ad-bb895f0d27ae": Phase="Pending", Reason="", readiness=false. Elapsed: 5.628852ms
Mar  5 08:55:02.084: INFO: Pod "pod-configmaps-c90d6bac-ba7d-4ea5-82ad-bb895f0d27ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017844382s
STEP: Saw pod success
Mar  5 08:55:02.084: INFO: Pod "pod-configmaps-c90d6bac-ba7d-4ea5-82ad-bb895f0d27ae" satisfied condition "Succeeded or Failed"
Mar  5 08:55:02.091: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-configmaps-c90d6bac-ba7d-4ea5-82ad-bb895f0d27ae container configmap-volume-test: <nil>
STEP: delete the pod
Mar  5 08:55:02.156: INFO: Waiting for pod pod-configmaps-c90d6bac-ba7d-4ea5-82ad-bb895f0d27ae to disappear
Mar  5 08:55:02.168: INFO: Pod pod-configmaps-c90d6bac-ba7d-4ea5-82ad-bb895f0d27ae no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:55:02.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3904" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":277,"completed":260,"skipped":4409,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:55:02.244: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6029
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:55:02.481: INFO: Waiting up to 5m0s for pod "busybox-user-65534-00d64cb5-6dcb-4cf4-9956-5b625f6fb0b7" in namespace "security-context-test-6029" to be "Succeeded or Failed"
Mar  5 08:55:02.489: INFO: Pod "busybox-user-65534-00d64cb5-6dcb-4cf4-9956-5b625f6fb0b7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.213424ms
Mar  5 08:55:04.498: INFO: Pod "busybox-user-65534-00d64cb5-6dcb-4cf4-9956-5b625f6fb0b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017255897s
Mar  5 08:55:04.498: INFO: Pod "busybox-user-65534-00d64cb5-6dcb-4cf4-9956-5b625f6fb0b7" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:55:04.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6029" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":261,"skipped":4416,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:55:04.527: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6043
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Mar  5 08:55:05.816: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:55:05.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0305 08:55:05.816707      26 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-6043" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":277,"completed":262,"skipped":4432,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:55:05.845: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4724
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4724
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4724
I0305 08:55:06.126597      26 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4724, replica count: 2
Mar  5 08:55:09.179: INFO: Creating new exec pod
I0305 08:55:09.179500      26 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  5 08:55:12.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=services-4724 execpodskz5j -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar  5 08:55:12.614: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  5 08:55:12.614: INFO: stdout: ""
Mar  5 08:55:12.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=services-4724 execpodskz5j -- /bin/sh -x -c nc -zv -t -w 2 10.109.137.76 80'
Mar  5 08:55:13.210: INFO: stderr: "+ nc -zv -t -w 2 10.109.137.76 80\nConnection to 10.109.137.76 80 port [tcp/http] succeeded!\n"
Mar  5 08:55:13.210: INFO: stdout: ""
Mar  5 08:55:13.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=services-4724 execpodskz5j -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.8 31080'
Mar  5 08:55:13.577: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.8 31080\nConnection to 192.168.0.8 31080 port [tcp/31080] succeeded!\n"
Mar  5 08:55:13.577: INFO: stdout: ""
Mar  5 08:55:13.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=services-4724 execpodskz5j -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.5 31080'
Mar  5 08:55:13.898: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.5 31080\nConnection to 192.168.0.5 31080 port [tcp/31080] succeeded!\n"
Mar  5 08:55:13.898: INFO: stdout: ""
Mar  5 08:55:13.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=services-4724 execpodskz5j -- /bin/sh -x -c nc -zv -t -w 2 157.90.124.216 31080'
Mar  5 08:55:14.217: INFO: stderr: "+ nc -zv -t -w 2 157.90.124.216 31080\nConnection to 157.90.124.216 31080 port [tcp/31080] succeeded!\n"
Mar  5 08:55:14.217: INFO: stdout: ""
Mar  5 08:55:14.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 exec --namespace=services-4724 execpodskz5j -- /bin/sh -x -c nc -zv -t -w 2 159.69.2.45 31080'
Mar  5 08:55:14.541: INFO: stderr: "+ nc -zv -t -w 2 159.69.2.45 31080\nConnection to 159.69.2.45 31080 port [tcp/31080] succeeded!\n"
Mar  5 08:55:14.541: INFO: stdout: ""
Mar  5 08:55:14.541: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:55:14.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4724" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:8.792 seconds]
[sig-network] Services
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":277,"completed":263,"skipped":4433,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:55:14.637: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9699
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9699.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9699.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9699.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9699.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9699.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9699.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  5 08:55:19.005: INFO: DNS probes using dns-9699/dns-test-c40c5296-7f4a-428c-bf1f-fc355a9810b3 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:55:19.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9699" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":277,"completed":264,"skipped":4442,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:55:19.071: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7439
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:55:32.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7439" for this suite.

â€¢ [SLOW TEST:13.357 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":277,"completed":265,"skipped":4454,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:55:32.430: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-83
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:55:32.700: INFO: The status of Pod test-webserver-64097ffb-4c6c-4259-b499-0d696a618765 is Pending, waiting for it to be Running (with Ready = true)
Mar  5 08:55:34.709: INFO: The status of Pod test-webserver-64097ffb-4c6c-4259-b499-0d696a618765 is Running (Ready = false)
Mar  5 08:55:36.709: INFO: The status of Pod test-webserver-64097ffb-4c6c-4259-b499-0d696a618765 is Running (Ready = false)
Mar  5 08:55:38.708: INFO: The status of Pod test-webserver-64097ffb-4c6c-4259-b499-0d696a618765 is Running (Ready = false)
Mar  5 08:55:40.707: INFO: The status of Pod test-webserver-64097ffb-4c6c-4259-b499-0d696a618765 is Running (Ready = false)
Mar  5 08:55:42.709: INFO: The status of Pod test-webserver-64097ffb-4c6c-4259-b499-0d696a618765 is Running (Ready = false)
Mar  5 08:55:44.709: INFO: The status of Pod test-webserver-64097ffb-4c6c-4259-b499-0d696a618765 is Running (Ready = false)
Mar  5 08:55:46.708: INFO: The status of Pod test-webserver-64097ffb-4c6c-4259-b499-0d696a618765 is Running (Ready = false)
Mar  5 08:55:48.709: INFO: The status of Pod test-webserver-64097ffb-4c6c-4259-b499-0d696a618765 is Running (Ready = false)
Mar  5 08:55:50.709: INFO: The status of Pod test-webserver-64097ffb-4c6c-4259-b499-0d696a618765 is Running (Ready = false)
Mar  5 08:55:52.709: INFO: The status of Pod test-webserver-64097ffb-4c6c-4259-b499-0d696a618765 is Running (Ready = false)
Mar  5 08:55:54.709: INFO: The status of Pod test-webserver-64097ffb-4c6c-4259-b499-0d696a618765 is Running (Ready = false)
Mar  5 08:55:56.710: INFO: The status of Pod test-webserver-64097ffb-4c6c-4259-b499-0d696a618765 is Running (Ready = true)
Mar  5 08:55:56.718: INFO: Container started at 2021-03-05 08:55:33 +0000 UTC, pod became ready at 2021-03-05 08:55:54 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:55:56.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-83" for this suite.

â€¢ [SLOW TEST:24.330 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":277,"completed":266,"skipped":4503,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:55:56.771: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1262
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:56:13.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1262" for this suite.

â€¢ [SLOW TEST:16.422 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":277,"completed":267,"skipped":4532,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:56:13.195: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 08:56:13.438: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar  5 08:56:13.463: INFO: Number of nodes with available pods: 0
Mar  5 08:56:13.464: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 08:56:14.487: INFO: Number of nodes with available pods: 0
Mar  5 08:56:14.487: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 08:56:15.494: INFO: Number of nodes with available pods: 3
Mar  5 08:56:15.494: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 08:56:16.492: INFO: Number of nodes with available pods: 6
Mar  5 08:56:16.492: INFO: Number of running nodes: 6, number of available pods: 6
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar  5 08:56:16.631: INFO: Wrong image for pod: daemon-set-6cs4h. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:16.631: INFO: Wrong image for pod: daemon-set-8jszx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:16.631: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:16.631: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:16.631: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:16.631: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:17.649: INFO: Wrong image for pod: daemon-set-6cs4h. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:17.649: INFO: Wrong image for pod: daemon-set-8jszx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:17.649: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:17.649: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:17.649: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:17.649: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:18.661: INFO: Wrong image for pod: daemon-set-6cs4h. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:18.662: INFO: Wrong image for pod: daemon-set-8jszx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:18.662: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:18.662: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:18.662: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:18.662: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:19.648: INFO: Wrong image for pod: daemon-set-6cs4h. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:19.649: INFO: Wrong image for pod: daemon-set-8jszx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:19.649: INFO: Pod daemon-set-8jszx is not available
Mar  5 08:56:19.649: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:19.649: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:19.649: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:19.649: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:20.663: INFO: Wrong image for pod: daemon-set-6cs4h. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:20.663: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:20.663: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:20.663: INFO: Pod daemon-set-tpkrr is not available
Mar  5 08:56:20.663: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:20.663: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:21.650: INFO: Wrong image for pod: daemon-set-6cs4h. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:21.650: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:21.650: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:21.650: INFO: Pod daemon-set-tpkrr is not available
Mar  5 08:56:21.650: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:21.650: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:22.647: INFO: Wrong image for pod: daemon-set-6cs4h. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:22.647: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:22.647: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:22.647: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:22.647: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:23.648: INFO: Wrong image for pod: daemon-set-6cs4h. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:23.649: INFO: Pod daemon-set-6cs4h is not available
Mar  5 08:56:23.649: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:23.649: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:23.649: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:23.649: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:24.652: INFO: Wrong image for pod: daemon-set-6cs4h. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:24.652: INFO: Pod daemon-set-6cs4h is not available
Mar  5 08:56:24.652: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:24.652: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:24.652: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:24.652: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:25.647: INFO: Wrong image for pod: daemon-set-6cs4h. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:25.647: INFO: Pod daemon-set-6cs4h is not available
Mar  5 08:56:25.647: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:25.647: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:25.647: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:25.647: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:26.652: INFO: Wrong image for pod: daemon-set-6cs4h. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:26.652: INFO: Pod daemon-set-6cs4h is not available
Mar  5 08:56:26.652: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:26.652: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:26.652: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:26.652: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:27.665: INFO: Wrong image for pod: daemon-set-6cs4h. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:27.665: INFO: Pod daemon-set-6cs4h is not available
Mar  5 08:56:27.665: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:27.665: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:27.665: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:27.665: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:28.649: INFO: Wrong image for pod: daemon-set-6cs4h. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:28.649: INFO: Pod daemon-set-6cs4h is not available
Mar  5 08:56:28.649: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:28.649: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:28.650: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:28.650: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:29.647: INFO: Wrong image for pod: daemon-set-6cs4h. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:29.647: INFO: Pod daemon-set-6cs4h is not available
Mar  5 08:56:29.647: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:29.647: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:29.647: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:29.647: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:30.648: INFO: Wrong image for pod: daemon-set-6cs4h. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:30.648: INFO: Pod daemon-set-6cs4h is not available
Mar  5 08:56:30.648: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:30.648: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:30.648: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:30.648: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:31.650: INFO: Wrong image for pod: daemon-set-6cs4h. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:31.650: INFO: Pod daemon-set-6cs4h is not available
Mar  5 08:56:31.650: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:31.650: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:31.650: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:31.650: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:32.647: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:32.647: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:32.647: INFO: Pod daemon-set-mtr8k is not available
Mar  5 08:56:32.647: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:32.647: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:33.653: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:33.653: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:33.653: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:33.653: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:34.649: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:34.649: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:34.649: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:34.649: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:35.653: INFO: Wrong image for pod: daemon-set-9n84b. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:35.653: INFO: Pod daemon-set-9n84b is not available
Mar  5 08:56:35.653: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:35.653: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:35.653: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:36.648: INFO: Pod daemon-set-b5wb9 is not available
Mar  5 08:56:36.648: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:36.648: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:36.648: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:37.647: INFO: Pod daemon-set-b5wb9 is not available
Mar  5 08:56:37.647: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:37.647: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:37.647: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:38.651: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:38.651: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:38.651: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:39.647: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:39.647: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:39.647: INFO: Wrong image for pod: daemon-set-wrqdg. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:39.647: INFO: Pod daemon-set-wrqdg is not available
Mar  5 08:56:40.649: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:40.649: INFO: Pod daemon-set-r9nkp is not available
Mar  5 08:56:40.649: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:41.647: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:41.648: INFO: Pod daemon-set-r9nkp is not available
Mar  5 08:56:41.648: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:42.648: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:42.648: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:43.646: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:43.646: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:44.646: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:44.646: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:44.646: INFO: Pod daemon-set-vplwr is not available
Mar  5 08:56:45.660: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:45.660: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:45.660: INFO: Pod daemon-set-vplwr is not available
Mar  5 08:56:46.654: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:46.654: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:46.654: INFO: Pod daemon-set-vplwr is not available
Mar  5 08:56:47.648: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:47.648: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:47.648: INFO: Pod daemon-set-vplwr is not available
Mar  5 08:56:48.648: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:48.648: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:48.648: INFO: Pod daemon-set-vplwr is not available
Mar  5 08:56:49.648: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:49.649: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:49.649: INFO: Pod daemon-set-vplwr is not available
Mar  5 08:56:50.654: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:50.654: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:50.654: INFO: Pod daemon-set-vplwr is not available
Mar  5 08:56:51.646: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:51.646: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:51.646: INFO: Pod daemon-set-vplwr is not available
Mar  5 08:56:52.648: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:52.648: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:52.648: INFO: Pod daemon-set-vplwr is not available
Mar  5 08:56:53.679: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:53.679: INFO: Wrong image for pod: daemon-set-vplwr. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:53.679: INFO: Pod daemon-set-vplwr is not available
Mar  5 08:56:54.649: INFO: Pod daemon-set-glbf2 is not available
Mar  5 08:56:54.650: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:55.648: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:56.676: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:57.646: INFO: Wrong image for pod: daemon-set-ldtbn. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar  5 08:56:57.646: INFO: Pod daemon-set-ldtbn is not available
Mar  5 08:56:58.647: INFO: Pod daemon-set-d8bmf is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Mar  5 08:56:58.679: INFO: Number of nodes with available pods: 5
Mar  5 08:56:58.679: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 08:56:59.703: INFO: Number of nodes with available pods: 5
Mar  5 08:56:59.703: INFO: Node devops-control-plane-1 is running more than one daemon pod
Mar  5 08:57:00.700: INFO: Number of nodes with available pods: 6
Mar  5 08:57:00.701: INFO: Number of running nodes: 6, number of available pods: 6
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3826, will wait for the garbage collector to delete the pods
Mar  5 08:57:00.838: INFO: Deleting DaemonSet.extensions daemon-set took: 14.513039ms
Mar  5 08:57:00.938: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.399537ms
Mar  5 08:57:13.749: INFO: Number of nodes with available pods: 0
Mar  5 08:57:13.749: INFO: Number of running nodes: 0, number of available pods: 0
Mar  5 08:57:13.756: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3826/daemonsets","resourceVersion":"5652707"},"items":null}

Mar  5 08:57:13.763: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3826/pods","resourceVersion":"5652707"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:57:13.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3826" for this suite.

â€¢ [SLOW TEST:60.666 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":277,"completed":268,"skipped":4570,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:57:13.861: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1429
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  5 08:57:14.104: INFO: Waiting up to 5m0s for pod "pod-22239dec-7755-4e4a-80bd-237937fa1a8f" in namespace "emptydir-1429" to be "Succeeded or Failed"
Mar  5 08:57:14.109: INFO: Pod "pod-22239dec-7755-4e4a-80bd-237937fa1a8f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.195729ms
Mar  5 08:57:16.116: INFO: Pod "pod-22239dec-7755-4e4a-80bd-237937fa1a8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012315042s
STEP: Saw pod success
Mar  5 08:57:16.116: INFO: Pod "pod-22239dec-7755-4e4a-80bd-237937fa1a8f" satisfied condition "Succeeded or Failed"
Mar  5 08:57:16.122: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-22239dec-7755-4e4a-80bd-237937fa1a8f container test-container: <nil>
STEP: delete the pod
Mar  5 08:57:16.174: INFO: Waiting for pod pod-22239dec-7755-4e4a-80bd-237937fa1a8f to disappear
Mar  5 08:57:16.179: INFO: Pod pod-22239dec-7755-4e4a-80bd-237937fa1a8f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:57:16.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1429" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":269,"skipped":4571,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:57:16.199: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4617
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 08:57:16.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc387ef0-5e2d-4aa5-bbee-c972c4743c40" in namespace "projected-4617" to be "Succeeded or Failed"
Mar  5 08:57:16.448: INFO: Pod "downwardapi-volume-fc387ef0-5e2d-4aa5-bbee-c972c4743c40": Phase="Pending", Reason="", readiness=false. Elapsed: 14.101077ms
Mar  5 08:57:18.458: INFO: Pod "downwardapi-volume-fc387ef0-5e2d-4aa5-bbee-c972c4743c40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023995656s
STEP: Saw pod success
Mar  5 08:57:18.458: INFO: Pod "downwardapi-volume-fc387ef0-5e2d-4aa5-bbee-c972c4743c40" satisfied condition "Succeeded or Failed"
Mar  5 08:57:18.466: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-fc387ef0-5e2d-4aa5-bbee-c972c4743c40 container client-container: <nil>
STEP: delete the pod
Mar  5 08:57:18.507: INFO: Waiting for pod downwardapi-volume-fc387ef0-5e2d-4aa5-bbee-c972c4743c40 to disappear
Mar  5 08:57:18.513: INFO: Pod downwardapi-volume-fc387ef0-5e2d-4aa5-bbee-c972c4743c40 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:57:18.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4617" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":270,"skipped":4581,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:57:18.538: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5374.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5374.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5374.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5374.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5374.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5374.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  5 08:57:20.942: INFO: DNS probes using dns-5374/dns-test-23c9a61b-deec-4848-a248-d0c959bc042a succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:57:21.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5374" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":277,"completed":271,"skipped":4596,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:57:21.066: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6447
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  5 08:57:21.284: INFO: Waiting up to 5m0s for pod "pod-3560a8c1-dc52-407c-a88a-1655ac1d4ad6" in namespace "emptydir-6447" to be "Succeeded or Failed"
Mar  5 08:57:21.293: INFO: Pod "pod-3560a8c1-dc52-407c-a88a-1655ac1d4ad6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.207287ms
Mar  5 08:57:23.301: INFO: Pod "pod-3560a8c1-dc52-407c-a88a-1655ac1d4ad6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016118552s
STEP: Saw pod success
Mar  5 08:57:23.301: INFO: Pod "pod-3560a8c1-dc52-407c-a88a-1655ac1d4ad6" satisfied condition "Succeeded or Failed"
Mar  5 08:57:23.306: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod pod-3560a8c1-dc52-407c-a88a-1655ac1d4ad6 container test-container: <nil>
STEP: delete the pod
Mar  5 08:57:23.362: INFO: Waiting for pod pod-3560a8c1-dc52-407c-a88a-1655ac1d4ad6 to disappear
Mar  5 08:57:23.368: INFO: Pod pod-3560a8c1-dc52-407c-a88a-1655ac1d4ad6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 08:57:23.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6447" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":272,"skipped":4628,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 08:57:23.396: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-721
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-113737f2-fed4-4222-980f-2a40baebc86a in namespace container-probe-721
Mar  5 08:57:25.698: INFO: Started pod busybox-113737f2-fed4-4222-980f-2a40baebc86a in namespace container-probe-721
STEP: checking the pod's current state and verifying that restartCount is present
Mar  5 08:57:25.706: INFO: Initial restart count of pod busybox-113737f2-fed4-4222-980f-2a40baebc86a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 09:01:26.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-721" for this suite.

â€¢ [SLOW TEST:243.476 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":277,"completed":273,"skipped":4648,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 09:01:26.874: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7928
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar  5 09:01:27.112: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70aa56c0-5214-46d2-a404-0e9c79411215" in namespace "downward-api-7928" to be "Succeeded or Failed"
Mar  5 09:01:27.118: INFO: Pod "downwardapi-volume-70aa56c0-5214-46d2-a404-0e9c79411215": Phase="Pending", Reason="", readiness=false. Elapsed: 5.390434ms
Mar  5 09:01:29.132: INFO: Pod "downwardapi-volume-70aa56c0-5214-46d2-a404-0e9c79411215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019891369s
Mar  5 09:01:31.141: INFO: Pod "downwardapi-volume-70aa56c0-5214-46d2-a404-0e9c79411215": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028224506s
STEP: Saw pod success
Mar  5 09:01:31.141: INFO: Pod "downwardapi-volume-70aa56c0-5214-46d2-a404-0e9c79411215" satisfied condition "Succeeded or Failed"
Mar  5 09:01:31.148: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod downwardapi-volume-70aa56c0-5214-46d2-a404-0e9c79411215 container client-container: <nil>
STEP: delete the pod
Mar  5 09:01:31.196: INFO: Waiting for pod downwardapi-volume-70aa56c0-5214-46d2-a404-0e9c79411215 to disappear
Mar  5 09:01:31.206: INFO: Pod downwardapi-volume-70aa56c0-5214-46d2-a404-0e9c79411215 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 09:01:31.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7928" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":277,"completed":274,"skipped":4661,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 09:01:31.234: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3155
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar  5 09:01:37.514: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 09:01:37.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0305 09:01:37.514281      26 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3155" for this suite.

â€¢ [SLOW TEST:6.317 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":277,"completed":275,"skipped":4664,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 09:01:37.554: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1181
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar  5 09:01:37.744: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  5 09:01:46.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-1181 create -f -'
Mar  5 09:01:47.347: INFO: stderr: ""
Mar  5 09:01:47.347: INFO: stdout: "e2e-test-crd-publish-openapi-680-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  5 09:01:47.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-1181 delete e2e-test-crd-publish-openapi-680-crds test-cr'
Mar  5 09:01:47.515: INFO: stderr: ""
Mar  5 09:01:47.515: INFO: stdout: "e2e-test-crd-publish-openapi-680-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar  5 09:01:47.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-1181 apply -f -'
Mar  5 09:01:48.112: INFO: stderr: ""
Mar  5 09:01:48.112: INFO: stdout: "e2e-test-crd-publish-openapi-680-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  5 09:01:48.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 --namespace=crd-publish-openapi-1181 delete e2e-test-crd-publish-openapi-680-crds test-cr'
Mar  5 09:01:48.245: INFO: stderr: ""
Mar  5 09:01:48.245: INFO: stdout: "e2e-test-crd-publish-openapi-680-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar  5 09:01:48.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-378331353 explain e2e-test-crd-publish-openapi-680-crds'
Mar  5 09:01:48.606: INFO: stderr: ""
Mar  5 09:01:48.607: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-680-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 09:01:56.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1181" for this suite.

â€¢ [SLOW TEST:19.403 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":277,"completed":276,"skipped":4684,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar  5 09:01:56.958: INFO: >>> kubeConfig: /tmp/kubeconfig-378331353
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5662
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override command
Mar  5 09:01:57.177: INFO: Waiting up to 5m0s for pod "client-containers-f12ad734-78a7-4cbb-a4ae-230a9339f6ae" in namespace "containers-5662" to be "Succeeded or Failed"
Mar  5 09:01:57.183: INFO: Pod "client-containers-f12ad734-78a7-4cbb-a4ae-230a9339f6ae": Phase="Pending", Reason="", readiness=false. Elapsed: 5.569949ms
Mar  5 09:01:59.194: INFO: Pod "client-containers-f12ad734-78a7-4cbb-a4ae-230a9339f6ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016613199s
STEP: Saw pod success
Mar  5 09:01:59.194: INFO: Pod "client-containers-f12ad734-78a7-4cbb-a4ae-230a9339f6ae" satisfied condition "Succeeded or Failed"
Mar  5 09:01:59.200: INFO: Trying to get logs from node devops-pool1-6c76d44df9-8tgs6 pod client-containers-f12ad734-78a7-4cbb-a4ae-230a9339f6ae container test-container: <nil>
STEP: delete the pod
Mar  5 09:01:59.239: INFO: Waiting for pod client-containers-f12ad734-78a7-4cbb-a4ae-230a9339f6ae to disappear
Mar  5 09:01:59.244: INFO: Pod client-containers-f12ad734-78a7-4cbb-a4ae-230a9339f6ae no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.6-rc.0.48+a9f7208b601483/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar  5 09:01:59.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5662" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":277,"completed":277,"skipped":4700,"failed":0}
SSSSSSSSSSSSSSSMar  5 09:01:59.265: INFO: Running AfterSuite actions on all nodes
Mar  5 09:01:59.265: INFO: Running AfterSuite actions on node 1
Mar  5 09:01:59.265: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":277,"completed":277,"skipped":4715,"failed":0}

Ran 277 of 4992 Specs in 4487.095 seconds
SUCCESS! -- 277 Passed | 0 Failed | 0 Pending | 4715 Skipped
PASS

Ginkgo ran 1 suite in 1h14m49.1079659s
Test Suite Passed
